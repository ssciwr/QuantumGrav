{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Simple model training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to train a simple classifier model using the QuantumGravPy framework. We will set up the data, define the model, and run the training process while monitoring performance metrics. \n",
    "This model does not perform well on the task, but serves as a basic example of how to use the framework for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "First, we import all the necessary packages and modules from QuantumGravPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main library\n",
    "import QuantumGrav as QG\n",
    "\n",
    "# logging, typing, configs, numpy\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# data\n",
    "import zarr\n",
    "from zarr.storage import LocalStore\n",
    "from zarr import Group\n",
    "\n",
    "# data handling and plotting\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ML with torch\n",
    "import torch\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Packages of note here: \n",
    "- sklearn is the scipy machine learning library and contains a vast collection tools, models, and metrics for machine learning tasks.\n",
    "- seaborn is a statistical data visualization library built on top of matplotlib, providing a high-level interface for creating informative and attractive statistical graphics. \n",
    "- zarr is a powerful library for working with large, chunked, compressed, N-dimensional arrays, enabling efficient storage and retrieval of large datasets.\n",
    "- torch_geometric is an extension library for PyTorch that provides tools and functionalities for deep learning on graph-structured data, enabling the development of graph neural networks and related models. QuantumGrav's Models are built on top of this library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Preparations \n",
    "\n",
    "First, we set the formatting that the logger should use to have a bit more structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loggin level to info for trainer output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # or logging.INFO if you want less verbosity\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "We also add a pretty printer function to display configuration sections in a more readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(data: dict[Any, Any], offset = \"\") -> None:\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    if isinstance(data, dict):\n",
    "        print()\n",
    "        for key, value in data.items():\n",
    "            print(f\"{offset}{key}:\", end=\" \")\n",
    "            pretty_print(value, offset + \"  \")\n",
    "    elif isinstance(data, list):\n",
    "        print()\n",
    "        for i, item in enumerate(data):\n",
    "            print(f\"{offset}- Item {i}:\", end=\" \")\n",
    "            pretty_print(item, offset + \"  \" )\n",
    "    else:\n",
    "        pp.pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Define reader function for data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "At first, we need functions that reads the data from disk and convert it into a format that can be used by the model.\n",
    "This format is the torch_geometric Data. Here, we split this task into two functions. \n",
    "The first is `read_data`: This function reads the raw data from a Zarr store on disk and builds a dictionary from them. QuantumGrav provides a helper function to read an entire zarr group into memory. This function has a fixed signature that needs to be adhered to: \n",
    "```python\n",
    "def read_data(store: zarr.storage.Store, idx: int, float_dtype: torch.dtype, int_dtype: torch.dtype, validate_data: bool) -> dict\n",
    "```\n",
    "These have the following meanings: \n",
    "- `store`: The zarr store from which to read the data.\n",
    "- `idx`: The index of the data sample to read. All samples in a given dataset are indexed from 0 to N-1, where N is the total number of samples. This index runs across all zarr stores that are a part of the dataset. \n",
    "- `float_dtype`: The desired floating-point data type for the data.\n",
    "- `int_dtype`: The desired integer data type for the data.\n",
    "- `validate_data`: A boolean flag indicating whether to validate the data after reading it.\n",
    "The function should return a dictionary containing the data read from the zarr store.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(\n",
    "    store: LocalStore,\n",
    "    idx: int,\n",
    "    # these parameters are needed for the data reading function signature\n",
    "    float_dtype: torch.dtype,\n",
    "    int_dtype: torch.dtype,\n",
    "    validate_data: bool,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Read raw data from file\"\"\"\n",
    "\n",
    "    # create an empty dict to hold the data\n",
    "    data: dict[str, Any] = dict()\n",
    "\n",
    "    # open the correct cset group. We always get the whole store, so we are not\n",
    "    # dependent on a root group being present. Julia does not build one by default\n",
    "    cset_group = zarr.open_group(store, path = f\"cset_{idx+1}\", mode=\"r\")\n",
    "\n",
    "    # read the data from the cset group into the data dict. This reads the entire group.\n",
    "    QG.zarr_group_to_dict(\n",
    "        cset_group, data\n",
    "    )  # insert name of cset group to read\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# transform into a Data object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The second function takes this dictionary and transforms it into a torch_geometric Data object. This function has a fixed signature that needs to be adhered to: \n",
    "```python\n",
    "def collect_data(data: dict,) -> Data\n",
    "```\n",
    "\n",
    "i.e., it transforms a dictionary into a Data object. \n",
    "What it does internally with the data dictionary is up to the author of the function. In our case, we extract the adjacency matrix, node features, and labels from the dictionary and convert them into the appropriate format for a Data object.\n",
    "However, we can also do more complex things, like computing eigenvalues, normalizing features, or adding additional attributes to the Data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_data(data: dict) -> Data:\n",
    "    \"\"\"transform the data dictionary into a PyG Data object and cache the result on disk\"\"\"\n",
    "\n",
    "    adj = data[\"linkmat\"]\n",
    "\n",
    "    # normally, a transpose would be necessary here b/c Julia stores column major\n",
    "    # and Python/numpy/torch row major by default but torch sparse does a transpose\n",
    "    # again, so we can leave it as is...\n",
    "    edge_index, _ = dense_to_sparse(torch.tensor(adj, dtype=torch.float32))\n",
    "\n",
    "    # in and out degrees for the degree labels of the link matrix\n",
    "    x = torch.tensor(\n",
    "        np.array([\n",
    "            data[\"in_degrees_link\"],\n",
    "            data[\"out_degrees_link\"],\n",
    "        ]),\n",
    "        dtype=torch.float32,\n",
    "    ).t()  # shape [N,num_labels]\n",
    "\n",
    "    # manifold like as target\n",
    "    y = torch.tensor(\n",
    "        np.array([\n",
    "            data[\"manifold_like\"],\n",
    "        ]),\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    # build the final PyG data object\n",
    "    tgdata = Data(\n",
    "        edge_index=edge_index,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        csettype = torch.tensor(data[\"cset_type\"]),\n",
    "        atom_count=torch.tensor(adj.shape[0]),\n",
    "    )\n",
    "\n",
    "    if not tgdata.validate():\n",
    "        raise ValueError(f\"Data validation failed for {tgdata}.\")\n",
    "\n",
    "    return tgdata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Why the split? Because you have a choice of when to apply the data transformations. You can do it once when reading the data from disk and cache the result (then it's called `pre_transform`) or you can do it on-the-fly during training (then it's called `transform`). What you choose depends on the complexity of the transformations and the size of your dataset, but also on the workflow you are using. If you are experimenting more on the data side, it can be more useful to do it on-the-fly. If the transformations are expensive, it can be better to do it once and cache the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Next, we need a set of functions that define the loss function to use to train the model, and functions that compute the monitoring metrics from the model outputs that we want to track during validation and testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# define loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "First, we define the loss function to use during training. In this case, we use the standard binary cross-entropy loss for binary classification tasks. The loss function takes the model outputs and the true labels as inputs and computes the loss value. It must be noted that the model outputs a dictionary of tensors (one per task head), so we need to extract the relevant tensor for the loss computation. \n",
    "\n",
    "Note that the loss function must be appropriate for the task at hand. For multi-class classification, for example, you would use a different loss function, such as cross-entropy loss. \n",
    "\n",
    "Here, we also added checks for not-a-number (NaN) values in the model outputs and labels to ensure that the loss computation is valid. If NaN values are detected, an error is raised to prevent invalid computations during training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(\n",
    "    predictions: dict[Any, torch.Tensor],\n",
    "    data: Data,\n",
    "    *args,\n",
    "    **kwargs,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute loss for a single classifier task at index 0\"\"\"\n",
    "\n",
    "    pred = predictions[0]\n",
    "    tgt = data.y\n",
    "\n",
    "    # check for nans -> comment out for speed, this is python overhead\n",
    "    if torch.isnan(pred).any():\n",
    "        raise ValueError(\"Nan in predictions\")\n",
    "\n",
    "    if torch.isnan(tgt).any():\n",
    "        raise ValueError(\"Nan in targets\")\n",
    "\n",
    "    return torch.nn.BCEWithLogitsLoss(\n",
    "        reduction=\"mean\",\n",
    "    )(pred, tgt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# Monitor functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Just like the loss function computes how 'well' a model is performing during training in order to compute the gradients for optimization, monitor functions compute metrics that help us understand how well the model is performing on validation and test datasets without affecting the training process itself. These therefore use metrics that measure model performance more directly, such as accuracy, precision, recall, F1-score, etc. \n",
    "\n",
    "In the same way as the loss function, the monitor functions take the model outputs and true labels as inputs and compute the desired metrics. Again, we need to extract the relevant tensor from the model outputs for the computation.\n",
    "\n",
    "Here we monitor two versions of the [f1 score](https://en.wikipedia.org/wiki/F-score), a standard metric for classification tasks. We compute the weighted f1 score, which takes into account class imbalances by weighting the contribution of each class according to its prevalence in the dataset. This is useful when dealing with imbalanced datasets where some classes are more frequent than others. This is done in the function `f1_monitor`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_monitor(\n",
    "    predictions: list[dict[Any, torch.Tensor]],\n",
    "    targets: list[torch.Tensor],\n",
    ") -> float:\n",
    "    \"\"\"Compute F1 score for a single classifier task (key 0).\"\"\"\n",
    "\n",
    "    logits_list: list[torch.Tensor] = []\n",
    "    for p in predictions:\n",
    "        if not isinstance(p, dict):\n",
    "            raise TypeError(f\"Expected dict of tensors, got {type(p)}\")\n",
    "        if 0 in p:\n",
    "            t = p[0]\n",
    "        elif \"0\" in p:\n",
    "            t = p[\"0\"]\n",
    "        else:\n",
    "            t = next(iter(p.values()))\n",
    "        logits_list.append(t.detach())\n",
    "\n",
    "    pred_logits = torch.cat([t.reshape(-1) for t in logits_list])\n",
    "    tgt_vec = torch.cat([t.detach().reshape(-1) for t in targets])\n",
    "\n",
    "    pred_labels = (torch.sigmoid(pred_logits) >= 0.5).to(torch.int32).cpu().numpy()\n",
    "    tgt_labels = tgt_vec.to(torch.int32).cpu().numpy()\n",
    "\n",
    "    return f1_score(tgt_labels, pred_labels, average=\"weighted\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "In addition, we monitor the f1 score per class, which provides insights into how well the model performs on each individual class. This is particularly useful for identifying classes that may be more challenging for the model to classify correctly. This is done in the function `f1_monitor_perclass` below. The only difference to the previous function is that we do not use the `average='weighted'` option in the sklearn f1_score function, which computes the f1 score for each class separately. \n",
    "\n",
    "The disadvantage of this approach is that we need to do some work twice, the assembling of the true and predicted labels into numpy arrays for instance. QuantumGrav allows you to build your own evaluator classes however so if this becomes a bottleneck you can implement a custom evaluator that computes both metrics in one pass for instance. This is covered elsewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f1_monitor_perclass(\n",
    "    predictions: list[dict[Any, torch.Tensor]],\n",
    "    targets: list[torch.Tensor],\n",
    ") -> float:\n",
    "    \"\"\"Compute F1 score for a single classifier task (key 0).\"\"\"\n",
    "\n",
    "    logits_list: list[torch.Tensor] = []\n",
    "    for p in predictions:\n",
    "        if not isinstance(p, dict):\n",
    "            raise TypeError(f\"Expected dict of tensors, got {type(p)}\")\n",
    "        if 0 in p:\n",
    "            t = p[0]\n",
    "        elif \"0\" in p:\n",
    "            t = p[\"0\"]\n",
    "        else:\n",
    "            t = next(iter(p.values()))\n",
    "        logits_list.append(t.detach())\n",
    "\n",
    "    pred_logits = torch.cat([t.reshape(-1) for t in logits_list])\n",
    "    tgt_vec = torch.cat([t.detach().reshape(-1) for t in targets])\n",
    "\n",
    "    pred_labels = (torch.sigmoid(pred_logits) >= 0.5).to(torch.int32).cpu().numpy()\n",
    "    tgt_labels = tgt_vec.to(torch.int32).cpu().numpy()\n",
    "\n",
    "    return f1_score(tgt_labels, pred_labels, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# Setup trainer and datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "The trainer class is build from a config file that defines the model, optimizer, training epochs, dataset, evaluators and early stopping system as well as the loss function - everything we need to build a functioning training machinery. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(\"../configs/train_classifier.yaml\")\n",
    "\n",
    "with open(config_path, \"r\") as configfile:\n",
    "    config = yaml.load(configfile, QG.get_loader())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "let's have a look at how that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(config[\"training\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(config[\"validation\"][\"validator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(config[\"testing\"][\"tester\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(config[\"early_stopping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up trainer\n",
    "trainer = QG.Trainer.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Prepare dataset and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "let's once more have a look at the config file for the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(config[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Here, we see that the functions for data reading and transformation are actually used: The `collect_data` function is used as a `pre_transform`, i.e., the results will be cached on disk, and `read_data` will read the raw data. Have a look at the documentation of `QGDastaset` to learn more about them. We also want the dataset to be shuffled here. Doing this, either here in the dataset or in the data loader, is important because otherwise we will stack the data by type otherwise (polynomial first, then complex, then random), which is bad for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "The `prepare_dataloaders` function normally takes care of preparing the dataset too, but we can do this by hand or also use our own, independently build dataset that we have put together in code without the config. Have a look at the arguments of `prepare_dataloader` for instance. \n",
    "The dataloaders have their own arguments: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(config[\"training\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Pay attention to the parameters from batch_size downwards. We have a batchsize of 64, and drop the last batch when it can't be made into one that is 64 datapoints long. We also use 12 worker processes to read data and build batches and try to pre-load as much as 8 batches into memory to minimize the time the gpu has to wait on data to become available. Finally, we use `pin_memory` which is a optimization to improve transfer speed from cpu to gpu (see [here](https://discuss.pytorch.org/t/when-to-set-pin-memory-to-true/19723) and [here](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/) for more). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(config[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloaders\n",
    "train_loader, valid_loader, test_loader = trainer.prepare_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "We have a dataset of 55000 datapoints in this case, and split this into 80% training-, 10% validation and 10% testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "# check out data and plot some statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Let us examine the dataset to verify we have no unwanted structures in it. First we put the data we want into pandas dataframes which makes plotting with seaborn easy. That's substantial work, hence we avoid doing it when the dataframe has already been saved on disk before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (Path(config[\"training\"][\"path\"]) / \"cset_data_summary.csv\").exists():\n",
    "\n",
    "    import tqdm\n",
    "\n",
    "\n",
    "    df_trainer = pd.DataFrame(\n",
    "        {\n",
    "            \"cset_type\": [],\n",
    "            \"cset_size\": [],\n",
    "            \"manifold_like\": [],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    df_validator = pd.DataFrame(\n",
    "        {\n",
    "            \"cset_type\": [],\n",
    "            \"cset_size\": [],\n",
    "            \"manifold_like\": [],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    df_tester = pd.DataFrame(\n",
    "        {\n",
    "            \"cset_type\": [],\n",
    "            \"cset_size\": [],\n",
    "            \"manifold_like\": [],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(trainer.train_dataset))):\n",
    "        data = trainer.train_dataset[i]\n",
    "        df_trainer.loc[len(df_trainer)] = {\n",
    "            \"cset_type\": data.csettype.to(torch.int32).item(),\n",
    "            \"cset_size\": data.atom_count.item(),\n",
    "            \"manifold_like\": data.y.item(),\n",
    "        }\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(trainer.val_dataset))):\n",
    "        data = trainer.val_dataset[i]\n",
    "        df_validator.loc[len(df_validator)] = {\n",
    "            \"cset_type\": data.csettype.to(torch.int32).item(),\n",
    "            \"cset_size\": data.atom_count.item(),\n",
    "            \"manifold_like\": data.y.item(),\n",
    "        }\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(trainer.test_dataset))):\n",
    "        data = trainer.test_dataset[i]\n",
    "        df_tester.loc[len(df_tester)] = {\n",
    "            \"cset_type\": data.csettype.to(torch.int32).item(),\n",
    "            \"cset_size\": data.atom_count.item(),\n",
    "            \"manifold_like\": data.y.item(),\n",
    "        }\n",
    "\n",
    "    df_trainer[\"dataset\"] = \"trainer\"\n",
    "    df_validator[\"dataset\"] = \"validator\"\n",
    "    df_tester[\"dataset\"] = \"tester\"\n",
    "\n",
    "    df_all = pd.concat([df_trainer, df_validator, df_tester], ignore_index=True)\n",
    "    df_all.head()\n",
    "    df_all[\"manifold_like\"] = df_all[\"manifold_like\"].astype(int)\n",
    "    df_all.to_csv(Path(config[\"training\"][\"path\"]) / \"cset_data_summary.csv\", index=False)\n",
    "else:\n",
    "    df_all = pd.read_csv(Path(config[\"training\"][\"path\"]) / \"cset_data_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "Here, we only record cset_type, cset_size and manifold_likeness, but we can do whatever we have in the output of `collect_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Let's have a look at the histogram of the `cset_type`: We see that they are all relatively evenly distributed between training, validation and test set, but since the dataset contains two stores of complex- and polynomial data (types 1, 2), their bars are roughly twice the height of the others too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "type_plot = sns.histplot(data=df_all, x=\"cset_type\", hue=\"dataset\", multiple=\"dodge\", shrink=0.8, stat = \"count\")\n",
    "fig = type_plot.get_figure()\n",
    "\n",
    "# save figure to output path\n",
    "fig.savefig(Path(config[\"training\"][\"path\"]) / \"cset_type_distribution.png\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "... consequently if we make a histogram of manifold_likeness, we see as expected that complex- and polynomial datasets are marked as manifold like and the others are not. They are still, overall, underreprepresented though compared to the other ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "manifoldlike_plot = sns.histplot(data=df_all, x=\"cset_type\", hue=\"manifold_like\", multiple=\"dodge\", palette = \"tab10\")\n",
    "fig = manifoldlike_plot.get_figure()\n",
    "\n",
    "# save figure to output path\n",
    "fig.savefig(Path(config[\"training\"][\"path\"]) / \"manifold_like_distribution.png\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "Let's make a more complex plot that shows marginal distributions over types and sizes to see if there are other biases. We see that there are fewer examples than for the others, This is because for complex csets, some nodes can be deleted when they are too close to a cut through the domain. The rest is consistent with what we have seen before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_plot = sns.jointplot(data=df_all, x=\"cset_size\", y=\"cset_type\", kind=\"hist\", cmap=\"mako\")\n",
    "\n",
    "# save figure to output path\n",
    "size_plot.savefig(Path(config[\"training\"][\"path\"]) / \"cset_size_distribution.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "# run training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "We can run training now. We first need to initialize the model and the optimizer that adjusts its parameter. These are defined in the config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(config[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "The trainer uses this config to set up the model when we call the `initialize_model` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "trainer.initialize_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "For the optimizer, we are  using the [AdamW optimizer](https://docs.pytorch.org/docs/main/generated/torch.optim.AdamW.html) here. In the same way, the trainer uses the config to build the optimizer when we call the `initalize_optimizer()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(config[\"training\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimizer\n",
    "trainer.initialize_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "Finally, we start the training process by calling the trainer's `run_training` function and pass it the train_dataloader and the validation dataloader as arguments to use as training and validation data. The rest - looping over epochs, calling the optimizer and recording data, is handled by the trainer in the background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results, validation_results = trainer.run_training(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=valid_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "After training, the model is tested on an unseen dataset. The results informs us about how well the model generalizes to unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.run_test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "testing results are somewhat mediocre so our model is not doing too well.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "# save training, validation, test results to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "because the validation dand testing results are dataframes, we can make use of pandas powerful abilities and interactions with seaborn for visualization, data handling and computation. Here, we save the results to csv files to the same directory we save the other training data to, and then proceed to visualize the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training, validation, test results to disk\n",
    "train_results = pd.DataFrame(train_results, columns=[\"loss_mean\", \"loss_std\"])\n",
    "\n",
    "train_results.to_csv(\n",
    "    Path(config[\"training\"][\"path\"]) / \"training_results.csv\"\n",
    ")\n",
    "\n",
    "# save validatoin results to disk\n",
    "validation_results.to_csv(Path(config[\"training\"][\"path\"]) / \"validation_result.csv\")\n",
    "\n",
    "# run test\n",
    "test_results.to_csv(Path(config[\"training\"][\"path\"]) / \"test_result.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "In order for plotting with seaborn to work, we need to change the validataion dataframe to long form. We also add an 'epoch' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results[\"epoch\"] = range(1, len(validation_results) + 1)\n",
    "\n",
    "validation_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_melted =validation_results.loc[:, [\"epoch\", \"loss_avg\", \"loss_min\", \"loss_max\", \"f1_weighted\"]].melt(\n",
    "    id_vars=[\"epoch\"],\n",
    "    value_vars=[\"loss_avg\", \"loss_min\", \"loss_max\", \"f1_weighted\"],\n",
    "    var_name=\"metric\",\n",
    "    value_name=\"value\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data=valid_melted, x=\"epoch\", y=\"value\", hue=\"metric\", linewidth=2.5, markers=\"o\", legend=\"brief\")\n",
    "ax.figure.savefig(Path(config[\"training\"][\"path\"]) / \"validation_metrics.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "As is apparent above, the model seems to be not powerful enough to learn well from the data, adn f1 scores and losses are not very stable. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
