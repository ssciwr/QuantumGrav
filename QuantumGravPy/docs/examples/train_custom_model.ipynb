{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter magic commands to avoid having to restart upon change of lib code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Training a custom model with QG \n",
    "\n",
    "You are not limited to the prebuild models that are available in QuantumGrav, but can use its facilities in full or in part to train your own models or replace parts of it with your own implementation. This notebook will give an example of how to build and train your own model with QuantumGrav. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Building the model \n",
    "\n",
    "We start with building a simple model by hand. This runs a torch_geometric graph convolutional layer in 'source-to-target' and 'target-to-source' message passing mode, which effectively acts as 'looking into the future neighborhood' and 'looking into the past neighborhood' if we think about causal sets (compare [torch_geometric's `DirGNNConv`](https://pytorch-geometric.readthedocs.io/en/2.5.2/generated/torch_geometric.nn.conv.DirGNNConv.html)). A trainable parameter will make the model learn to weight the two results in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from typing import Any\n",
    "from copy import deepcopy\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from zarr.storage import LocalStore\n",
    "import numpy as np\n",
    "import QuantumGrav as QG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "a more general way of building this would be the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, conv: type[torch.nn.Module], args: list[Any], kwargs: dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.futureconv  = deepcopy(conv(*args, **kwargs)) # deepcopy to avoid reusing conv in both parts\n",
    "        self.pastconv = deepcopy(conv(*args, **kwargs))\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.weightf = torch.nn.Parameter(torch.tensor(1.0))\n",
    "        self.weightb = torch.nn.Parameter(torch.tensor(1.0))\n",
    "        self.mlp = torch_geometric.nn.dense.Linear(args[1], 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor | None = None) -> torch.Tensor:\n",
    "        x_f = self.futureconv(x, edge_index) # future direction\n",
    "        x_p = self.pastconv(x, edge_index.flip([0])) # past direction\n",
    "        x_ = self.weightf*x_f + self.weightb*x_p\n",
    "        x_ = self.activation(x_)\n",
    "        x_ = torch_geometric.nn.pool.global_mean_pool(x_, batch)\n",
    "        x_ = self.mlp(x_)\n",
    "        return x_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## instantiating the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = BidirectionalModel(torch_geometric.nn.conv.SAGEConv, [2,32], {\"normalize\": True, \"root_weight\": False,})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# using our custom model in a config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "in order to use this with the trainer, directly, we can use this in a config node. To this end, first have a look at the config we already know and how the model is defined there: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "```yaml\n",
    "model:\n",
    "  encoder_type: !pyobject QuantumGrav.models.GNNBlock\n",
    "  encoder_args: [2, 32]\n",
    "  encoder_kwargs:\n",
    "    dropout: 0.3\n",
    "    with_skip: True\n",
    "    gnn_layer_type: !pyobject torch_geometric.nn.conv.GCNConv\n",
    "    gnn_layer_args: []\n",
    "    gnn_layer_kwargs: {cached: False, bias: True, add_self_loops: True}\n",
    "    normalizer_type: !pyobject torch.nn.BatchNorm1d\n",
    "    norm_args: [32]\n",
    "    norm_kwargs: {eps: 0.00001, momentum: 0.2}\n",
    "    activation_type: !pyobject torch.nn.ReLU\n",
    "    skip_args: [2, 32]\n",
    "    skip_kwargs: {weight_initializer: \"glorot\"}\n",
    "  downstream_tasks:\n",
    "    - [\n",
    "        !pyobject QuantumGrav.models.LinearSequential,\n",
    "        [\n",
    "            [[32, 1],],\n",
    "            [!pyobject torch.nn.Identity,],\n",
    "        ],\n",
    "        {\n",
    "            linear_kwargs: [\n",
    "                {bias: True,},\n",
    "            ],\n",
    "            activation_kwargs: [{},],\n",
    "        },\n",
    "    ]\n",
    "  pooling_layers:\n",
    "    - [!pyobject torch_geometric.nn.global_mean_pool, [], {}]\n",
    "\n",
    "  active_tasks:\n",
    "    0: True\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "this is geared towards building a `QG.GNNModel` instance, which for many usecases is general enough and provides a cognitive scheme to think about architecture. For our custom model, this scheme is not suitable anymore, and therefore we use a different way to define the model in the config, namely the usual (type, args, kwargs) scheme. \n",
    "\n",
    "**Note** This approach can become complicated for complex models and can lead to confusion in the config definition. Therefore, it's usually useful to think about the level of generality and complexity first, and parameterize the Model classes accordingly.\n",
    "\n",
    "For our simple example, this is well within the boundaries of the manageable, so here is the config definition for the custom model: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "```yaml \n",
    "model:\n",
    "  type: !pyobject __main__.BidirectionalModelGenearl\n",
    "  args:\n",
    "    - !pyobject torch_geometric.nn.conv.SAGEConv # type\n",
    "    - [2, 32,] # args \n",
    "    - {\n",
    "      normalize: True,\n",
    "      root_weight: False\n",
    "    } # kwargs \n",
    "  # no further kwargs for `BidirectionalModel`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "let's define the loss function, data reader and -transform so the trainer has something to work with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions: torch.Tensor,data:torch_geometric.data.Data, *args) -> torch.Tensor:\n",
    "    return torch.nn.functional.mse_loss(predictions, data.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(\n",
    "    store: LocalStore,\n",
    "    idx: int,\n",
    "    # these parameters are needed for the data reading function signature\n",
    "    float_dtype: torch.dtype,\n",
    "    int_dtype: torch.dtype,\n",
    "    validate_data: bool,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Read raw data from file\"\"\"\n",
    "\n",
    "    # create an empty dict to hold the data\n",
    "    data: dict[str, Any] = dict()\n",
    "\n",
    "    # open the correct cset group. We always get the whole store, so we are not\n",
    "    # dependent on a root group being present. Julia does not build one by default\n",
    "    cset_group = zarr.open_group(store, path = f\"cset_{idx+1}\", mode=\"r\")\n",
    "\n",
    "    # read the data from the cset group into the data dict. This reads the entire group.\n",
    "    QG.zarr_group_to_dict(\n",
    "        cset_group, data\n",
    "    )  # insert name of cset group to read\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_data(data: dict) -> torch_geometric.data.Data:\n",
    "    \"\"\"transform the data dictionary into a PyG Data object and cache the result on disk\"\"\"\n",
    "\n",
    "    adj = data[\"linkmat\"]\n",
    "\n",
    "    # normally, a transpose would be necessary here b/c Julia stores column major\n",
    "    # and Python/numpy/torch row major by default but torch sparse does a transpose\n",
    "    # again, so we can leave it as is...\n",
    "    edge_index, _ = torch_geometric.utils.dense_to_sparse(torch.tensor(adj, dtype=torch.float32))\n",
    "\n",
    "    # in and out degrees for the degree labels of the link matrix\n",
    "    x = torch.tensor(\n",
    "        np.array([\n",
    "            data[\"in_degrees_link\"],\n",
    "            data[\"out_degrees_link\"],\n",
    "        ]),\n",
    "        dtype=torch.float32,\n",
    "    ).t()  # shape [N,num_labels]\n",
    "\n",
    "    # manifold like as target\n",
    "    y = torch.tensor(\n",
    "        np.array([\n",
    "            data[\"manifold_like\"],\n",
    "        ]),\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    # build the final PyG data object\n",
    "    tgdata = torch_geometric.data.Data(\n",
    "        edge_index=edge_index,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        csettype = torch.tensor(data[\"cset_type\"]),\n",
    "        atom_count=torch.tensor(adj.shape[0]),\n",
    "    )\n",
    "\n",
    "    if not tgdata.validate():\n",
    "        raise ValueError(f\"Data validation failed for {tgdata}.\")\n",
    "\n",
    "    return tgdata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "we skipped the definition of additioanl monitor functions here, so only the average losses will be monitored. we also didn't make use of early stopping - the goal is to show how to use a custom model, not how to use the infrastructure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# load config and build trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(\"../../configs/train_custom_model.yaml\")\n",
    "\n",
    "with open(config_path, \"r\") as configfile:\n",
    "    config = yaml.load(configfile, QG.get_loader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QG.Trainer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = trainer.prepare_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run_training(train_loader=train_loader, val_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QuantumGrav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
