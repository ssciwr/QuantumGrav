{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter magic commands to avoid having to restart upon change of lib code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Simple model training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to train a simple classifier model using the QuantumGravPy framework. This follows the `train_simple_model.ipynb` notebook, but we will show how to build and include a weighted sampler for imbalanced data. If you are not interested in reviewing the details of setting up the model training, you can skip to the \"Prepare dataset and dataloaders with weighted sampler\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "First, we import all the necessary packages and modules from QuantumGravPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main library\n",
    "import QuantumGrav as QG\n",
    "\n",
    "# logging, typing, configs, numpy\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# data\n",
    "import zarr\n",
    "from zarr.storage import LocalStore\n",
    "from zarr import Group\n",
    "\n",
    "# data handling and plotting\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ML with torch\n",
    "import torch\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Packages of note here: \n",
    "- sklearn is the scipy machine learning library and contains a vast collection tools, models, and metrics for machine learning tasks.\n",
    "- seaborn is a statistical data visualization library built on top of matplotlib, providing a high-level interface for creating informative and attractive statistical graphics. \n",
    "- zarr is a powerful library for working with large, chunked, compressed, N-dimensional arrays, enabling efficient storage and retrieval of large datasets.\n",
    "- torch_geometric is an extension library for PyTorch that provides tools and functionalities for deep learning on graph-structured data, enabling the development of graph neural networks and related models. QuantumGrav's Models are built on top of this library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Preparations \n",
    "\n",
    "First, we set the formatting that the logger should use to have a bit more structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loggin level to info for trainer output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # or logging.INFO if you want less verbosity\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "We also add a pretty printer function to display configuration sections in a more readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(data: dict[Any, Any], offset = \"\") -> None:\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    if isinstance(data, dict):\n",
    "        print()\n",
    "        for key, value in data.items():\n",
    "            print(f\"{offset}{key}:\", end=\" \")\n",
    "            pretty_print(value, offset + \"  \")\n",
    "    elif isinstance(data, list):\n",
    "        print()\n",
    "        for i, item in enumerate(data):\n",
    "            print(f\"{offset}- Item {i}:\", end=\" \")\n",
    "            pretty_print(item, offset + \"  \" )\n",
    "    else:\n",
    "        pp.pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Define reader function for data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "At first, we need functions that reads the data from disk and convert it into a format that can be used by the model.\n",
    "This format is the torch_geometric Data. Here, we split this task into two functions. \n",
    "The first is `read_data`: This function reads the raw data from a Zarr store on disk and builds a dictionary from them. QuantumGrav provides a helper function to read an entire zarr group into memory. This function has a fixed signature that needs to be adhered to: \n",
    "```python\n",
    "def read_data(store: zarr.storage.Store, idx: int, float_dtype: torch.dtype, int_dtype: torch.dtype, validate_data: bool) -> dict\n",
    "```\n",
    "These have the following meanings: \n",
    "- `store`: The zarr store from which to read the data.\n",
    "- `idx`: The index of the data sample to read. All samples in a given dataset are indexed from 0 to N-1, where N is the total number of samples. This index runs across all zarr stores that are a part of the dataset. \n",
    "- `float_dtype`: The desired floating-point data type for the data.\n",
    "- `int_dtype`: The desired integer data type for the data.\n",
    "- `validate_data`: A boolean flag indicating whether to validate the data after reading it.\n",
    "The function should return a dictionary containing the data read from the zarr store.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(\n",
    "    store: LocalStore,\n",
    "    idx: int,\n",
    "    # these parameters are needed for the data reading function signature\n",
    "    float_dtype: torch.dtype,\n",
    "    int_dtype: torch.dtype,\n",
    "    validate_data: bool,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Read raw data from file\"\"\"\n",
    "\n",
    "    # create an empty dict to hold the data\n",
    "    data: dict[str, Any] = dict()\n",
    "\n",
    "    # open the correct cset group. We always get the whole store, so we are not\n",
    "    # dependent on a root group being present. Julia does not build one by default\n",
    "    cset_group = zarr.open_group(store, path = f\"cset_{idx+1}\", mode=\"r\")\n",
    "\n",
    "    # read the data from the cset group into the data dict. This reads the entire group.\n",
    "    QG.zarr_group_to_dict(\n",
    "        cset_group, data\n",
    "    )  # insert name of cset group to read\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# transform into a Data object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The second function takes this dictionary and transforms it into a torch_geometric Data object. This function has a fixed signature that needs to be adhered to: \n",
    "```python\n",
    "def collect_data(data: dict,) -> Data\n",
    "```\n",
    "\n",
    "i.e., it transforms a dictionary into a Data object. \n",
    "What it does internally with the data dictionary is up to the author of the function. In our case, we extract the adjacency matrix, node features, and labels from the dictionary and convert them into the appropriate format for a Data object.\n",
    "However, we can also do more complex things, like computing eigenvalues, normalizing features, or adding additional attributes to the Data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_data(data: dict) -> Data:\n",
    "    \"\"\"transform the data dictionary into a PyG Data object and cache the result on disk\"\"\"\n",
    "\n",
    "    adj = data[\"linkmat\"]\n",
    "\n",
    "    # normally, a transpose would be necessary here b/c Julia stores column major\n",
    "    # and Python/numpy/torch row major by default but torch sparse does a transpose\n",
    "    # again, so we can leave it as is...\n",
    "    edge_index, _ = dense_to_sparse(torch.tensor(adj, dtype=torch.float32))\n",
    "\n",
    "    # in and out degrees for the degree labels of the link matrix\n",
    "    x = torch.tensor(\n",
    "        np.array([\n",
    "            data[\"in_degrees_link\"],\n",
    "            data[\"out_degrees_link\"],\n",
    "        ]),\n",
    "        dtype=torch.float32,\n",
    "    ).t()  # shape [N,num_labels]\n",
    "\n",
    "    # manifold like as target\n",
    "    y = torch.tensor(\n",
    "        np.array([\n",
    "            data[\"manifold_like\"],\n",
    "        ]),\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    # build the final PyG data object\n",
    "    tgdata = Data(\n",
    "        edge_index=edge_index,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        csettype = torch.tensor(data[\"cset_type\"]),\n",
    "        atom_count=torch.tensor(adj.shape[0]),\n",
    "    )\n",
    "\n",
    "    if not tgdata.validate():\n",
    "        raise ValueError(f\"Data validation failed for {tgdata}.\")\n",
    "\n",
    "    return tgdata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Why the split? Because you have a choice of when to apply the data transformations. You can do it once when reading the data from disk and cache the result (then it's called `pre_transform`) or you can do it on-the-fly during training (then it's called `transform`). What you choose depends on the complexity of the transformations and the size of your dataset, but also on the workflow you are using. If you are experimenting more on the data side, it can be more useful to do it on-the-fly. If the transformations are expensive, it can be better to do it once and cache the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Next, we need a set of functions that define the loss function to use to train the model, and functions that compute the monitoring metrics from the model outputs that we want to track during validation and testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# define loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "First, we define the loss function to use during training. In this case, we use the standard binary cross-entropy loss for binary classification tasks. The loss function takes the model outputs and the true labels as inputs and computes the loss value. It must be noted that the model outputs a dictionary of tensors (one per task head), so we need to extract the relevant tensor for the loss computation. \n",
    "\n",
    "Note that the loss function must be appropriate for the task at hand. For multi-class classification, for example, you would use a different loss function, such as cross-entropy loss. \n",
    "\n",
    "Here, we also added checks for not-a-number (NaN) values in the model outputs and labels to ensure that the loss computation is valid. If NaN values are detected, an error is raised to prevent invalid computations during training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(\n",
    "    predictions: dict[Any, torch.Tensor],\n",
    "    data: Data,\n",
    "    *args,\n",
    "    **kwargs,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute loss for a single classifier task at index 0\"\"\"\n",
    "\n",
    "    pred = predictions[0]\n",
    "    tgt = data.y\n",
    "\n",
    "    # check for nans -> comment out for speed, this is python overhead\n",
    "    if torch.isnan(pred).any():\n",
    "        raise ValueError(\"Nan in predictions\")\n",
    "\n",
    "    if torch.isnan(tgt).any():\n",
    "        raise ValueError(\"Nan in targets\")\n",
    "\n",
    "    return torch.nn.BCEWithLogitsLoss(\n",
    "        reduction=\"mean\",\n",
    "    )(pred, tgt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Monitor functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Just like the loss function computes how 'well' a model is performing during training in order to compute the gradients for optimization, monitor functions compute metrics that help us understand how well the model is performing on validation and test datasets without affecting the training process itself. These therefore use metrics that measure model performance more directly, such as accuracy, precision, recall, F1-score, etc. \n",
    "\n",
    "In the same way as the loss function, the monitor functions take the model outputs and true labels as inputs and compute the desired metrics. Again, we need to extract the relevant tensor from the model outputs for the computation.\n",
    "\n",
    "Here we monitor two versions of the [f1 score](https://en.wikipedia.org/wiki/F-score), a standard metric for classification tasks. We compute the weighted f1 score, which takes into account class imbalances by weighting the contribution of each class according to its prevalence in the dataset. This is useful when dealing with imbalanced datasets where some classes are more frequent than others. This is done in the function `f1_monitor`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_monitor(\n",
    "    predictions: list[dict[Any, torch.Tensor]],\n",
    "    targets: list[torch.Tensor],\n",
    ") -> float:\n",
    "    \"\"\"Compute F1 score for a single classifier task (key 0).\"\"\"\n",
    "\n",
    "    logits_list: list[torch.Tensor] = []\n",
    "    for p in predictions:\n",
    "        if not isinstance(p, dict):\n",
    "            raise TypeError(f\"Expected dict of tensors, got {type(p)}\")\n",
    "        if 0 in p:\n",
    "            t = p[0]\n",
    "        elif \"0\" in p:\n",
    "            t = p[\"0\"]\n",
    "        else:\n",
    "            t = next(iter(p.values()))\n",
    "        logits_list.append(t.detach())\n",
    "\n",
    "    pred_logits = torch.cat([t.reshape(-1) for t in logits_list])\n",
    "    tgt_vec = torch.cat([t.detach().reshape(-1) for t in targets])\n",
    "\n",
    "    pred_labels = (torch.sigmoid(pred_logits) >= 0.5).to(torch.int32).cpu().numpy()\n",
    "    tgt_labels = tgt_vec.to(torch.int32).cpu().numpy()\n",
    "\n",
    "    return f1_score(tgt_labels, pred_labels, average=\"weighted\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "In addition, we monitor the f1 score per class, which provides insights into how well the model performs on each individual class. This is particularly useful for identifying classes that may be more challenging for the model to classify correctly. This is done in the function `f1_monitor_perclass` below. The only difference to the previous function is that we do not use the `average='weighted'` option in the sklearn f1_score function, which computes the f1 score for each class separately. \n",
    "\n",
    "The disadvantage of this approach is that we need to do some work twice, the assembling of the true and predicted labels into numpy arrays for instance. QuantumGrav allows you to build your own evaluator classes however so if this becomes a bottleneck you can implement a custom evaluator that computes both metrics in one pass for instance. This is covered elsewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f1_monitor_perclass(\n",
    "    predictions: list[dict[Any, torch.Tensor]],\n",
    "    targets: list[torch.Tensor],\n",
    ") -> float:\n",
    "    \"\"\"Compute F1 score for a single classifier task (key 0).\"\"\"\n",
    "\n",
    "    logits_list: list[torch.Tensor] = []\n",
    "    for p in predictions:\n",
    "        if not isinstance(p, dict):\n",
    "            raise TypeError(f\"Expected dict of tensors, got {type(p)}\")\n",
    "        if 0 in p:\n",
    "            t = p[0]\n",
    "        elif \"0\" in p:\n",
    "            t = p[\"0\"]\n",
    "        else:\n",
    "            t = next(iter(p.values()))\n",
    "        logits_list.append(t.detach())\n",
    "\n",
    "    pred_logits = torch.cat([t.reshape(-1) for t in logits_list])\n",
    "    tgt_vec = torch.cat([t.detach().reshape(-1) for t in targets])\n",
    "\n",
    "    pred_labels = (torch.sigmoid(pred_logits) >= 0.5).to(torch.int32).cpu().numpy()\n",
    "    tgt_labels = tgt_vec.to(torch.int32).cpu().numpy()\n",
    "\n",
    "    return f1_score(tgt_labels, pred_labels, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Setup trainer and datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "The trainer class is build from a config file that defines the model, optimizer, training epochs, dataset, evaluators and early stopping system as well as the loss function - everything we need to build a functioning training machinery. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(\"../configs/train_classifier.yaml\")\n",
    "\n",
    "with open(config_path, \"r\") as configfile:\n",
    "    config = yaml.load(configfile, QG.get_loader())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "let's have a look at how that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(config[\"training\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(config[\"validation\"][\"validator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(config[\"testing\"][\"tester\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(config[\"early_stopping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up trainer\n",
    "trainer = QG.Trainer.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# Prepare dataset and dataloaders with weighted sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "This time, we explicitly first prepare the datasets by calling `trainer.prepare_dataset()`. This returns the train, validation, and test datasets that we can use to build our dataloaders. We will build a weighted sampler for the training dataloader to deal with imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = trainer.prepare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "first, we need to build the weights to use for the sampler. We do this by computing the class weights based on the frequency of each class in the training dataset. The weights are then assigned to each sample in the dataset based on its class label. For the binary classificatoin task we have (manifold-like or not) we only have two classes, so we compute the weights for both classes and assign them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect training labels for weighted sampler\n",
    "train_labels = torch.tensor(\n",
    "                [train_dataset[i].y[:, 0].long().item() for i in range(len(train_dataset))]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "next, we count how many we have per class and then compute the weights as the inverse of these counts. Finally, we assign the weights to each sample in the training dataset based on its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = torch.bincount(train_labels)\n",
    "class_weights = 1.0 - class_counts.float() / len(train_dataset)\n",
    "sample_weights = class_weights[train_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "finally, we set up the sampler. Note that we only use the training dataset for weighted sampling - we only need to oversample during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "... and now pass it to the dataloader preparation function. We also pass in the already build train-, validation- and test datasets.  Note that the 'shuffle' parameter cannot be used when using the weighted sampler, because the sampler already defines the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloaders\n",
    "train_loader, valid_loader, test_loader = trainer.prepare_dataloaders(train_dataset = train_dataset, val_dataset=valid_dataset, test_dataset=test_dataset, training_sampler=weighted_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "We have a dataset of 55000 datapoints in this case, and split this into 80% training-, 10% validation and 10% testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "Have a look [at the pytorch data documentation](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.SequentialSampler) to see what other kind of samplers are available, or how to build your own if needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "# run training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Now, the training is run the usual way: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "trainer.initialize_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimizer\n",
    "trainer.initialize_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results, validation_results = trainer.run_training(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=valid_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "After training, the model is tested on an unseen dataset. The results informs us about how well the model generalizes to unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.run_test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "testing results are somewhat mediocre so our model is not doing too well.. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
