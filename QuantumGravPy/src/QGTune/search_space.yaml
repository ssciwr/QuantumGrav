model:
  name: "QuantumGravSearchSpace"
  gcn_net:
    - in_dim: 12 # number of node features
      out_dim: [128, 256]
      dropout:
        type: tuple # to distinguish from categorical
        value: [0.2, 0.5, 0.1] # range for dropout, min, max, step
      gnn_layer_type: ["sage", "gcn", "gat", "gco"]
      normalizer: ["batch_norm", "identity", "layer_norm"]
      activation: ["relu", "leaky_relu", "sigmoid", "tanh", "identity"]
      norm_args: 
        - 128 # should match out_dim, manually set later
      gnn_layer_kwargs:
        normalize: False
        bias: True
        project: False
        root_weight: False
        aggr: "mean"
    - in_dim: 128 # should match previous layer's out_dim, manually set later
      out_dim: [256, 512]
      dropout:
        type: tuple
        value: [0.2, 0.5, 0.1]
      gnn_layer_type: ["sage", "gcn", "gat", "gco"]
      normalizer: ["batch_norm", "identity", "layer_norm"]
      activation: ["relu", "leaky_relu", "sigmoid", "tanh", "identity"]
      norm_args: 
        - 256 # should match out_dim, manually set later
      gnn_layer_kwargs:
        normalize: False
        bias: True
        project: False
        root_weight: False
        aggr: "mean"
    - in_dim: 256 # should match previous layer's out_dim, manually set later
      out_dim: [128, 256]
      dropout:
        type: tuple
        value: [0.2, 0.5, 0.1]
      gnn_layer_type: ["sage", "gcn", "gat", "gco"]
      normalizer: ["batch_norm", "identity", "layer_norm"]
      activation: ["relu", "leaky_relu", "sigmoid", "tanh", "identity"]
      norm_args: 
        - 128 # should match out_dim, manually set later
      gnn_layer_kwargs:
        normalize: False
        bias: True
        project: False
        root_weight: False
        aggr: "mean"
  pooling_layer: ["mean", "max", "sum"]
  classifier:
    input_dim: 128 # should match last gcn_net layer's out_dim, manually set later
    output_dims: 
      - 2 # number of classes in classification task
    hidden_dims: 
      - 48
      - 18
    activation: ["relu", "leaky_relu", "sigmoid", "tanh", "identity"]
    backbone_kwargs: [{}, {}]
    output_kwargs: [{}]
    activation_kwargs: [{ "inplace": False }]

training:
  seed: 42
  # training loop
  device: "cuda"
  early_stopping_patience: 5
  early_stopping_window: 7
  early_stopping_tol: 0.001
  early_stopping_metric: "f1_weighted"
  checkpoint_at: 2
  checkpoint_path: /path/to/where/the/intermediate/models/should/go
  # optimizer
  learning_rate:
    type: tuple
    value: [1e-5, 1e-1, true]
  weight_decay:
    type: tuple
    value: [1e-6, 1e-2, true]
  # training loader
  batch_size: [32, 64]
  num_workers: 12
  pin_memory: False
  drop_last: True
  num_epochs: [50, 100, 200]
  split: 0.8
validation: &valtest
  batch_size: 32
  num_workers: 12
  pin_memory: False
  drop_last: True
  shuffle: True
  persistent_workers: True
  split: 0.1
testing: *valtest