{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to the QuantumGrav documentation!","text":"<p>This project is dedicated to providing tools for creating causal sets as used in the corresponding approach to quantum gravity, and to building machine learning systems for analyzing them. Therefore, this project consists of two parts:</p> <ul> <li> <p>A Julia package called <code>QuantumGrav.jl</code> which is build on top of <code>CausalSets.jl</code>, which creates a causal sets of a different varieties (manifold-like, random non-manifold like, non-causal-set DAGs). For now, manifold-like causal sets are restricted to 2D. This package also provides functions for deriving a set of quantities from the graph-level properties of the produced causal sets. It also allows for storing the data in  <code>Zarr</code> files.</p> </li> <li> <p>A Python package called <code>QuantumGravPy</code> which is based on <code>pytorch-geometric</code> and <code>zarr</code>. This package  is thus responsible for the data preprocessing, and model training. This package is based on a configuration-code separation in which you will define your model using YAML files and only supply code where the predefined abstractions do not suffice.</p> </li> </ul> <p>Start with the Getting started page to get up and running.</p> <p>For the Python package <code>QuantumGravPy</code>, the <code>Datasets and Preprocessing</code> section will show you how to use the supplied dataset classes for processing your raw data. Next, you should learn about the model architecture used in this package in <code>Graph Neural Network models</code>. To learn how to train a model, check out the <code>Model training</code> section. Finally, the <code>API documentation</code> will tell you everything you need to know about the source code of the package.</p> <p>Note that the two packages are designed to function in unison, with the Julia package producing data that the python package consumes.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#main-model-class","title":"Main model class","text":"<p>The main model class <code>GNNModel</code> is there to tie together the Graph neural network backbone and a multilayer perceptron classifier model that can be configured for various tasks.  </p>"},{"location":"api/#QuantumGrav.gnn_model.GNNModel","title":"<code>GNNModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Torch module for the full GCN model, which consists of a GCN backbone, a set of downstream tasks, and a pooling layer, augmented with optional graph features network. Args:     torch.nn.Module: base class</p> Source code in <code>src/QuantumGrav/gnn_model.py</code> <pre><code>class GNNModel(torch.nn.Module):\n    \"\"\"Torch module for the full GCN model, which consists of a GCN backbone, a set of downstream tasks, and a pooling layer, augmented with optional graph features network.\n    Args:\n        torch.nn.Module: base class\n    \"\"\"\n\n    # FIXME: change this such that all elements are build internally, and only types are passed in\n    # this is more in line with the other parts of the system and also with how the configs work\n    def __init__(\n        self,\n        encoder: Sequence[QGGNN.GNNBlock],\n        downstream_tasks: Sequence[torch.nn.Module],\n        pooling_layers: Sequence[torch.nn.Module] | None = None,\n        aggregate_pooling: torch.nn.Module | Callable | None = None,\n        graph_features_net: torch.nn.Module | None = None,\n        aggregate_graph_features: torch.nn.Module | Callable | None = None,\n        active_tasks: list[int] | None = None,\n    ):\n        \"\"\"Initialize the GNNModel.\n\n        Args:\n            encoder (GCNBackbone): GCN backbone network.\n            downstream_tasks (Sequence[torch.nn.Module]): Downstream task blocks. These are assumed to be independent of each other.\n            pooling_layers (Sequence[torch.nn.Module]): Pooling layers. Defaults to None.\n            aggregate_pooling (torch.nn.Module | Callable | None): Aggregation of pooling layer output. Defaults to None.\n            graph_features_net (torch.nn.Module, optional): Graph features network. Defaults to None.\n            aggregate_graph_features (torch.nn.Module | Callable | None): Aggregation of graph features. Defaults to None.\n        \"\"\"\n        super().__init__()\n\n        # encoder is a sequence of GNN blocks. There must be at least one\n        self.encoder = torch.nn.ModuleList(encoder)\n\n        if len(self.encoder) == 0:\n            raise ValueError(\"At least one GNN block must be provided.\")\n\n        # set up downstream tasks. These are independent of each other, but there must be one at least\n        self.downstream_tasks = torch.nn.ModuleList(downstream_tasks)\n\n        if len(self.downstream_tasks) == 0:\n            raise ValueError(\"At least one downstream task must be provided.\")\n\n        if active_tasks is None:\n            raise ValueError(\"active_tasks must be provided.\")\n        else:\n            self.active_tasks = active_tasks\n\n        # set up pooling layers and their aggregation\n        if pooling_layers is not None:\n            if len(pooling_layers) == 0:\n                raise ValueError(\"At least one pooling layer must be provided.\")\n\n            self.pooling_layers = torch.nn.ModuleList(\n                [\n                    p\n                    if isclass(type(p)) and issubclass(type(p), torch.nn.Module)\n                    else ModuleWrapper(p)\n                    for p in pooling_layers\n                ]\n            )\n        else:\n            self.pooling_layers = None\n\n        # aggregate pooling layer\n        self.aggregate_pooling = aggregate_pooling\n\n        if aggregate_pooling is not None:\n            if not isclass(aggregate_pooling) or not issubclass(\n                aggregate_pooling, torch.nn.Module\n            ):\n                self.aggregate_pooling = ModuleWrapper(aggregate_pooling)\n\n        pooling_funcs = [self.aggregate_pooling, self.pooling_layers]\n        if any([p is not None for p in pooling_funcs]) and not all(\n            p is not None for p in pooling_funcs\n        ):\n            raise ValueError(\n                \"If pooling layers are to be used, both an aggregate pooling method and pooling layers must be provided.\"\n            )\n\n        # set up graph features processing if provided\n        self.graph_features_net = graph_features_net\n\n        self.aggregate_graph_features = aggregate_graph_features\n\n        if aggregate_graph_features is not None:\n            if not isclass(aggregate_graph_features) or not issubclass(\n                aggregate_graph_features, torch.nn.Module\n            ):\n                self.aggregate_graph_features = ModuleWrapper(aggregate_graph_features)\n\n        graph_processors = [self.graph_features_net, self.aggregate_graph_features]\n\n        if any([g is not None for g in graph_processors]) and not all(\n            g is not None for g in graph_processors\n        ):\n            raise ValueError(\n                \"If graph features are to be used, both a graph features network and an aggregation method must be provided.\"\n            )\n\n    def set_task_active(self, i: int) -&gt; None:\n        \"\"\"Set a downstream task as active.\n\n        Args:\n            i (int): Index of the downstream task to activate.\n        \"\"\"\n\n        if i &lt; 0 or i &gt;= len(self.active_tasks):\n            raise ValueError(\"Invalid task index.\")\n\n        self.active_tasks[i] = True\n\n    def set_task_inactive(self, i: int) -&gt; None:\n        \"\"\"Set a downstream task as inactive.\n\n        Args:\n            i (int): Index of the downstream task to deactivate.\n        \"\"\"\n\n        if i &lt; 0 or i &gt;= len(self.active_tasks):\n            raise ValueError(\"Invalid task index.\")\n\n        self.active_tasks[i] = False\n\n    def eval_encoder(\n        self,\n        x: torch.Tensor,\n        edge_index: torch.Tensor,\n        gcn_kwargs: dict[Any, Any] | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Evaluate the GCN network on the input data.\n\n        Args:\n            x (torch.Tensor): Input node features.\n            edge_index (torch.Tensor): Graph connectivity information.\n            gcn_kwargs (dict[Any, Any], optional): Additional arguments for the GCN. Defaults to None.\n\n        Returns:\n            torch.Tensor: Output of the GCN network.\n        \"\"\"\n        # Apply each GCN layer to the input features\n        features = x\n        for gnn_layer in self.encoder:\n            features = gnn_layer(\n                features, edge_index, **(gcn_kwargs if gcn_kwargs else {})\n            )\n        return features\n\n    def get_embeddings(\n        self,\n        x: torch.Tensor,\n        edge_index: torch.Tensor,\n        batch: torch.Tensor | None = None,\n        gcn_kwargs: dict | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Get embeddings from the GCN model.\n\n        Args:\n            x (torch.Tensor): Input node features.\n            edge_index (torch.Tensor): Graph connectivity information.\n            batch (torch.Tensor): Batch vector for pooling.\n            gcn_kwargs (dict, optional): Additional arguments for the GCN. Defaults to None.\n\n        Returns:\n            torch.Tensor: Embedding vector for the graph features.\n        \"\"\"\n        # apply the GCN backbone to the node features\n        embeddings = self.eval_encoder(\n            x, edge_index, **(gcn_kwargs if gcn_kwargs else {})\n        )\n\n        # pool everything together into a single graph representation\n        if self.pooling_layers is not None and self.aggregate_pooling is not None:\n            pooled_embeddings = [\n                pooling_op(embeddings, batch) for pooling_op in self.pooling_layers\n            ]\n\n            return self.aggregate_pooling(pooled_embeddings)\n        else:\n            return embeddings\n\n    def compute_downstream_tasks(\n        self,\n        x: torch.Tensor,\n        downstream_task_args: Sequence[tuple | list] | None = None,\n        downstream_task_kwargs: Sequence[dict] | None = None,\n    ) -&gt; dict[int, torch.Tensor]:\n        \"\"\"Compute the outputs of the downstream tasks. Only the active tasks will be computed.\n\n        Args:\n            x (torch.Tensor): Input embeddings tensor\n            downstream_task_args (Sequence[tuple | list] | None, optional): Arguments for downstream tasks. Defaults to None.\n            downstream_task_kwargs (Sequence[dict] | None, optional): Keyword arguments for downstream tasks. Defaults to None.\n\n        Returns:\n            dict[int, torch.Tensor]: Outputs of the downstream tasks.\n        \"\"\"\n\n        output = {}\n\n        for i in range(len(self.downstream_tasks)):\n            if self.active_tasks[i]:\n                task = self.downstream_tasks[i]\n\n                task_args = []\n                task_kwargs = {}\n                if (\n                    downstream_task_args is not None\n                    and i &lt; len(downstream_task_args)\n                    and downstream_task_args[i]\n                ):\n                    task_args = downstream_task_args[i]\n\n                if (\n                    downstream_task_kwargs is not None\n                    and i &lt; len(downstream_task_kwargs)\n                    and downstream_task_kwargs[i]\n                ):\n                    task_kwargs = downstream_task_kwargs[i]\n\n                res = task(x, *task_args, **task_kwargs)\n                output[i] = res\n\n        return output\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        edge_index: torch.Tensor,\n        batch: torch.Tensor,\n        graph_features: torch.Tensor | None = None,\n        downstream_task_args: Sequence[tuple | list] | None = None,\n        downstream_task_kwargs: Sequence[dict] | None = None,\n        embedding_kwargs: dict[Any, Any] | None = None,\n    ) -&gt; dict[int, torch.Tensor]:\n        \"\"\"Forward run of the gnn model with optional graph features.\n        # First execute the graph-neural network backbone, then process the graph features, and finally apply the downstream tasks.\n\n        Args:\n            x (torch.Tensor): Input node features.\n            edge_index (torch.Tensor): Graph connectivity information.\n            batch (torch.Tensor): Batch vector for pooling.\n            graph_features (torch.Tensor | None, optional): Additional graph features. Defaults to None.\n            downstream_task_args (Sequence[tuple] | None, optional): Arguments for downstream tasks. Defaults to None.\n            downstream_task_kwargs (Sequence[dict] | None, optional): Keyword arguments for downstream tasks. Defaults to None.\n            embedding_kwargs (dict[Any, Any] | None, optional): Additional arguments for the GCN. Defaults to None.\n\n        Returns:\n            Sequence[torch.Tensor]: Raw output of downstream tasks.\n        \"\"\"\n        # apply the GCN backbone to the node features\n\n        embeddings = self.get_embeddings(\n            x, edge_index, batch, gcn_kwargs=embedding_kwargs\n        )\n\n        # If we have graph features, we need to process them and concatenate them with the node features\n        if graph_features is not None and self.graph_features_net is not None:\n            graph_features = self.graph_features_net(graph_features)\n\n        if self.aggregate_graph_features is not None and graph_features is not None:\n            embeddings = self.aggregate_graph_features(embeddings, graph_features)\n\n        # downstream tasks are given out as is, no softmax or other assumptions\n        return self.compute_downstream_tasks(\n            embeddings,\n            downstream_task_args=downstream_task_args,\n            downstream_task_kwargs=downstream_task_kwargs,\n        )\n\n    @classmethod\n    def _cfg_helper(\n        cls, cfg: dict[str, Any], utility_function: Callable, throw_message: str\n    ) -&gt; torch.nn.Module | Callable:\n        \"\"\"Helper function to create a module or callable from a config.\n\n        Args:\n            cfg (dict[str, Any]): config node. must contain type, args, kwargs\n            utility_function (Callable): utility function to create the module or callable\n            throw_message (str): message to throw in case of error\n\n        Raises:\n            ValueError: if the config node is invalid\n            ValueError: if the utility function could not find the specified type\n\n        Returns:\n            torch.nn.Module | Callable: created module or callable\n        \"\"\"\n        if not utils.verify_config_node(cfg):\n            raise ValueError(throw_message)\n        f = utility_function(cfg[\"type\"])\n\n        if f is None:\n            raise ValueError(\n                f\"Utility function '{utility_function.__name__}' could not find '{cfg['type']}'\"\n            )\n\n        if isinstance(f, type):\n            return f(\n                cfg[\"args\"] if \"args\" in cfg else [],\n                **(cfg[\"kwargs\"] if \"kwargs\" in cfg else {}),\n            )\n        else:\n            return f\n\n    @classmethod\n    def from_config(cls, config: dict) -&gt; \"GNNModel\":\n        \"\"\"Create a GNNModel from a configuration dictionary.\n\n        Args:\n            config (dict): Configuration dictionary containing parameters for the model.\n\n        Returns:\n            GNNModel: An instance of GNNModel.\n        \"\"\"\n\n        # create encoder\n        encoder = [QGGNN.GNNBlock.from_config(cfg) for cfg in config[\"encoder\"]]\n\n        # create downstream tasks\n        downstream_tasks = [\n            QGLS.LinearSequential.from_config(cfg) for cfg in config[\"downstream_tasks\"]\n        ]  # TODO: generalize this!\n\n        # make pooling layers\n        pooling_layers_cfg = config.get(\"pooling_layers\", None)\n\n        if pooling_layers_cfg is not None:\n            pooling_layers = []\n            for pool_cfg in pooling_layers_cfg:\n                pooling_layer = cls._cfg_helper(\n                    pool_cfg,\n                    utils.get_registered_pooling_layer,\n                    f\"The config for a pooling layer is invalid: {pool_cfg}\",\n                )\n                pooling_layers.append(pooling_layer)\n        else:\n            pooling_layers = None\n\n        # graph aggregation pooling\n        aggregate_pooling_cfg = config.get(\"aggregate_pooling\", None)\n        if aggregate_pooling_cfg is not None:\n            aggregate_pooling = cls._cfg_helper(\n                config[\"aggregate_pooling\"],\n                utils.get_pooling_aggregation,\n                f\"The config for 'aggregate_pooling' is invalid: {config['aggregate_pooling']}\",\n            )\n        else:\n            aggregate_pooling = None\n\n        # make graph features network and aggregations\n        if \"graph_features_net\" in config and config[\"graph_features_net\"] is not None:\n            graph_features_net = QGLS.LinearSequential.from_config(\n                config[\"graph_features_net\"]\n            )\n        else:\n            graph_features_net = None\n\n        if graph_features_net is not None:\n            aggregate_graph_features = cls._cfg_helper(\n                config[\"aggregate_graph_features\"],\n                utils.get_graph_features_aggregation,\n                f\"The config for 'aggregate_graph_features' is invalid: {config['aggregate_graph_features']}\",\n            )\n        else:\n            aggregate_graph_features = None\n\n        active_tasks = [cfg.get(\"active\", False) for cfg in config[\"downstream_tasks\"]]\n\n        # return the model\n        return cls(\n            encoder=encoder,\n            downstream_tasks=downstream_tasks,\n            pooling_layers=pooling_layers,\n            graph_features_net=graph_features_net,\n            aggregate_graph_features=aggregate_graph_features,\n            aggregate_pooling=aggregate_pooling,\n            active_tasks=active_tasks,\n        )\n\n    def to_config(self) -&gt; dict[str, Any]:\n        \"\"\"Serialize the model to a config\n\n        Returns:\n            dict[str, Any]: _description_\n        \"\"\"\n        pooling_layer_names = None\n        if self.pooling_layers is not None:\n            pooling_layer_names = []\n            for layer in self.pooling_layers:\n                if isinstance(layer, ModuleWrapper):\n                    pooling_layer_names.append(\n                        {\n                            \"type\": utils.pooling_layers_names[layer.get_fn()],\n                            \"args\": [],\n                            \"kwargs\": {},\n                        }\n                    )\n                else:\n                    pooling_layer_names.append(\n                        {\n                            \"type\": utils.pooling_layers_names[layer],\n                            \"args\": [],\n                            \"kwargs\": {},\n                        }\n                    )\n\n        aggregate_graph_features_names = None\n        if self.aggregate_graph_features is not None:\n            if isinstance(self.aggregate_graph_features, ModuleWrapper):\n                aggregate_graph_features_names = {\n                    \"type\": utils.graph_features_aggregations_names[\n                        self.aggregate_graph_features.get_fn()\n                    ],\n                    \"args\": [],\n                    \"kwargs\": {},\n                }\n            else:\n                aggregate_graph_features_names = {\n                    \"type\": utils.graph_features_aggregations_names[\n                        self.aggregate_graph_features\n                    ],\n                    \"args\": [],\n                    \"kwargs\": {},\n                }\n\n        aggregate_pooling_names = None\n        if self.aggregate_pooling is not None:\n            if isinstance(self.aggregate_pooling, ModuleWrapper):\n                aggregate_pooling_names = {\n                    \"type\": utils.pooling_aggregations_names[\n                        self.aggregate_pooling.get_fn()\n                    ],\n                    \"args\": [],\n                    \"kwargs\": {},\n                }\n            else:\n                aggregate_pooling_names = {\n                    \"type\": utils.pooling_aggregations_names[self.aggregate_pooling],\n                    \"args\": [],\n                    \"kwargs\": {},\n                }\n\n        # downstream_task_configs\n\n        downstream_task_configs = [task.to_config() for task in self.downstream_tasks]\n        for i in range(len(self.downstream_tasks)):\n            downstream_task_configs[i][\"active\"] = self.active_tasks[i]\n\n        config = {\n            \"encoder\": [encoder_layer.to_config() for encoder_layer in self.encoder],\n            \"downstream_tasks\": downstream_task_configs,\n            \"pooling_layers\": pooling_layer_names,\n            \"graph_features_net\": self.graph_features_net.to_config()\n            if self.graph_features_net\n            else None,\n            \"aggregate_graph_features\": aggregate_graph_features_names,\n            \"aggregate_pooling\": aggregate_pooling_names,\n            \"active_tasks\": self.active_tasks,\n        }\n\n        return config\n\n    def save(self, path: str | Path) -&gt; None:\n        \"\"\"Save the model state to file. This saves a dictionary structured like this:\n         'encoder': self.encoder,\n         'downstream_tasks': self.downstream_tasks,\n         'pooling_layers': self.pooling_layers,\n         'graph_features_net': self.graph_features_net,\n         'aggregate_graph_features': self.aggregate_graph_features,\n         'aggregate_pooling': self.aggregate_pooling,\n\n        Args:\n            path (str | Path): Path to save the model to\n        \"\"\"\n\n        config = self.to_config()\n\n        torch.save(\n            {\"config\": config, \"model\": self.state_dict()},\n            path,\n        )\n\n    @classmethod\n    def load(\n        cls, path: str | Path, device: torch.device = torch.device(\"cpu\")\n    ) -&gt; \"GNNModel\":\n        \"\"\"Load a model from file that has previously been save with the function 'save'.\n\n        Args:\n            path (str | Path): path to load the model from.\n            device (torch.device): device to put the model to. Defaults to torch.device(\"cpu\")\n        Returns:\n            GNNModel: model instance initialized with the sub-models loaded from file.\n        \"\"\"\n        model_dict = torch.load(path, weights_only=False)\n        model = cls.from_config(model_dict[\"config\"]).to(device)\n        model.load_state_dict(model_dict[\"model\"])\n\n        return model\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_model.GNNModel.__init__","title":"<code>__init__(encoder, downstream_tasks, pooling_layers=None, aggregate_pooling=None, graph_features_net=None, aggregate_graph_features=None, active_tasks=None)</code>","text":"<p>Initialize the GNNModel.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>GCNBackbone</code> <p>GCN backbone network.</p> required <code>downstream_tasks</code> <code>Sequence[Module]</code> <p>Downstream task blocks. These are assumed to be independent of each other.</p> required <code>pooling_layers</code> <code>Sequence[Module]</code> <p>Pooling layers. Defaults to None.</p> <code>None</code> <code>aggregate_pooling</code> <code>Module | Callable | None</code> <p>Aggregation of pooling layer output. Defaults to None.</p> <code>None</code> <code>graph_features_net</code> <code>Module</code> <p>Graph features network. Defaults to None.</p> <code>None</code> <code>aggregate_graph_features</code> <code>Module | Callable | None</code> <p>Aggregation of graph features. Defaults to None.</p> <code>None</code> Source code in <code>src/QuantumGrav/gnn_model.py</code> <pre><code>def __init__(\n    self,\n    encoder: Sequence[QGGNN.GNNBlock],\n    downstream_tasks: Sequence[torch.nn.Module],\n    pooling_layers: Sequence[torch.nn.Module] | None = None,\n    aggregate_pooling: torch.nn.Module | Callable | None = None,\n    graph_features_net: torch.nn.Module | None = None,\n    aggregate_graph_features: torch.nn.Module | Callable | None = None,\n    active_tasks: list[int] | None = None,\n):\n    \"\"\"Initialize the GNNModel.\n\n    Args:\n        encoder (GCNBackbone): GCN backbone network.\n        downstream_tasks (Sequence[torch.nn.Module]): Downstream task blocks. These are assumed to be independent of each other.\n        pooling_layers (Sequence[torch.nn.Module]): Pooling layers. Defaults to None.\n        aggregate_pooling (torch.nn.Module | Callable | None): Aggregation of pooling layer output. Defaults to None.\n        graph_features_net (torch.nn.Module, optional): Graph features network. Defaults to None.\n        aggregate_graph_features (torch.nn.Module | Callable | None): Aggregation of graph features. Defaults to None.\n    \"\"\"\n    super().__init__()\n\n    # encoder is a sequence of GNN blocks. There must be at least one\n    self.encoder = torch.nn.ModuleList(encoder)\n\n    if len(self.encoder) == 0:\n        raise ValueError(\"At least one GNN block must be provided.\")\n\n    # set up downstream tasks. These are independent of each other, but there must be one at least\n    self.downstream_tasks = torch.nn.ModuleList(downstream_tasks)\n\n    if len(self.downstream_tasks) == 0:\n        raise ValueError(\"At least one downstream task must be provided.\")\n\n    if active_tasks is None:\n        raise ValueError(\"active_tasks must be provided.\")\n    else:\n        self.active_tasks = active_tasks\n\n    # set up pooling layers and their aggregation\n    if pooling_layers is not None:\n        if len(pooling_layers) == 0:\n            raise ValueError(\"At least one pooling layer must be provided.\")\n\n        self.pooling_layers = torch.nn.ModuleList(\n            [\n                p\n                if isclass(type(p)) and issubclass(type(p), torch.nn.Module)\n                else ModuleWrapper(p)\n                for p in pooling_layers\n            ]\n        )\n    else:\n        self.pooling_layers = None\n\n    # aggregate pooling layer\n    self.aggregate_pooling = aggregate_pooling\n\n    if aggregate_pooling is not None:\n        if not isclass(aggregate_pooling) or not issubclass(\n            aggregate_pooling, torch.nn.Module\n        ):\n            self.aggregate_pooling = ModuleWrapper(aggregate_pooling)\n\n    pooling_funcs = [self.aggregate_pooling, self.pooling_layers]\n    if any([p is not None for p in pooling_funcs]) and not all(\n        p is not None for p in pooling_funcs\n    ):\n        raise ValueError(\n            \"If pooling layers are to be used, both an aggregate pooling method and pooling layers must be provided.\"\n        )\n\n    # set up graph features processing if provided\n    self.graph_features_net = graph_features_net\n\n    self.aggregate_graph_features = aggregate_graph_features\n\n    if aggregate_graph_features is not None:\n        if not isclass(aggregate_graph_features) or not issubclass(\n            aggregate_graph_features, torch.nn.Module\n        ):\n            self.aggregate_graph_features = ModuleWrapper(aggregate_graph_features)\n\n    graph_processors = [self.graph_features_net, self.aggregate_graph_features]\n\n    if any([g is not None for g in graph_processors]) and not all(\n        g is not None for g in graph_processors\n    ):\n        raise ValueError(\n            \"If graph features are to be used, both a graph features network and an aggregation method must be provided.\"\n        )\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_model.GNNModel.compute_downstream_tasks","title":"<code>compute_downstream_tasks(x, downstream_task_args=None, downstream_task_kwargs=None)</code>","text":"<p>Compute the outputs of the downstream tasks. Only the active tasks will be computed.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input embeddings tensor</p> required <code>downstream_task_args</code> <code>Sequence[tuple | list] | None</code> <p>Arguments for downstream tasks. Defaults to None.</p> <code>None</code> <code>downstream_task_kwargs</code> <code>Sequence[dict] | None</code> <p>Keyword arguments for downstream tasks. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[int, Tensor]</code> <p>dict[int, torch.Tensor]: Outputs of the downstream tasks.</p> Source code in <code>src/QuantumGrav/gnn_model.py</code> <pre><code>def compute_downstream_tasks(\n    self,\n    x: torch.Tensor,\n    downstream_task_args: Sequence[tuple | list] | None = None,\n    downstream_task_kwargs: Sequence[dict] | None = None,\n) -&gt; dict[int, torch.Tensor]:\n    \"\"\"Compute the outputs of the downstream tasks. Only the active tasks will be computed.\n\n    Args:\n        x (torch.Tensor): Input embeddings tensor\n        downstream_task_args (Sequence[tuple | list] | None, optional): Arguments for downstream tasks. Defaults to None.\n        downstream_task_kwargs (Sequence[dict] | None, optional): Keyword arguments for downstream tasks. Defaults to None.\n\n    Returns:\n        dict[int, torch.Tensor]: Outputs of the downstream tasks.\n    \"\"\"\n\n    output = {}\n\n    for i in range(len(self.downstream_tasks)):\n        if self.active_tasks[i]:\n            task = self.downstream_tasks[i]\n\n            task_args = []\n            task_kwargs = {}\n            if (\n                downstream_task_args is not None\n                and i &lt; len(downstream_task_args)\n                and downstream_task_args[i]\n            ):\n                task_args = downstream_task_args[i]\n\n            if (\n                downstream_task_kwargs is not None\n                and i &lt; len(downstream_task_kwargs)\n                and downstream_task_kwargs[i]\n            ):\n                task_kwargs = downstream_task_kwargs[i]\n\n            res = task(x, *task_args, **task_kwargs)\n            output[i] = res\n\n    return output\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_model.GNNModel.eval_encoder","title":"<code>eval_encoder(x, edge_index, gcn_kwargs=None)</code>","text":"<p>Evaluate the GCN network on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input node features.</p> required <code>edge_index</code> <code>Tensor</code> <p>Graph connectivity information.</p> required <code>gcn_kwargs</code> <code>dict[Any, Any]</code> <p>Additional arguments for the GCN. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output of the GCN network.</p> Source code in <code>src/QuantumGrav/gnn_model.py</code> <pre><code>def eval_encoder(\n    self,\n    x: torch.Tensor,\n    edge_index: torch.Tensor,\n    gcn_kwargs: dict[Any, Any] | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Evaluate the GCN network on the input data.\n\n    Args:\n        x (torch.Tensor): Input node features.\n        edge_index (torch.Tensor): Graph connectivity information.\n        gcn_kwargs (dict[Any, Any], optional): Additional arguments for the GCN. Defaults to None.\n\n    Returns:\n        torch.Tensor: Output of the GCN network.\n    \"\"\"\n    # Apply each GCN layer to the input features\n    features = x\n    for gnn_layer in self.encoder:\n        features = gnn_layer(\n            features, edge_index, **(gcn_kwargs if gcn_kwargs else {})\n        )\n    return features\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_model.GNNModel.forward","title":"<code>forward(x, edge_index, batch, graph_features=None, downstream_task_args=None, downstream_task_kwargs=None, embedding_kwargs=None)</code>","text":"<p>Forward run of the gnn model with optional graph features.</p>"},{"location":"api/#QuantumGrav.gnn_model.GNNModel.forward--first-execute-the-graph-neural-network-backbone-then-process-the-graph-features-and-finally-apply-the-downstream-tasks","title":"First execute the graph-neural network backbone, then process the graph features, and finally apply the downstream tasks.","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input node features.</p> required <code>edge_index</code> <code>Tensor</code> <p>Graph connectivity information.</p> required <code>batch</code> <code>Tensor</code> <p>Batch vector for pooling.</p> required <code>graph_features</code> <code>Tensor | None</code> <p>Additional graph features. Defaults to None.</p> <code>None</code> <code>downstream_task_args</code> <code>Sequence[tuple] | None</code> <p>Arguments for downstream tasks. Defaults to None.</p> <code>None</code> <code>downstream_task_kwargs</code> <code>Sequence[dict] | None</code> <p>Keyword arguments for downstream tasks. Defaults to None.</p> <code>None</code> <code>embedding_kwargs</code> <code>dict[Any, Any] | None</code> <p>Additional arguments for the GCN. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[int, Tensor]</code> <p>Sequence[torch.Tensor]: Raw output of downstream tasks.</p> Source code in <code>src/QuantumGrav/gnn_model.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    edge_index: torch.Tensor,\n    batch: torch.Tensor,\n    graph_features: torch.Tensor | None = None,\n    downstream_task_args: Sequence[tuple | list] | None = None,\n    downstream_task_kwargs: Sequence[dict] | None = None,\n    embedding_kwargs: dict[Any, Any] | None = None,\n) -&gt; dict[int, torch.Tensor]:\n    \"\"\"Forward run of the gnn model with optional graph features.\n    # First execute the graph-neural network backbone, then process the graph features, and finally apply the downstream tasks.\n\n    Args:\n        x (torch.Tensor): Input node features.\n        edge_index (torch.Tensor): Graph connectivity information.\n        batch (torch.Tensor): Batch vector for pooling.\n        graph_features (torch.Tensor | None, optional): Additional graph features. Defaults to None.\n        downstream_task_args (Sequence[tuple] | None, optional): Arguments for downstream tasks. Defaults to None.\n        downstream_task_kwargs (Sequence[dict] | None, optional): Keyword arguments for downstream tasks. Defaults to None.\n        embedding_kwargs (dict[Any, Any] | None, optional): Additional arguments for the GCN. Defaults to None.\n\n    Returns:\n        Sequence[torch.Tensor]: Raw output of downstream tasks.\n    \"\"\"\n    # apply the GCN backbone to the node features\n\n    embeddings = self.get_embeddings(\n        x, edge_index, batch, gcn_kwargs=embedding_kwargs\n    )\n\n    # If we have graph features, we need to process them and concatenate them with the node features\n    if graph_features is not None and self.graph_features_net is not None:\n        graph_features = self.graph_features_net(graph_features)\n\n    if self.aggregate_graph_features is not None and graph_features is not None:\n        embeddings = self.aggregate_graph_features(embeddings, graph_features)\n\n    # downstream tasks are given out as is, no softmax or other assumptions\n    return self.compute_downstream_tasks(\n        embeddings,\n        downstream_task_args=downstream_task_args,\n        downstream_task_kwargs=downstream_task_kwargs,\n    )\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_model.GNNModel.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create a GNNModel from a configuration dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary containing parameters for the model.</p> required <p>Returns:</p> Name Type Description <code>GNNModel</code> <code>GNNModel</code> <p>An instance of GNNModel.</p> Source code in <code>src/QuantumGrav/gnn_model.py</code> <pre><code>@classmethod\ndef from_config(cls, config: dict) -&gt; \"GNNModel\":\n    \"\"\"Create a GNNModel from a configuration dictionary.\n\n    Args:\n        config (dict): Configuration dictionary containing parameters for the model.\n\n    Returns:\n        GNNModel: An instance of GNNModel.\n    \"\"\"\n\n    # create encoder\n    encoder = [QGGNN.GNNBlock.from_config(cfg) for cfg in config[\"encoder\"]]\n\n    # create downstream tasks\n    downstream_tasks = [\n        QGLS.LinearSequential.from_config(cfg) for cfg in config[\"downstream_tasks\"]\n    ]  # TODO: generalize this!\n\n    # make pooling layers\n    pooling_layers_cfg = config.get(\"pooling_layers\", None)\n\n    if pooling_layers_cfg is not None:\n        pooling_layers = []\n        for pool_cfg in pooling_layers_cfg:\n            pooling_layer = cls._cfg_helper(\n                pool_cfg,\n                utils.get_registered_pooling_layer,\n                f\"The config for a pooling layer is invalid: {pool_cfg}\",\n            )\n            pooling_layers.append(pooling_layer)\n    else:\n        pooling_layers = None\n\n    # graph aggregation pooling\n    aggregate_pooling_cfg = config.get(\"aggregate_pooling\", None)\n    if aggregate_pooling_cfg is not None:\n        aggregate_pooling = cls._cfg_helper(\n            config[\"aggregate_pooling\"],\n            utils.get_pooling_aggregation,\n            f\"The config for 'aggregate_pooling' is invalid: {config['aggregate_pooling']}\",\n        )\n    else:\n        aggregate_pooling = None\n\n    # make graph features network and aggregations\n    if \"graph_features_net\" in config and config[\"graph_features_net\"] is not None:\n        graph_features_net = QGLS.LinearSequential.from_config(\n            config[\"graph_features_net\"]\n        )\n    else:\n        graph_features_net = None\n\n    if graph_features_net is not None:\n        aggregate_graph_features = cls._cfg_helper(\n            config[\"aggregate_graph_features\"],\n            utils.get_graph_features_aggregation,\n            f\"The config for 'aggregate_graph_features' is invalid: {config['aggregate_graph_features']}\",\n        )\n    else:\n        aggregate_graph_features = None\n\n    active_tasks = [cfg.get(\"active\", False) for cfg in config[\"downstream_tasks\"]]\n\n    # return the model\n    return cls(\n        encoder=encoder,\n        downstream_tasks=downstream_tasks,\n        pooling_layers=pooling_layers,\n        graph_features_net=graph_features_net,\n        aggregate_graph_features=aggregate_graph_features,\n        aggregate_pooling=aggregate_pooling,\n        active_tasks=active_tasks,\n    )\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_model.GNNModel.get_embeddings","title":"<code>get_embeddings(x, edge_index, batch=None, gcn_kwargs=None)</code>","text":"<p>Get embeddings from the GCN model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input node features.</p> required <code>edge_index</code> <code>Tensor</code> <p>Graph connectivity information.</p> required <code>batch</code> <code>Tensor</code> <p>Batch vector for pooling.</p> <code>None</code> <code>gcn_kwargs</code> <code>dict</code> <p>Additional arguments for the GCN. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Embedding vector for the graph features.</p> Source code in <code>src/QuantumGrav/gnn_model.py</code> <pre><code>def get_embeddings(\n    self,\n    x: torch.Tensor,\n    edge_index: torch.Tensor,\n    batch: torch.Tensor | None = None,\n    gcn_kwargs: dict | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Get embeddings from the GCN model.\n\n    Args:\n        x (torch.Tensor): Input node features.\n        edge_index (torch.Tensor): Graph connectivity information.\n        batch (torch.Tensor): Batch vector for pooling.\n        gcn_kwargs (dict, optional): Additional arguments for the GCN. Defaults to None.\n\n    Returns:\n        torch.Tensor: Embedding vector for the graph features.\n    \"\"\"\n    # apply the GCN backbone to the node features\n    embeddings = self.eval_encoder(\n        x, edge_index, **(gcn_kwargs if gcn_kwargs else {})\n    )\n\n    # pool everything together into a single graph representation\n    if self.pooling_layers is not None and self.aggregate_pooling is not None:\n        pooled_embeddings = [\n            pooling_op(embeddings, batch) for pooling_op in self.pooling_layers\n        ]\n\n        return self.aggregate_pooling(pooled_embeddings)\n    else:\n        return embeddings\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_model.GNNModel.load","title":"<code>load(path, device=torch.device('cpu'))</code>  <code>classmethod</code>","text":"<p>Load a model from file that has previously been save with the function 'save'.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>path to load the model from.</p> required <code>device</code> <code>device</code> <p>device to put the model to. Defaults to torch.device(\"cpu\")</p> <code>device('cpu')</code> <p>Returns:     GNNModel: model instance initialized with the sub-models loaded from file.</p> Source code in <code>src/QuantumGrav/gnn_model.py</code> <pre><code>@classmethod\ndef load(\n    cls, path: str | Path, device: torch.device = torch.device(\"cpu\")\n) -&gt; \"GNNModel\":\n    \"\"\"Load a model from file that has previously been save with the function 'save'.\n\n    Args:\n        path (str | Path): path to load the model from.\n        device (torch.device): device to put the model to. Defaults to torch.device(\"cpu\")\n    Returns:\n        GNNModel: model instance initialized with the sub-models loaded from file.\n    \"\"\"\n    model_dict = torch.load(path, weights_only=False)\n    model = cls.from_config(model_dict[\"config\"]).to(device)\n    model.load_state_dict(model_dict[\"model\"])\n\n    return model\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_model.GNNModel.save","title":"<code>save(path)</code>","text":"<p>Save the model state to file. This saves a dictionary structured like this:  'encoder': self.encoder,  'downstream_tasks': self.downstream_tasks,  'pooling_layers': self.pooling_layers,  'graph_features_net': self.graph_features_net,  'aggregate_graph_features': self.aggregate_graph_features,  'aggregate_pooling': self.aggregate_pooling,</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to save the model to</p> required Source code in <code>src/QuantumGrav/gnn_model.py</code> <pre><code>def save(self, path: str | Path) -&gt; None:\n    \"\"\"Save the model state to file. This saves a dictionary structured like this:\n     'encoder': self.encoder,\n     'downstream_tasks': self.downstream_tasks,\n     'pooling_layers': self.pooling_layers,\n     'graph_features_net': self.graph_features_net,\n     'aggregate_graph_features': self.aggregate_graph_features,\n     'aggregate_pooling': self.aggregate_pooling,\n\n    Args:\n        path (str | Path): Path to save the model to\n    \"\"\"\n\n    config = self.to_config()\n\n    torch.save(\n        {\"config\": config, \"model\": self.state_dict()},\n        path,\n    )\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_model.GNNModel.set_task_active","title":"<code>set_task_active(i)</code>","text":"<p>Set a downstream task as active.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>Index of the downstream task to activate.</p> required Source code in <code>src/QuantumGrav/gnn_model.py</code> <pre><code>def set_task_active(self, i: int) -&gt; None:\n    \"\"\"Set a downstream task as active.\n\n    Args:\n        i (int): Index of the downstream task to activate.\n    \"\"\"\n\n    if i &lt; 0 or i &gt;= len(self.active_tasks):\n        raise ValueError(\"Invalid task index.\")\n\n    self.active_tasks[i] = True\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_model.GNNModel.set_task_inactive","title":"<code>set_task_inactive(i)</code>","text":"<p>Set a downstream task as inactive.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>Index of the downstream task to deactivate.</p> required Source code in <code>src/QuantumGrav/gnn_model.py</code> <pre><code>def set_task_inactive(self, i: int) -&gt; None:\n    \"\"\"Set a downstream task as inactive.\n\n    Args:\n        i (int): Index of the downstream task to deactivate.\n    \"\"\"\n\n    if i &lt; 0 or i &gt;= len(self.active_tasks):\n        raise ValueError(\"Invalid task index.\")\n\n    self.active_tasks[i] = False\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_model.GNNModel.to_config","title":"<code>to_config()</code>","text":"<p>Serialize the model to a config</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: description</p> Source code in <code>src/QuantumGrav/gnn_model.py</code> <pre><code>def to_config(self) -&gt; dict[str, Any]:\n    \"\"\"Serialize the model to a config\n\n    Returns:\n        dict[str, Any]: _description_\n    \"\"\"\n    pooling_layer_names = None\n    if self.pooling_layers is not None:\n        pooling_layer_names = []\n        for layer in self.pooling_layers:\n            if isinstance(layer, ModuleWrapper):\n                pooling_layer_names.append(\n                    {\n                        \"type\": utils.pooling_layers_names[layer.get_fn()],\n                        \"args\": [],\n                        \"kwargs\": {},\n                    }\n                )\n            else:\n                pooling_layer_names.append(\n                    {\n                        \"type\": utils.pooling_layers_names[layer],\n                        \"args\": [],\n                        \"kwargs\": {},\n                    }\n                )\n\n    aggregate_graph_features_names = None\n    if self.aggregate_graph_features is not None:\n        if isinstance(self.aggregate_graph_features, ModuleWrapper):\n            aggregate_graph_features_names = {\n                \"type\": utils.graph_features_aggregations_names[\n                    self.aggregate_graph_features.get_fn()\n                ],\n                \"args\": [],\n                \"kwargs\": {},\n            }\n        else:\n            aggregate_graph_features_names = {\n                \"type\": utils.graph_features_aggregations_names[\n                    self.aggregate_graph_features\n                ],\n                \"args\": [],\n                \"kwargs\": {},\n            }\n\n    aggregate_pooling_names = None\n    if self.aggregate_pooling is not None:\n        if isinstance(self.aggregate_pooling, ModuleWrapper):\n            aggregate_pooling_names = {\n                \"type\": utils.pooling_aggregations_names[\n                    self.aggregate_pooling.get_fn()\n                ],\n                \"args\": [],\n                \"kwargs\": {},\n            }\n        else:\n            aggregate_pooling_names = {\n                \"type\": utils.pooling_aggregations_names[self.aggregate_pooling],\n                \"args\": [],\n                \"kwargs\": {},\n            }\n\n    # downstream_task_configs\n\n    downstream_task_configs = [task.to_config() for task in self.downstream_tasks]\n    for i in range(len(self.downstream_tasks)):\n        downstream_task_configs[i][\"active\"] = self.active_tasks[i]\n\n    config = {\n        \"encoder\": [encoder_layer.to_config() for encoder_layer in self.encoder],\n        \"downstream_tasks\": downstream_task_configs,\n        \"pooling_layers\": pooling_layer_names,\n        \"graph_features_net\": self.graph_features_net.to_config()\n        if self.graph_features_net\n        else None,\n        \"aggregate_graph_features\": aggregate_graph_features_names,\n        \"aggregate_pooling\": aggregate_pooling_names,\n        \"active_tasks\": self.active_tasks,\n    }\n\n    return config\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_model.ModuleWrapper","title":"<code>ModuleWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper to make pooling functions compatible with ModuleList.</p> Source code in <code>src/QuantumGrav/gnn_model.py</code> <pre><code>class ModuleWrapper(torch.nn.Module):\n    \"\"\"Wrapper to make pooling functions compatible with ModuleList.\"\"\"\n\n    def __init__(self, fn: Callable):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n        return self.fn(*args, **kwargs)\n\n    def get_fn(self) -&gt; Callable:\n        return self.fn\n</code></pre>"},{"location":"api/#graph-neural-network-submodels","title":"Graph Neural network submodels","text":"<p>The submodel classes in this section comprise the graph neural network backbone of a QuantumGrav model. </p>"},{"location":"api/#graph-model-block","title":"Graph model block","text":"<p>This submodel is the main part of the graph neural network backbone, composed of a set of GNN layers from <code>pytorch-geometric</code> with dropout and <code>BatchNorm</code>.  </p>"},{"location":"api/#QuantumGrav.gnn_block.GNNBlock","title":"<code>GNNBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Graph Neural Network Block. Consists of a GNN layer, a normalizer, an activation function, and a residual connection. The gnn-layer is applied first, followed by the normalizer and activation function. The result is then projected from the input dimensions to the output dimensions using a linear layer and added to the original input (residual connection). Finally, dropout is applied for regularization.</p> Source code in <code>src/QuantumGrav/gnn_block.py</code> <pre><code>class GNNBlock(torch.nn.Module):\n    \"\"\"Graph Neural Network Block. Consists of a GNN layer, a normalizer, an activation function,\n    and a residual connection. The gnn-layer is applied first, followed by the normalizer and activation function. The result is then projected from the input dimensions to the output dimensions using a linear layer and added to the original input (residual connection). Finally, dropout is applied for regularization.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_dim: int,\n        out_dim: int,\n        dropout: float = 0.3,\n        with_skip: bool = True,\n        gnn_layer_type: type[torch.nn.Module] = tgnn.conv.GCNConv,\n        normalizer: type[torch.nn.Module] = torch.nn.Identity,\n        activation: type[torch.nn.Module] = torch.nn.ReLU,\n        gnn_layer_args: list[Any] | None = None,\n        gnn_layer_kwargs: dict[str, Any] | None = None,\n        norm_args: list[Any] | None = None,\n        norm_kwargs: dict[str, Any] | None = None,\n        activation_args: list[Any] | None = None,\n        activation_kwargs: dict[str, Any] | None = None,\n        projection_args: list[Any] | None = None,\n        projection_kwargs: dict[str, Any] | None = None,\n    ):\n        \"\"\"Create a GNNBlock instance.\n\n        Args:\n            in_dim (int): The dimensions of the input features.\n            out_dim (int): The dimensions of the output features.\n            dropout (float, optional): The dropout probability. Defaults to 0.3.\n            with_skip (bool, optional): Whether to use a skip connection. Defaults to True.\n            gnn_layer_type (torch.nn.Module, optional): The type of GNN-layer to use. Defaults to tgnn.conv.GCNConv.\n            normalizer (torch.nn.Module, optional): The normalizer layer to use. Defaults to torch.nn.Identity.\n            activation (torch.nn.Module, optional): The activation function to use. Defaults to torch.nn.ReLU.\n            gnn_layer_args (list[Any], optional): Additional arguments for the GNN layer. Defaults to None.\n            gnn_layer_kwargs (dict[str, Any], optional): Additional keyword arguments for the GNN layer. Defaults to None.\n            norm_args (list[Any], optional): Additional arguments for the normalizer layer. Defaults to None.\n            norm_kwargs (dict[str, Any], optional): Additional keyword arguments for the normalizer layer. Defaults to None.\n            activation_args (list[Any], optional): Additional arguments for the activation function. Defaults to None.\n            activation_kwargs (dict[str, Any], optional): Additional keyword arguments for the activation function. Defaults to None.\n            projection_args (list[Any], optional): Additional arguments for the projection layer. Defaults to None.\n            projection_kwargs (dict[str, Any], optional): Additional keyword arguments for the projection layer. Defaults to None.\n\n        \"\"\"\n        super(GNNBlock, self).__init__()\n\n        # save parameters\n        self.dropout_p = dropout\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.with_skip = with_skip\n        # save args/kwargs\n        self.gnn_layer_args = gnn_layer_args\n        self.gnn_layer_kwargs = gnn_layer_kwargs\n        self.norm_args = norm_args\n        self.norm_kwargs = norm_kwargs\n        self.activation_args = activation_args\n        self.activation_kwargs = activation_kwargs\n        self.projection_args = projection_args\n        self.projection_kwargs = projection_kwargs\n\n        # initialize layers\n        self.dropout = torch.nn.Dropout(p=dropout, inplace=False)\n\n        self.normalizer = normalizer(\n            *(norm_args if norm_args is not None else []),\n            **(norm_kwargs if norm_kwargs is not None else {}),\n        )\n\n        self.activation = activation(\n            *(activation_args if activation_args is not None else []),\n            **(activation_kwargs if activation_kwargs is not None else {}),\n        )\n\n        self.conv = gnn_layer_type(\n            in_dim,\n            out_dim,\n            *(gnn_layer_args if gnn_layer_args is not None else []),\n            **(gnn_layer_kwargs if gnn_layer_kwargs is not None else {}),\n        )\n\n        if self.projection_kwargs is None:\n            self.projection_kwargs = {\"bias\": False}\n\n        if self.projection_args is None:\n            self.projection_args = [in_dim, out_dim]\n\n        if in_dim != out_dim:\n            self.projection = torch.nn.Linear(\n                *self.projection_args,\n                **self.projection_kwargs,\n            )\n        else:\n            self.projection = torch.nn.Identity()\n\n    def forward(\n        self, x: torch.Tensor, edge_index: torch.Tensor, **kwargs\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward pass for the GNNBlock.\n        First apply the graph convolution layer, then normalize and apply the activation function.\n        Finally, apply a residual connection and dropout.\n        Args:\n            x (torch.Tensor): The input node features.\n            edge_index (torch.Tensor): The graph connectivity information.\n            edge_weight (torch.Tensor, optional): The edge weights. Defaults to None.\n            kwargs (dict[Any, Any], optional): Additional keyword arguments for the GNN layer. Defaults to None.\n\n        Returns:\n            torch.Tensor: The output node features.\n        \"\"\"\n\n        # convolution, then normalize and apply nonlinearity\n        x_res = self.conv(x, edge_index, **kwargs)\n        x_res = self.normalizer(x_res)\n        x_res = self.activation(x_res)\n\n        # Residual connection\n        if self.with_skip:\n            x_res = x_res + self.projection(x)\n\n        # Apply dropout as regularization\n        x_res = self.dropout(x_res)\n\n        return x_res\n\n    @classmethod\n    def from_config(cls, config: dict[str, Any]) -&gt; \"GNNBlock\":\n        \"\"\"Create a GNNBlock from a configuration dictionary.\n        When the config does not have 'dropout', it defaults to 0.3.\n\n        Args:\n            config (dict[str, Any]): Configuration dictionary containing the parameters for the GNNBlock.\n\n        Returns:\n            GNNBlock: An instance of GNNBlock initialized with the provided configuration.\n        \"\"\"\n        return cls(\n            in_dim=config[\"in_dim\"],\n            out_dim=config[\"out_dim\"],\n            dropout=config.get(\"dropout\", 0.3),\n            gnn_layer_type=utils.gnn_layers[config[\"gnn_layer_type\"]],\n            normalizer=utils.normalizer_layers[config[\"normalizer\"]],\n            activation=utils.activation_layers[config[\"activation\"]],\n            gnn_layer_args=config.get(\"gnn_layer_args\", []),\n            gnn_layer_kwargs=config.get(\"gnn_layer_kwargs\", {}),\n            norm_args=config.get(\"norm_args\", []),\n            norm_kwargs=config.get(\"norm_kwargs\", {}),\n            activation_args=config.get(\"activation_args\", []),\n            activation_kwargs=config.get(\"activation_kwargs\", {}),\n            projection_args=config.get(\"projection_args\", None),\n            projection_kwargs=config.get(\"projection_kwargs\", None),\n        )\n\n    def to_config(self) -&gt; dict[str, Any]:\n        \"\"\"Convert the GNNBlock instance to a configuration dictionary.\"\"\"\n        config = {\n            \"in_dim\": self.in_dim,\n            \"out_dim\": self.out_dim,\n            \"dropout\": self.dropout.p,\n            \"with_skip\": self.with_skip,\n            \"gnn_layer_type\": utils.gnn_layers_names[type(self.conv)],\n            \"normalizer\": utils.normalizer_layers_names[type(self.normalizer)],\n            \"activation\": utils.activation_layers_names[type(self.activation)],\n            \"gnn_layer_args\": self.gnn_layer_args,\n            \"gnn_layer_kwargs\": self.gnn_layer_kwargs,\n            \"norm_args\": self.norm_args,\n            \"norm_kwargs\": self.norm_kwargs,\n            \"activation_args\": self.activation_args,\n            \"activation_kwargs\": self.activation_kwargs,\n            \"projection_args\": self.projection_args,\n            \"projection_kwargs\": self.projection_kwargs,\n        }\n        return config\n\n    def save(self, path: str | Path) -&gt; None:\n        \"\"\"Save the model's state to file.\n\n        Args:\n            path (str | Path): path to save the model to.\n        \"\"\"\n\n        self_as_cfg = self.to_config()\n\n        torch.save({\"config\": self_as_cfg, \"state_dict\": self.state_dict()}, path)\n\n    @classmethod\n    def load(\n        cls, path: str | Path, device: torch.device = torch.device(\"cpu\")\n    ) -&gt; \"GNNBlock\":\n        \"\"\"Load a mode instance from file\n\n        Args:\n            path (str | Path): Path to the file to load.\n            device (torch.device): device to put the model to. Defaults to torch.device(\"cpu\")\n        Returns:\n            GNNBlock: A GNNBlock instance initialized from the data loaded from the file.\n        \"\"\"\n\n        modeldata = torch.load(path, weights_only=False)\n        model = cls.from_config(modeldata[\"config\"]).to(device)\n        model.load_state_dict(modeldata[\"state_dict\"])\n        return model\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_block.GNNBlock.__init__","title":"<code>__init__(in_dim, out_dim, dropout=0.3, with_skip=True, gnn_layer_type=tgnn.conv.GCNConv, normalizer=torch.nn.Identity, activation=torch.nn.ReLU, gnn_layer_args=None, gnn_layer_kwargs=None, norm_args=None, norm_kwargs=None, activation_args=None, activation_kwargs=None, projection_args=None, projection_kwargs=None)</code>","text":"<p>Create a GNNBlock instance.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>The dimensions of the input features.</p> required <code>out_dim</code> <code>int</code> <p>The dimensions of the output features.</p> required <code>dropout</code> <code>float</code> <p>The dropout probability. Defaults to 0.3.</p> <code>0.3</code> <code>with_skip</code> <code>bool</code> <p>Whether to use a skip connection. Defaults to True.</p> <code>True</code> <code>gnn_layer_type</code> <code>Module</code> <p>The type of GNN-layer to use. Defaults to tgnn.conv.GCNConv.</p> <code>GCNConv</code> <code>normalizer</code> <code>Module</code> <p>The normalizer layer to use. Defaults to torch.nn.Identity.</p> <code>Identity</code> <code>activation</code> <code>Module</code> <p>The activation function to use. Defaults to torch.nn.ReLU.</p> <code>ReLU</code> <code>gnn_layer_args</code> <code>list[Any]</code> <p>Additional arguments for the GNN layer. Defaults to None.</p> <code>None</code> <code>gnn_layer_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments for the GNN layer. Defaults to None.</p> <code>None</code> <code>norm_args</code> <code>list[Any]</code> <p>Additional arguments for the normalizer layer. Defaults to None.</p> <code>None</code> <code>norm_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments for the normalizer layer. Defaults to None.</p> <code>None</code> <code>activation_args</code> <code>list[Any]</code> <p>Additional arguments for the activation function. Defaults to None.</p> <code>None</code> <code>activation_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments for the activation function. Defaults to None.</p> <code>None</code> <code>projection_args</code> <code>list[Any]</code> <p>Additional arguments for the projection layer. Defaults to None.</p> <code>None</code> <code>projection_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments for the projection layer. Defaults to None.</p> <code>None</code> Source code in <code>src/QuantumGrav/gnn_block.py</code> <pre><code>def __init__(\n    self,\n    in_dim: int,\n    out_dim: int,\n    dropout: float = 0.3,\n    with_skip: bool = True,\n    gnn_layer_type: type[torch.nn.Module] = tgnn.conv.GCNConv,\n    normalizer: type[torch.nn.Module] = torch.nn.Identity,\n    activation: type[torch.nn.Module] = torch.nn.ReLU,\n    gnn_layer_args: list[Any] | None = None,\n    gnn_layer_kwargs: dict[str, Any] | None = None,\n    norm_args: list[Any] | None = None,\n    norm_kwargs: dict[str, Any] | None = None,\n    activation_args: list[Any] | None = None,\n    activation_kwargs: dict[str, Any] | None = None,\n    projection_args: list[Any] | None = None,\n    projection_kwargs: dict[str, Any] | None = None,\n):\n    \"\"\"Create a GNNBlock instance.\n\n    Args:\n        in_dim (int): The dimensions of the input features.\n        out_dim (int): The dimensions of the output features.\n        dropout (float, optional): The dropout probability. Defaults to 0.3.\n        with_skip (bool, optional): Whether to use a skip connection. Defaults to True.\n        gnn_layer_type (torch.nn.Module, optional): The type of GNN-layer to use. Defaults to tgnn.conv.GCNConv.\n        normalizer (torch.nn.Module, optional): The normalizer layer to use. Defaults to torch.nn.Identity.\n        activation (torch.nn.Module, optional): The activation function to use. Defaults to torch.nn.ReLU.\n        gnn_layer_args (list[Any], optional): Additional arguments for the GNN layer. Defaults to None.\n        gnn_layer_kwargs (dict[str, Any], optional): Additional keyword arguments for the GNN layer. Defaults to None.\n        norm_args (list[Any], optional): Additional arguments for the normalizer layer. Defaults to None.\n        norm_kwargs (dict[str, Any], optional): Additional keyword arguments for the normalizer layer. Defaults to None.\n        activation_args (list[Any], optional): Additional arguments for the activation function. Defaults to None.\n        activation_kwargs (dict[str, Any], optional): Additional keyword arguments for the activation function. Defaults to None.\n        projection_args (list[Any], optional): Additional arguments for the projection layer. Defaults to None.\n        projection_kwargs (dict[str, Any], optional): Additional keyword arguments for the projection layer. Defaults to None.\n\n    \"\"\"\n    super(GNNBlock, self).__init__()\n\n    # save parameters\n    self.dropout_p = dropout\n    self.in_dim = in_dim\n    self.out_dim = out_dim\n    self.with_skip = with_skip\n    # save args/kwargs\n    self.gnn_layer_args = gnn_layer_args\n    self.gnn_layer_kwargs = gnn_layer_kwargs\n    self.norm_args = norm_args\n    self.norm_kwargs = norm_kwargs\n    self.activation_args = activation_args\n    self.activation_kwargs = activation_kwargs\n    self.projection_args = projection_args\n    self.projection_kwargs = projection_kwargs\n\n    # initialize layers\n    self.dropout = torch.nn.Dropout(p=dropout, inplace=False)\n\n    self.normalizer = normalizer(\n        *(norm_args if norm_args is not None else []),\n        **(norm_kwargs if norm_kwargs is not None else {}),\n    )\n\n    self.activation = activation(\n        *(activation_args if activation_args is not None else []),\n        **(activation_kwargs if activation_kwargs is not None else {}),\n    )\n\n    self.conv = gnn_layer_type(\n        in_dim,\n        out_dim,\n        *(gnn_layer_args if gnn_layer_args is not None else []),\n        **(gnn_layer_kwargs if gnn_layer_kwargs is not None else {}),\n    )\n\n    if self.projection_kwargs is None:\n        self.projection_kwargs = {\"bias\": False}\n\n    if self.projection_args is None:\n        self.projection_args = [in_dim, out_dim]\n\n    if in_dim != out_dim:\n        self.projection = torch.nn.Linear(\n            *self.projection_args,\n            **self.projection_kwargs,\n        )\n    else:\n        self.projection = torch.nn.Identity()\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_block.GNNBlock.forward","title":"<code>forward(x, edge_index, **kwargs)</code>","text":"<p>Forward pass for the GNNBlock. First apply the graph convolution layer, then normalize and apply the activation function. Finally, apply a residual connection and dropout. Args:     x (torch.Tensor): The input node features.     edge_index (torch.Tensor): The graph connectivity information.     edge_weight (torch.Tensor, optional): The edge weights. Defaults to None.     kwargs (dict[Any, Any], optional): Additional keyword arguments for the GNN layer. Defaults to None.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output node features.</p> Source code in <code>src/QuantumGrav/gnn_block.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, edge_index: torch.Tensor, **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass for the GNNBlock.\n    First apply the graph convolution layer, then normalize and apply the activation function.\n    Finally, apply a residual connection and dropout.\n    Args:\n        x (torch.Tensor): The input node features.\n        edge_index (torch.Tensor): The graph connectivity information.\n        edge_weight (torch.Tensor, optional): The edge weights. Defaults to None.\n        kwargs (dict[Any, Any], optional): Additional keyword arguments for the GNN layer. Defaults to None.\n\n    Returns:\n        torch.Tensor: The output node features.\n    \"\"\"\n\n    # convolution, then normalize and apply nonlinearity\n    x_res = self.conv(x, edge_index, **kwargs)\n    x_res = self.normalizer(x_res)\n    x_res = self.activation(x_res)\n\n    # Residual connection\n    if self.with_skip:\n        x_res = x_res + self.projection(x)\n\n    # Apply dropout as regularization\n    x_res = self.dropout(x_res)\n\n    return x_res\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_block.GNNBlock.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create a GNNBlock from a configuration dictionary. When the config does not have 'dropout', it defaults to 0.3.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary containing the parameters for the GNNBlock.</p> required <p>Returns:</p> Name Type Description <code>GNNBlock</code> <code>GNNBlock</code> <p>An instance of GNNBlock initialized with the provided configuration.</p> Source code in <code>src/QuantumGrav/gnn_block.py</code> <pre><code>@classmethod\ndef from_config(cls, config: dict[str, Any]) -&gt; \"GNNBlock\":\n    \"\"\"Create a GNNBlock from a configuration dictionary.\n    When the config does not have 'dropout', it defaults to 0.3.\n\n    Args:\n        config (dict[str, Any]): Configuration dictionary containing the parameters for the GNNBlock.\n\n    Returns:\n        GNNBlock: An instance of GNNBlock initialized with the provided configuration.\n    \"\"\"\n    return cls(\n        in_dim=config[\"in_dim\"],\n        out_dim=config[\"out_dim\"],\n        dropout=config.get(\"dropout\", 0.3),\n        gnn_layer_type=utils.gnn_layers[config[\"gnn_layer_type\"]],\n        normalizer=utils.normalizer_layers[config[\"normalizer\"]],\n        activation=utils.activation_layers[config[\"activation\"]],\n        gnn_layer_args=config.get(\"gnn_layer_args\", []),\n        gnn_layer_kwargs=config.get(\"gnn_layer_kwargs\", {}),\n        norm_args=config.get(\"norm_args\", []),\n        norm_kwargs=config.get(\"norm_kwargs\", {}),\n        activation_args=config.get(\"activation_args\", []),\n        activation_kwargs=config.get(\"activation_kwargs\", {}),\n        projection_args=config.get(\"projection_args\", None),\n        projection_kwargs=config.get(\"projection_kwargs\", None),\n    )\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_block.GNNBlock.load","title":"<code>load(path, device=torch.device('cpu'))</code>  <code>classmethod</code>","text":"<p>Load a mode instance from file</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file to load.</p> required <code>device</code> <code>device</code> <p>device to put the model to. Defaults to torch.device(\"cpu\")</p> <code>device('cpu')</code> <p>Returns:     GNNBlock: A GNNBlock instance initialized from the data loaded from the file.</p> Source code in <code>src/QuantumGrav/gnn_block.py</code> <pre><code>@classmethod\ndef load(\n    cls, path: str | Path, device: torch.device = torch.device(\"cpu\")\n) -&gt; \"GNNBlock\":\n    \"\"\"Load a mode instance from file\n\n    Args:\n        path (str | Path): Path to the file to load.\n        device (torch.device): device to put the model to. Defaults to torch.device(\"cpu\")\n    Returns:\n        GNNBlock: A GNNBlock instance initialized from the data loaded from the file.\n    \"\"\"\n\n    modeldata = torch.load(path, weights_only=False)\n    model = cls.from_config(modeldata[\"config\"]).to(device)\n    model.load_state_dict(modeldata[\"state_dict\"])\n    return model\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_block.GNNBlock.save","title":"<code>save(path)</code>","text":"<p>Save the model's state to file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>path to save the model to.</p> required Source code in <code>src/QuantumGrav/gnn_block.py</code> <pre><code>def save(self, path: str | Path) -&gt; None:\n    \"\"\"Save the model's state to file.\n\n    Args:\n        path (str | Path): path to save the model to.\n    \"\"\"\n\n    self_as_cfg = self.to_config()\n\n    torch.save({\"config\": self_as_cfg, \"state_dict\": self.state_dict()}, path)\n</code></pre>"},{"location":"api/#QuantumGrav.gnn_block.GNNBlock.to_config","title":"<code>to_config()</code>","text":"<p>Convert the GNNBlock instance to a configuration dictionary.</p> Source code in <code>src/QuantumGrav/gnn_block.py</code> <pre><code>def to_config(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the GNNBlock instance to a configuration dictionary.\"\"\"\n    config = {\n        \"in_dim\": self.in_dim,\n        \"out_dim\": self.out_dim,\n        \"dropout\": self.dropout.p,\n        \"with_skip\": self.with_skip,\n        \"gnn_layer_type\": utils.gnn_layers_names[type(self.conv)],\n        \"normalizer\": utils.normalizer_layers_names[type(self.normalizer)],\n        \"activation\": utils.activation_layers_names[type(self.activation)],\n        \"gnn_layer_args\": self.gnn_layer_args,\n        \"gnn_layer_kwargs\": self.gnn_layer_kwargs,\n        \"norm_args\": self.norm_args,\n        \"norm_kwargs\": self.norm_kwargs,\n        \"activation_args\": self.activation_args,\n        \"activation_kwargs\": self.activation_kwargs,\n        \"projection_args\": self.projection_args,\n        \"projection_kwargs\": self.projection_kwargs,\n    }\n    return config\n</code></pre>"},{"location":"api/#base-class-for-models-composed-of-linear-layers","title":"Base class for models composed of linear layers","text":""},{"location":"api/#QuantumGrav.linear_sequential.LinearSequential","title":"<code>LinearSequential</code>","text":"<p>               Bases: <code>Module</code></p> <p>This class implements a neural network block consisting of a backbone (a sequence of linear layers with activation functions) and multiple output layers for classification tasks. It supports multi-objective classification by allowing multiple output layers, each corresponding to a different classification task, but can also be used for any other type of sequential processing that involves linear layers.</p> Source code in <code>src/QuantumGrav/linear_sequential.py</code> <pre><code>class LinearSequential(torch.nn.Module):\n    \"\"\"This class implements a neural network block consisting of a backbone\n    (a sequence of linear layers with activation functions) and multiple\n    output layers for classification tasks. It supports multi-objective\n    classification by allowing multiple output layers, each corresponding\n    to a different classification task, but can also be used for any other type of sequential processing that involves linear layers.\n    \"\"\"\n\n    def __init__(\n        self,\n        dims: list[Sequence[int]],\n        activations: list[type[torch.nn.Module]] = [torch.nn.ReLU],\n        linear_kwargs: list[dict] | None = None,\n        activation_kwargs: list[dict] | None = None,\n    ):\n        \"\"\"Create a LinearSequential object with a backbone and multiple output layers. All layers are of type `Linear` with an activation function in between (the backbone) and a set of linear output layers.\n\n        Args:\n            input_dim (int): input dimension of the LinearSequential object\n            output_dim (int): output dimension for the output layer, i.e., the classification task\n            hidden_dims (list[int]): list of hidden dimensions for the backbone\n            activation (type[torch.nn.Module], optional): activation function to use. Defaults to torch.nn.ReLU.\n            backbone_kwargs (list[dict], optional): additional arguments for the backbone layers. Defaults to None.\n            output_kwargs (dict, optional): additional keyword arguments for the output layers. Defaults to None.\n\n        Raises:\n            ValueError: If hidden_dims contains non-positive integers.\n            ValueError: If output_dim is a non-positive integer.\n        \"\"\"\n        super().__init__()\n\n        if len(dims) == 0:\n            raise ValueError(\"dims must not be empty\")\n\n        if len(dims) != len(activations):\n            raise ValueError(\"dims and activations must have the same length\")\n\n        if linear_kwargs is None:\n            linear_kwargs = [{} for _ in range(len(dims))]\n\n        if activation_kwargs is None:\n            activation_kwargs = [{} for _ in range(len(dims))]\n\n        if len(linear_kwargs) != len(dims):\n            raise ValueError(\"linear_kwargs must have the same length as dims\")\n\n        if len(activation_kwargs) != len(dims):\n            raise ValueError(\"activation_kwargs must have the same length as dims\")\n\n        # build backbone with Sequential\n        layers = []\n        for i in range(len(dims)):\n            in_dim = dims[i][0]\n            out_dim = dims[i][1]\n            layers.append(\n                torch_geometric.nn.dense.Linear(\n                    in_dim,\n                    out_dim,\n                    **linear_kwargs[i],\n                )\n            )\n            layers.append(\n                activations[i](\n                    **activation_kwargs[i],\n                )\n            )\n\n        self.layers = torch.nn.Sequential(*layers)\n        self.linear_kwargs = linear_kwargs\n        self.activation_kwargs = activation_kwargs\n        self.logger = logging.getLogger(__name__)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n    ) -&gt; list[torch.Tensor]:\n        \"\"\"Forward pass through the LinearSequential object.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            list[torch.Tensor]: List of output tensors from each classifier layer.\n        \"\"\"\n        # Sequential handles passing output from one layer to the next\n        logits = self.layers(x)  # No need for manual looping or cloning\n\n        return logits\n\n    @classmethod\n    def from_config(cls, config: dict[str, Any]) -&gt; \"LinearSequential\":\n        \"\"\"Create a LinearSequential from a configuration dictionary.\n\n        Args:\n            config (dict[str, Any]): Configuration dictionary containing parameters for the LinearSequential.\n\n        Returns:\n            LinearSequential: An instance of LinearSequential initialized with the provided configuration.\n\n        Raises:\n            ValueError: If the specified activation function is not registered.\n        \"\"\"\n        activations = config[\"activations\"]\n        activations = [utils.get_registered_activation(act) for act in activations]\n\n        if None in activations:\n            raise ValueError(\n                f\"Activation function '{config.get('activation')}' is not registered.\"\n            )\n\n        return cls(\n            dims=config[\"dims\"],\n            activations=activations,\n            linear_kwargs=config.get(\"linear_kwargs\", None),\n            activation_kwargs=config.get(\"activation_kwargs\", None),\n        )\n\n    def to_config(self) -&gt; dict[str, Any]:\n        \"\"\"Build a config file from the current model\n\n        Returns:\n            dict[str, Any]: Model config\n        \"\"\"\n        linear_dims = []\n        activations = []\n\n        for layer in self.layers:\n            if isinstance(layer, torch_geometric.nn.dense.Linear):\n                linear_dims.append((layer.in_channels, layer.out_channels))\n            elif isinstance(layer, torch.nn.Linear):\n                linear_dims.append((layer.in_features, layer.out_features))\n            elif isinstance(layer, torch.nn.Module):\n                activations.append(utils.activation_layers_names[type(layer)])\n            else:\n                self.logger.warning(f\"Unknown layer type: {type(layer)}\")\n\n        config = {\n            \"dims\": linear_dims,\n            \"activations\": activations,\n            \"linear_kwargs\": self.linear_kwargs,\n            \"activation_kwargs\": self.activation_kwargs,\n        }\n\n        return config\n\n    def save(self, path: str | Path) -&gt; None:\n        \"\"\"Save the model's state to file.\n\n        Args:\n            path (str | Path): path to save the model to.\n        \"\"\"\n\n        self_as_config = self.to_config()\n\n        torch.save({\"config\": self_as_config, \"state_dict\": self.state_dict()}, path)\n\n    @classmethod\n    def load(\n        cls, path: str | Path, device: torch.device = torch.device(\"cpu\")\n    ) -&gt; \"LinearSequential\":\n        \"\"\"Load a LinearSequential instance from file\n\n        Args:\n            path (str | Path): path to the file to load the model from\n            device (torch.device): device to put the model to. Defaults to torch.device(\"cpu\")\n        Returns:\n            LinearSequential: An instance of LinearSequential initialized from the loaded data.\n        \"\"\"\n        cfg = torch.load(path)\n\n        model = cls.from_config(cfg[\"config\"])\n        model.load_state_dict(cfg[\"state_dict\"], strict=False)\n        model.to(device)\n\n        return model\n</code></pre>"},{"location":"api/#QuantumGrav.linear_sequential.LinearSequential.__init__","title":"<code>__init__(dims, activations=[torch.nn.ReLU], linear_kwargs=None, activation_kwargs=None)</code>","text":"<p>Create a LinearSequential object with a backbone and multiple output layers. All layers are of type <code>Linear</code> with an activation function in between (the backbone) and a set of linear output layers.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>input dimension of the LinearSequential object</p> required <code>output_dim</code> <code>int</code> <p>output dimension for the output layer, i.e., the classification task</p> required <code>hidden_dims</code> <code>list[int]</code> <p>list of hidden dimensions for the backbone</p> required <code>activation</code> <code>type[Module]</code> <p>activation function to use. Defaults to torch.nn.ReLU.</p> required <code>backbone_kwargs</code> <code>list[dict]</code> <p>additional arguments for the backbone layers. Defaults to None.</p> required <code>output_kwargs</code> <code>dict</code> <p>additional keyword arguments for the output layers. Defaults to None.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If hidden_dims contains non-positive integers.</p> <code>ValueError</code> <p>If output_dim is a non-positive integer.</p> Source code in <code>src/QuantumGrav/linear_sequential.py</code> <pre><code>def __init__(\n    self,\n    dims: list[Sequence[int]],\n    activations: list[type[torch.nn.Module]] = [torch.nn.ReLU],\n    linear_kwargs: list[dict] | None = None,\n    activation_kwargs: list[dict] | None = None,\n):\n    \"\"\"Create a LinearSequential object with a backbone and multiple output layers. All layers are of type `Linear` with an activation function in between (the backbone) and a set of linear output layers.\n\n    Args:\n        input_dim (int): input dimension of the LinearSequential object\n        output_dim (int): output dimension for the output layer, i.e., the classification task\n        hidden_dims (list[int]): list of hidden dimensions for the backbone\n        activation (type[torch.nn.Module], optional): activation function to use. Defaults to torch.nn.ReLU.\n        backbone_kwargs (list[dict], optional): additional arguments for the backbone layers. Defaults to None.\n        output_kwargs (dict, optional): additional keyword arguments for the output layers. Defaults to None.\n\n    Raises:\n        ValueError: If hidden_dims contains non-positive integers.\n        ValueError: If output_dim is a non-positive integer.\n    \"\"\"\n    super().__init__()\n\n    if len(dims) == 0:\n        raise ValueError(\"dims must not be empty\")\n\n    if len(dims) != len(activations):\n        raise ValueError(\"dims and activations must have the same length\")\n\n    if linear_kwargs is None:\n        linear_kwargs = [{} for _ in range(len(dims))]\n\n    if activation_kwargs is None:\n        activation_kwargs = [{} for _ in range(len(dims))]\n\n    if len(linear_kwargs) != len(dims):\n        raise ValueError(\"linear_kwargs must have the same length as dims\")\n\n    if len(activation_kwargs) != len(dims):\n        raise ValueError(\"activation_kwargs must have the same length as dims\")\n\n    # build backbone with Sequential\n    layers = []\n    for i in range(len(dims)):\n        in_dim = dims[i][0]\n        out_dim = dims[i][1]\n        layers.append(\n            torch_geometric.nn.dense.Linear(\n                in_dim,\n                out_dim,\n                **linear_kwargs[i],\n            )\n        )\n        layers.append(\n            activations[i](\n                **activation_kwargs[i],\n            )\n        )\n\n    self.layers = torch.nn.Sequential(*layers)\n    self.linear_kwargs = linear_kwargs\n    self.activation_kwargs = activation_kwargs\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"api/#QuantumGrav.linear_sequential.LinearSequential.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the LinearSequential object.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>list[torch.Tensor]: List of output tensors from each classifier layer.</p> Source code in <code>src/QuantumGrav/linear_sequential.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n) -&gt; list[torch.Tensor]:\n    \"\"\"Forward pass through the LinearSequential object.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        list[torch.Tensor]: List of output tensors from each classifier layer.\n    \"\"\"\n    # Sequential handles passing output from one layer to the next\n    logits = self.layers(x)  # No need for manual looping or cloning\n\n    return logits\n</code></pre>"},{"location":"api/#QuantumGrav.linear_sequential.LinearSequential.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create a LinearSequential from a configuration dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary containing parameters for the LinearSequential.</p> required <p>Returns:</p> Name Type Description <code>LinearSequential</code> <code>LinearSequential</code> <p>An instance of LinearSequential initialized with the provided configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified activation function is not registered.</p> Source code in <code>src/QuantumGrav/linear_sequential.py</code> <pre><code>@classmethod\ndef from_config(cls, config: dict[str, Any]) -&gt; \"LinearSequential\":\n    \"\"\"Create a LinearSequential from a configuration dictionary.\n\n    Args:\n        config (dict[str, Any]): Configuration dictionary containing parameters for the LinearSequential.\n\n    Returns:\n        LinearSequential: An instance of LinearSequential initialized with the provided configuration.\n\n    Raises:\n        ValueError: If the specified activation function is not registered.\n    \"\"\"\n    activations = config[\"activations\"]\n    activations = [utils.get_registered_activation(act) for act in activations]\n\n    if None in activations:\n        raise ValueError(\n            f\"Activation function '{config.get('activation')}' is not registered.\"\n        )\n\n    return cls(\n        dims=config[\"dims\"],\n        activations=activations,\n        linear_kwargs=config.get(\"linear_kwargs\", None),\n        activation_kwargs=config.get(\"activation_kwargs\", None),\n    )\n</code></pre>"},{"location":"api/#QuantumGrav.linear_sequential.LinearSequential.load","title":"<code>load(path, device=torch.device('cpu'))</code>  <code>classmethod</code>","text":"<p>Load a LinearSequential instance from file</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>path to the file to load the model from</p> required <code>device</code> <code>device</code> <p>device to put the model to. Defaults to torch.device(\"cpu\")</p> <code>device('cpu')</code> <p>Returns:     LinearSequential: An instance of LinearSequential initialized from the loaded data.</p> Source code in <code>src/QuantumGrav/linear_sequential.py</code> <pre><code>@classmethod\ndef load(\n    cls, path: str | Path, device: torch.device = torch.device(\"cpu\")\n) -&gt; \"LinearSequential\":\n    \"\"\"Load a LinearSequential instance from file\n\n    Args:\n        path (str | Path): path to the file to load the model from\n        device (torch.device): device to put the model to. Defaults to torch.device(\"cpu\")\n    Returns:\n        LinearSequential: An instance of LinearSequential initialized from the loaded data.\n    \"\"\"\n    cfg = torch.load(path)\n\n    model = cls.from_config(cfg[\"config\"])\n    model.load_state_dict(cfg[\"state_dict\"], strict=False)\n    model.to(device)\n\n    return model\n</code></pre>"},{"location":"api/#QuantumGrav.linear_sequential.LinearSequential.save","title":"<code>save(path)</code>","text":"<p>Save the model's state to file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>path to save the model to.</p> required Source code in <code>src/QuantumGrav/linear_sequential.py</code> <pre><code>def save(self, path: str | Path) -&gt; None:\n    \"\"\"Save the model's state to file.\n\n    Args:\n        path (str | Path): path to save the model to.\n    \"\"\"\n\n    self_as_config = self.to_config()\n\n    torch.save({\"config\": self_as_config, \"state_dict\": self.state_dict()}, path)\n</code></pre>"},{"location":"api/#QuantumGrav.linear_sequential.LinearSequential.to_config","title":"<code>to_config()</code>","text":"<p>Build a config file from the current model</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Model config</p> Source code in <code>src/QuantumGrav/linear_sequential.py</code> <pre><code>def to_config(self) -&gt; dict[str, Any]:\n    \"\"\"Build a config file from the current model\n\n    Returns:\n        dict[str, Any]: Model config\n    \"\"\"\n    linear_dims = []\n    activations = []\n\n    for layer in self.layers:\n        if isinstance(layer, torch_geometric.nn.dense.Linear):\n            linear_dims.append((layer.in_channels, layer.out_channels))\n        elif isinstance(layer, torch.nn.Linear):\n            linear_dims.append((layer.in_features, layer.out_features))\n        elif isinstance(layer, torch.nn.Module):\n            activations.append(utils.activation_layers_names[type(layer)])\n        else:\n            self.logger.warning(f\"Unknown layer type: {type(layer)}\")\n\n    config = {\n        \"dims\": linear_dims,\n        \"activations\": activations,\n        \"linear_kwargs\": self.linear_kwargs,\n        \"activation_kwargs\": self.activation_kwargs,\n    }\n\n    return config\n</code></pre>"},{"location":"api/#model-evaluation","title":"Model evaluation","text":"<p>This module provides base classes that take the output of applying the model to a validation or training dataset, and derive useful quantities to evaluate the model quality. These do not do anything useful by default. Rather, you must derive your own class from them that implemements your desired evaluation, e.g., using an F1 score. </p>"},{"location":"api/#QuantumGrav.evaluate.DefaultEarlyStopping","title":"<code>DefaultEarlyStopping</code>","text":"<p>Early stopping based on a validation metric.</p> Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>class DefaultEarlyStopping:\n    \"\"\"Early stopping based on a validation metric.\"\"\"\n\n    # put this into the package\n    def __init__(\n        self,\n        patience: int,\n        delta: list[float] = [1e-4],\n        window: list[int] = [7],\n        metric: list[str] = [\"loss\"],\n        smoothing: bool = False,\n        criterion: Callable = lambda early_stopping_instance,\n        data: early_stopping_instance.current_patience &lt;= 0,\n        init_best_score: float = np.inf,\n        mode: str | Callable[[list[bool]], bool] = \"any\",\n        grace_period: list[int] = [\n            0,\n        ],\n        minimize: bool = False,\n    ):\n        \"\"\"Early stopping initialization.\n\n        Args:\n            patience (int): Number of epochs with no improvement after which training will be stopped.\n            delta (float, optional): Minimum change to consider an improvement. Defaults to 1e-4.\n            window (int, optional): Size of the moving window for smoothing. Defaults to 7.\n            metric (str, optional): Metric to monitor for early stopping. Defaults to \"loss\". This class always assumes that lower values for 'metric' are better.\n            smoothing (bool, optional): Whether to apply smoothed mean to the metric to dampen fluctuations. Defaults to False.\n            criterion (Callable, optional): Custom stopping criterion. Defaults to a function that stops when patience is exhausted.\n            init_best_score (float): initial best score value.\n            mode (str | Callable[[list[bool]], bool], optional): The mode for early stopping. Can be \"any\", \"all\", or a custom function. Defaults to \"any\". This decides wheather all tracked metrics have to improve or only one of them, or if something else should be done by applying a custom function to the array of evaluation results.\n            minimize: Bool: Whether to minimize or maximize the criterion\n        \"\"\"\n        lw = len(window)\n        lm = len(metric)\n        ld = len(delta)\n        lg = len(grace_period)\n\n        if min([lw, lm, ld, lg]) != max([lw, lm, ld, lg]):\n            raise ValueError(\n                f\"Inconsistent lengths for early stopping parameters: {lw}, {lm}, {ld}, {lg}\"\n            )\n\n        self.patience = patience\n        self.current_patience = patience\n        self.delta = delta\n        self.window = window\n        self.found_better = [False for _ in range(lw)]\n        self.metric = metric\n        self.init_best_score = init_best_score\n        self.best_score = [init_best_score for _ in range(lw)]\n        self.smoothing = smoothing\n        self.logger = logging.getLogger(__name__)\n        self.criterion = criterion\n        self.mode = mode\n        self.found_better = [False for _ in range(lw)]\n        self.grace_period = grace_period\n        self.current_grace_period = [grace_period[i] for i in range(lg)]\n        self.minimize = minimize\n        self.logger = logging.getLogger(__name__)\n        self.num_tasks = len(self.window)\n\n    @property\n    def found_better_model(self) -&gt; bool:\n        \"\"\"Check if a better model has been found.\"\"\"\n\n        if self.mode == \"any\":\n            self.logger.debug(\"Checking early stopping criteria (any mode).\")\n            return any(self.found_better)\n        elif self.mode == \"all\":\n            self.logger.debug(\"Checking early stopping criteria (all mode).\")\n            return all(self.found_better)\n        elif callable(self.mode):\n            self.logger.debug(\"Checking early stopping criteria (custom mode).\")\n            return self.mode(self.found_better)\n        else:\n            raise ValueError(\"Mode must be 'any', 'all', or a callable in Evaluator\")\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset early stopping state.\"\"\"\n        self.logger.debug(\"Resetting early stopping state.\")\n        self.current_patience = self.patience\n        self.found_better = [False for _ in range(len(self.window))]\n        self.current_grace_period = self.grace_period\n\n    def add_task(\n        self, delta: float, window: int, metric: str, grace_period: int\n    ) -&gt; None:\n        \"\"\"Add a new task for early stopping.\n\n        Args:\n            delta (float): Minimum change to consider an improvement.\n            window (int): Size of the moving window for smoothing.\n            metric (str): Metric to monitor for early stopping.\n            grace_period (int): Grace period for early stopping.\n        \"\"\"\n        self.logger.debug(\n            f\"Adding new task for early stopping: {metric}, grace period {grace_period}, window {window}, delta {delta}\"\n        )\n        self.delta.append(delta)\n        self.window.append(window)\n        self.metric.append(metric)\n        self.grace_period.append(grace_period)\n        self.current_grace_period.append(grace_period)\n        self.best_score.append(self.init_best_score)\n        self.found_better.append(False)\n        self.num_tasks += 1\n\n    def remove_task(self, index: int) -&gt; None:\n        \"\"\"Remove a task from early stopping.\n\n        Args:\n            index (int): The index of the task to remove.\n        \"\"\"\n        self.logger.debug(f\"Removing task at index {index}\")\n        self.delta.pop(index)\n        self.window.pop(index)\n        self.metric.pop(index)\n        self.grace_period.pop(index)\n        self.best_score.pop(index)\n        self.num_tasks -= 1\n\n    def __call__(self, data: pd.DataFrame | pd.Series) -&gt; bool:\n        \"\"\"Evaluate early stopping criteria. This is done by comparing the last value of data[self.metric] with the current best value recorded. If that value is better than the current best, the current best is updated,\n        patience is reset and 'found_better' is set to True. Otherwise, if the number of datapoints in 'data' is greater than self.window, the patience is decremented.\n\n        Args:\n            data (pd.DataFrame | pd.Series): Recorded evaluation metrics in a pandas structure.\n\n        Returns:\n            bool: True if early stopping criteria are met, False otherwise.\n        \"\"\"\n        self.found_better = [False for _ in range(self.num_tasks)]\n        ds = {}  # dict to hold current metric values\n\n        # go over all registered metrics and check if the model performs better on any of them\n        # then aggregrate the result with 'found_better_model'.\n        for i in range(self.num_tasks):\n            self.logger.debug(f\"Evaluating early stopping criteria for task {i}.\")\n            # prevent a skipped metric from affecting early stopping\n            if self.metric[i] not in data.columns:\n                self.logger.warning(f\"    Metric {self.metric[i]} not found in data.\")\n                self.found_better[i] = True\n                ds[i] = 0.0\n                continue\n\n            if self.smoothing:\n                self.logger.debug(f\"Applying smoothing to metric {self.metric[i]}.\")\n                d = (\n                    data[self.metric[i]]\n                    .rolling(window=self.window[i], min_periods=1)\n                    .mean()\n                )\n            else:\n                self.logger.debug(f\"Using raw metric values for {self.metric[i]}.\")\n                d = data[self.metric[i]]\n\n            check = False\n            if self.minimize:\n                self.logger.debug(\"Early stopping is in 'minimize' mode.\")\n                check = self.best_score[i] - self.delta[i] &gt; d.iloc[-1]\n            else:\n                self.logger.debug(\"Early stopping is in 'maximize' mode.\")\n                check = self.best_score[i] + self.delta[i] &lt; d.iloc[-1]\n\n            if check and len(data):  # always minimize the metric\n                self.logger.info(\n                    f\"    Better model found at task {i}: {d.iloc[-1]:.8f}, current best: {self.best_score[i]:.8f}\"\n                )\n                self.found_better[i] = True\n\n            ds[i] = d.iloc[-1]\n\n        if self.found_better_model:\n            # when we found a better model the stopping patience gets reset\n            self.logger.debug(\"Found better model\")\n            for i in range(self.num_tasks):\n                self.logger.info(\n                    f\"current best score: {self.best_score[i]:.8f}, current score: {ds[i]:.8f}\"\n                )\n                self.best_score[i] = ds[i]  # record best score\n            self.current_patience = self.patience  # reset patience\n\n        # only when all grace periods are done will we reduce the patience\n        elif all([g &lt;= 0 for g in self.current_grace_period]):\n            self.logger.debug(\"All grace periods are done, reducing patience.\")\n            self.current_patience -= 1\n        else:\n            pass\n            # don't do anything here, we want at least 'grace_period' many epochs before patience is reduced\n\n        for i in range(self.num_tasks):\n            self.logger.info(\n                f\"EarlyStopping: current patience: {self.current_patience}, best score: {self.best_score[i]:.8f}, grace_period: {self.current_grace_period[i]}\"\n            )\n\n        for i in range(self.num_tasks):\n            if self.current_grace_period[i] &gt; 0:\n                self.current_grace_period[i] -= 1\n\n        return self.criterion(self, data)\n</code></pre>"},{"location":"api/#QuantumGrav.evaluate.DefaultEarlyStopping.found_better_model","title":"<code>found_better_model</code>  <code>property</code>","text":"<p>Check if a better model has been found.</p>"},{"location":"api/#QuantumGrav.evaluate.DefaultEarlyStopping.__call__","title":"<code>__call__(data)</code>","text":"<p>Evaluate early stopping criteria. This is done by comparing the last value of data[self.metric] with the current best value recorded. If that value is better than the current best, the current best is updated, patience is reset and 'found_better' is set to True. Otherwise, if the number of datapoints in 'data' is greater than self.window, the patience is decremented.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame | Series</code> <p>Recorded evaluation metrics in a pandas structure.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if early stopping criteria are met, False otherwise.</p> Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>def __call__(self, data: pd.DataFrame | pd.Series) -&gt; bool:\n    \"\"\"Evaluate early stopping criteria. This is done by comparing the last value of data[self.metric] with the current best value recorded. If that value is better than the current best, the current best is updated,\n    patience is reset and 'found_better' is set to True. Otherwise, if the number of datapoints in 'data' is greater than self.window, the patience is decremented.\n\n    Args:\n        data (pd.DataFrame | pd.Series): Recorded evaluation metrics in a pandas structure.\n\n    Returns:\n        bool: True if early stopping criteria are met, False otherwise.\n    \"\"\"\n    self.found_better = [False for _ in range(self.num_tasks)]\n    ds = {}  # dict to hold current metric values\n\n    # go over all registered metrics and check if the model performs better on any of them\n    # then aggregrate the result with 'found_better_model'.\n    for i in range(self.num_tasks):\n        self.logger.debug(f\"Evaluating early stopping criteria for task {i}.\")\n        # prevent a skipped metric from affecting early stopping\n        if self.metric[i] not in data.columns:\n            self.logger.warning(f\"    Metric {self.metric[i]} not found in data.\")\n            self.found_better[i] = True\n            ds[i] = 0.0\n            continue\n\n        if self.smoothing:\n            self.logger.debug(f\"Applying smoothing to metric {self.metric[i]}.\")\n            d = (\n                data[self.metric[i]]\n                .rolling(window=self.window[i], min_periods=1)\n                .mean()\n            )\n        else:\n            self.logger.debug(f\"Using raw metric values for {self.metric[i]}.\")\n            d = data[self.metric[i]]\n\n        check = False\n        if self.minimize:\n            self.logger.debug(\"Early stopping is in 'minimize' mode.\")\n            check = self.best_score[i] - self.delta[i] &gt; d.iloc[-1]\n        else:\n            self.logger.debug(\"Early stopping is in 'maximize' mode.\")\n            check = self.best_score[i] + self.delta[i] &lt; d.iloc[-1]\n\n        if check and len(data):  # always minimize the metric\n            self.logger.info(\n                f\"    Better model found at task {i}: {d.iloc[-1]:.8f}, current best: {self.best_score[i]:.8f}\"\n            )\n            self.found_better[i] = True\n\n        ds[i] = d.iloc[-1]\n\n    if self.found_better_model:\n        # when we found a better model the stopping patience gets reset\n        self.logger.debug(\"Found better model\")\n        for i in range(self.num_tasks):\n            self.logger.info(\n                f\"current best score: {self.best_score[i]:.8f}, current score: {ds[i]:.8f}\"\n            )\n            self.best_score[i] = ds[i]  # record best score\n        self.current_patience = self.patience  # reset patience\n\n    # only when all grace periods are done will we reduce the patience\n    elif all([g &lt;= 0 for g in self.current_grace_period]):\n        self.logger.debug(\"All grace periods are done, reducing patience.\")\n        self.current_patience -= 1\n    else:\n        pass\n        # don't do anything here, we want at least 'grace_period' many epochs before patience is reduced\n\n    for i in range(self.num_tasks):\n        self.logger.info(\n            f\"EarlyStopping: current patience: {self.current_patience}, best score: {self.best_score[i]:.8f}, grace_period: {self.current_grace_period[i]}\"\n        )\n\n    for i in range(self.num_tasks):\n        if self.current_grace_period[i] &gt; 0:\n            self.current_grace_period[i] -= 1\n\n    return self.criterion(self, data)\n</code></pre>"},{"location":"api/#QuantumGrav.evaluate.DefaultEarlyStopping.__init__","title":"<code>__init__(patience, delta=[0.0001], window=[7], metric=['loss'], smoothing=False, criterion=lambda early_stopping_instance, data: early_stopping_instance.current_patience &lt;= 0, init_best_score=np.inf, mode='any', grace_period=[0], minimize=False)</code>","text":"<p>Early stopping initialization.</p> <p>Parameters:</p> Name Type Description Default <code>patience</code> <code>int</code> <p>Number of epochs with no improvement after which training will be stopped.</p> required <code>delta</code> <code>float</code> <p>Minimum change to consider an improvement. Defaults to 1e-4.</p> <code>[0.0001]</code> <code>window</code> <code>int</code> <p>Size of the moving window for smoothing. Defaults to 7.</p> <code>[7]</code> <code>metric</code> <code>str</code> <p>Metric to monitor for early stopping. Defaults to \"loss\". This class always assumes that lower values for 'metric' are better.</p> <code>['loss']</code> <code>smoothing</code> <code>bool</code> <p>Whether to apply smoothed mean to the metric to dampen fluctuations. Defaults to False.</p> <code>False</code> <code>criterion</code> <code>Callable</code> <p>Custom stopping criterion. Defaults to a function that stops when patience is exhausted.</p> <code>lambda early_stopping_instance, data: current_patience &lt;= 0</code> <code>init_best_score</code> <code>float</code> <p>initial best score value.</p> <code>inf</code> <code>mode</code> <code>str | Callable[[list[bool]], bool]</code> <p>The mode for early stopping. Can be \"any\", \"all\", or a custom function. Defaults to \"any\". This decides wheather all tracked metrics have to improve or only one of them, or if something else should be done by applying a custom function to the array of evaluation results.</p> <code>'any'</code> <code>minimize</code> <code>bool</code> <p>Bool: Whether to minimize or maximize the criterion</p> <code>False</code> Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>def __init__(\n    self,\n    patience: int,\n    delta: list[float] = [1e-4],\n    window: list[int] = [7],\n    metric: list[str] = [\"loss\"],\n    smoothing: bool = False,\n    criterion: Callable = lambda early_stopping_instance,\n    data: early_stopping_instance.current_patience &lt;= 0,\n    init_best_score: float = np.inf,\n    mode: str | Callable[[list[bool]], bool] = \"any\",\n    grace_period: list[int] = [\n        0,\n    ],\n    minimize: bool = False,\n):\n    \"\"\"Early stopping initialization.\n\n    Args:\n        patience (int): Number of epochs with no improvement after which training will be stopped.\n        delta (float, optional): Minimum change to consider an improvement. Defaults to 1e-4.\n        window (int, optional): Size of the moving window for smoothing. Defaults to 7.\n        metric (str, optional): Metric to monitor for early stopping. Defaults to \"loss\". This class always assumes that lower values for 'metric' are better.\n        smoothing (bool, optional): Whether to apply smoothed mean to the metric to dampen fluctuations. Defaults to False.\n        criterion (Callable, optional): Custom stopping criterion. Defaults to a function that stops when patience is exhausted.\n        init_best_score (float): initial best score value.\n        mode (str | Callable[[list[bool]], bool], optional): The mode for early stopping. Can be \"any\", \"all\", or a custom function. Defaults to \"any\". This decides wheather all tracked metrics have to improve or only one of them, or if something else should be done by applying a custom function to the array of evaluation results.\n        minimize: Bool: Whether to minimize or maximize the criterion\n    \"\"\"\n    lw = len(window)\n    lm = len(metric)\n    ld = len(delta)\n    lg = len(grace_period)\n\n    if min([lw, lm, ld, lg]) != max([lw, lm, ld, lg]):\n        raise ValueError(\n            f\"Inconsistent lengths for early stopping parameters: {lw}, {lm}, {ld}, {lg}\"\n        )\n\n    self.patience = patience\n    self.current_patience = patience\n    self.delta = delta\n    self.window = window\n    self.found_better = [False for _ in range(lw)]\n    self.metric = metric\n    self.init_best_score = init_best_score\n    self.best_score = [init_best_score for _ in range(lw)]\n    self.smoothing = smoothing\n    self.logger = logging.getLogger(__name__)\n    self.criterion = criterion\n    self.mode = mode\n    self.found_better = [False for _ in range(lw)]\n    self.grace_period = grace_period\n    self.current_grace_period = [grace_period[i] for i in range(lg)]\n    self.minimize = minimize\n    self.logger = logging.getLogger(__name__)\n    self.num_tasks = len(self.window)\n</code></pre>"},{"location":"api/#QuantumGrav.evaluate.DefaultEarlyStopping.add_task","title":"<code>add_task(delta, window, metric, grace_period)</code>","text":"<p>Add a new task for early stopping.</p> <p>Parameters:</p> Name Type Description Default <code>delta</code> <code>float</code> <p>Minimum change to consider an improvement.</p> required <code>window</code> <code>int</code> <p>Size of the moving window for smoothing.</p> required <code>metric</code> <code>str</code> <p>Metric to monitor for early stopping.</p> required <code>grace_period</code> <code>int</code> <p>Grace period for early stopping.</p> required Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>def add_task(\n    self, delta: float, window: int, metric: str, grace_period: int\n) -&gt; None:\n    \"\"\"Add a new task for early stopping.\n\n    Args:\n        delta (float): Minimum change to consider an improvement.\n        window (int): Size of the moving window for smoothing.\n        metric (str): Metric to monitor for early stopping.\n        grace_period (int): Grace period for early stopping.\n    \"\"\"\n    self.logger.debug(\n        f\"Adding new task for early stopping: {metric}, grace period {grace_period}, window {window}, delta {delta}\"\n    )\n    self.delta.append(delta)\n    self.window.append(window)\n    self.metric.append(metric)\n    self.grace_period.append(grace_period)\n    self.current_grace_period.append(grace_period)\n    self.best_score.append(self.init_best_score)\n    self.found_better.append(False)\n    self.num_tasks += 1\n</code></pre>"},{"location":"api/#QuantumGrav.evaluate.DefaultEarlyStopping.remove_task","title":"<code>remove_task(index)</code>","text":"<p>Remove a task from early stopping.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the task to remove.</p> required Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>def remove_task(self, index: int) -&gt; None:\n    \"\"\"Remove a task from early stopping.\n\n    Args:\n        index (int): The index of the task to remove.\n    \"\"\"\n    self.logger.debug(f\"Removing task at index {index}\")\n    self.delta.pop(index)\n    self.window.pop(index)\n    self.metric.pop(index)\n    self.grace_period.pop(index)\n    self.best_score.pop(index)\n    self.num_tasks -= 1\n</code></pre>"},{"location":"api/#QuantumGrav.evaluate.DefaultEarlyStopping.reset","title":"<code>reset()</code>","text":"<p>Reset early stopping state.</p> Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset early stopping state.\"\"\"\n    self.logger.debug(\"Resetting early stopping state.\")\n    self.current_patience = self.patience\n    self.found_better = [False for _ in range(len(self.window))]\n    self.current_grace_period = self.grace_period\n</code></pre>"},{"location":"api/#QuantumGrav.evaluate.DefaultEvaluator","title":"<code>DefaultEvaluator</code>","text":"<p>Default evaluator for model evaluation - testing and validation during training</p> Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>class DefaultEvaluator:\n    \"\"\"Default evaluator for model evaluation - testing and validation during training\"\"\"\n\n    def __init__(\n        self,\n        device: str | torch.device | int,\n        criterion: Callable,\n        apply_model: Callable | None = None,\n    ):\n        \"\"\"Default evaluator for model evaluation.\n\n        Args:\n            device (str | torch.device | int): The device to run the evaluation on.\n            criterion (Callable): The loss function to use for evaluation.\n            apply_model (Callable): A function to apply the model to the data.\n        \"\"\"\n        self.criterion = criterion\n        self.apply_model = apply_model\n        self.device = device\n        self.data: pd.DataFrame | list = []\n        self.logger = logging.getLogger(__name__)\n\n    def evaluate(\n        self,\n        model: torch.nn.Module,\n        data_loader: torch_geometric.loader.DataLoader,  # type: ignore\n    ) -&gt; list[Any]:\n        \"\"\"Evaluate the model on the given data loader.\n\n        Args:\n            model (torch.nn.Module): Model to evaluate.\n            data_loader (torch_geometric.loader.DataLoader): Data loader for evaluation.\n\n        Returns:\n             list[Any]: A list of evaluation results.\n        \"\"\"\n        model.eval()\n        current_data = []\n\n        with torch.no_grad():\n            for i, batch in enumerate(data_loader):\n                data = batch.to(self.device)\n                if self.apply_model:\n                    outputs = self.apply_model(model, data)\n                else:\n                    outputs = model(data.x, data.edge_index, data.batch)\n                loss = self.criterion(outputs, data)\n                current_data.append(loss)\n\n        return current_data\n\n    def report(self, data: list | pd.Series | torch.Tensor | np.ndarray) -&gt; None:\n        \"\"\"Report the evaluation results.\n\n        Args:\n            data (list | pd.Series | torch.Tensor | np.ndarray): The evaluation results.\n        \"\"\"\n\n        if isinstance(data, torch.Tensor):\n            data = data.cpu().numpy()\n\n        if isinstance(data, list):\n            for i, d in enumerate(data):\n                if isinstance(d, torch.Tensor):\n                    data[i] = d.cpu().numpy()\n\n        avg = np.mean(data)\n        sigma = np.std(data)\n        self.logger.info(f\"Average loss: {avg}, Standard deviation: {sigma}\")\n\n        if isinstance(self.data, list):\n            self.data.append((avg, sigma))\n        else:\n            self.data = pd.concat(\n                [\n                    self.data,\n                    pd.DataFrame({\"loss\": avg, \"std\": sigma}, index=[0]),\n                ],\n                axis=0,\n                ignore_index=True,\n            )\n</code></pre>"},{"location":"api/#QuantumGrav.evaluate.DefaultEvaluator.__init__","title":"<code>__init__(device, criterion, apply_model=None)</code>","text":"<p>Default evaluator for model evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str | device | int</code> <p>The device to run the evaluation on.</p> required <code>criterion</code> <code>Callable</code> <p>The loss function to use for evaluation.</p> required <code>apply_model</code> <code>Callable</code> <p>A function to apply the model to the data.</p> <code>None</code> Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>def __init__(\n    self,\n    device: str | torch.device | int,\n    criterion: Callable,\n    apply_model: Callable | None = None,\n):\n    \"\"\"Default evaluator for model evaluation.\n\n    Args:\n        device (str | torch.device | int): The device to run the evaluation on.\n        criterion (Callable): The loss function to use for evaluation.\n        apply_model (Callable): A function to apply the model to the data.\n    \"\"\"\n    self.criterion = criterion\n    self.apply_model = apply_model\n    self.device = device\n    self.data: pd.DataFrame | list = []\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"api/#QuantumGrav.evaluate.DefaultEvaluator.evaluate","title":"<code>evaluate(model, data_loader)</code>","text":"<p>Evaluate the model on the given data loader.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to evaluate.</p> required <code>data_loader</code> <code>DataLoader</code> <p>Data loader for evaluation.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>list[Any]: A list of evaluation results.</p> Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>def evaluate(\n    self,\n    model: torch.nn.Module,\n    data_loader: torch_geometric.loader.DataLoader,  # type: ignore\n) -&gt; list[Any]:\n    \"\"\"Evaluate the model on the given data loader.\n\n    Args:\n        model (torch.nn.Module): Model to evaluate.\n        data_loader (torch_geometric.loader.DataLoader): Data loader for evaluation.\n\n    Returns:\n         list[Any]: A list of evaluation results.\n    \"\"\"\n    model.eval()\n    current_data = []\n\n    with torch.no_grad():\n        for i, batch in enumerate(data_loader):\n            data = batch.to(self.device)\n            if self.apply_model:\n                outputs = self.apply_model(model, data)\n            else:\n                outputs = model(data.x, data.edge_index, data.batch)\n            loss = self.criterion(outputs, data)\n            current_data.append(loss)\n\n    return current_data\n</code></pre>"},{"location":"api/#QuantumGrav.evaluate.DefaultEvaluator.report","title":"<code>report(data)</code>","text":"<p>Report the evaluation results.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list | Series | Tensor | ndarray</code> <p>The evaluation results.</p> required Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>def report(self, data: list | pd.Series | torch.Tensor | np.ndarray) -&gt; None:\n    \"\"\"Report the evaluation results.\n\n    Args:\n        data (list | pd.Series | torch.Tensor | np.ndarray): The evaluation results.\n    \"\"\"\n\n    if isinstance(data, torch.Tensor):\n        data = data.cpu().numpy()\n\n    if isinstance(data, list):\n        for i, d in enumerate(data):\n            if isinstance(d, torch.Tensor):\n                data[i] = d.cpu().numpy()\n\n    avg = np.mean(data)\n    sigma = np.std(data)\n    self.logger.info(f\"Average loss: {avg}, Standard deviation: {sigma}\")\n\n    if isinstance(self.data, list):\n        self.data.append((avg, sigma))\n    else:\n        self.data = pd.concat(\n            [\n                self.data,\n                pd.DataFrame({\"loss\": avg, \"std\": sigma}, index=[0]),\n            ],\n            axis=0,\n            ignore_index=True,\n        )\n</code></pre>"},{"location":"api/#QuantumGrav.evaluate.DefaultTester","title":"<code>DefaultTester</code>","text":"<p>               Bases: <code>DefaultEvaluator</code></p> <p>Default tester for model testing.</p> <p>Parameters:</p> Name Type Description Default <code>DefaultEvaluator</code> <code>Class</code> <p>Inherits from DefaultEvaluator and provides functionality for validating models</p> required <p>using a specified criterion and optional model application function.</p> Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>class DefaultTester(DefaultEvaluator):\n    \"\"\"Default tester for model testing.\n\n    Args:\n        DefaultEvaluator (Class): Inherits from DefaultEvaluator and provides functionality for validating models\n    using a specified criterion and optional model application function.\n    \"\"\"\n\n    def __init__(\n        self,\n        device: str | torch.device | int,\n        criterion: Callable,\n        apply_model: Callable | None = None,\n    ):\n        \"\"\"Default tester for model testing.\n\n        Args:\n            device (str | torch.device | int,): The device to run the testing on.\n            criterion (Callable): The loss function to use for testing.\n            apply_model (Callable): A function to apply the model to the data.\n        \"\"\"\n        super().__init__(device, criterion, apply_model)\n\n    def test(\n        self,\n        model: torch.nn.Module,\n        data_loader: torch_geometric.loader.DataLoader,  # type: ignore\n    ) -&gt; list[Any]:\n        \"\"\"Test the model on the given data loader.\n\n        Args:\n            model (torch.nn.Module): Model to test.\n            data_loader (torch_geometric.loader.DataLoader): Data loader for testing.\n\n        Returns:\n            list[Any]: A list of testing results.\n        \"\"\"\n        return self.evaluate(model, data_loader)\n</code></pre>"},{"location":"api/#QuantumGrav.evaluate.DefaultTester.__init__","title":"<code>__init__(device, criterion, apply_model=None)</code>","text":"<p>Default tester for model testing.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>(str | device | int,)</code> <p>The device to run the testing on.</p> required <code>criterion</code> <code>Callable</code> <p>The loss function to use for testing.</p> required <code>apply_model</code> <code>Callable</code> <p>A function to apply the model to the data.</p> <code>None</code> Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>def __init__(\n    self,\n    device: str | torch.device | int,\n    criterion: Callable,\n    apply_model: Callable | None = None,\n):\n    \"\"\"Default tester for model testing.\n\n    Args:\n        device (str | torch.device | int,): The device to run the testing on.\n        criterion (Callable): The loss function to use for testing.\n        apply_model (Callable): A function to apply the model to the data.\n    \"\"\"\n    super().__init__(device, criterion, apply_model)\n</code></pre>"},{"location":"api/#QuantumGrav.evaluate.DefaultTester.test","title":"<code>test(model, data_loader)</code>","text":"<p>Test the model on the given data loader.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to test.</p> required <code>data_loader</code> <code>DataLoader</code> <p>Data loader for testing.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>list[Any]: A list of testing results.</p> Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>def test(\n    self,\n    model: torch.nn.Module,\n    data_loader: torch_geometric.loader.DataLoader,  # type: ignore\n) -&gt; list[Any]:\n    \"\"\"Test the model on the given data loader.\n\n    Args:\n        model (torch.nn.Module): Model to test.\n        data_loader (torch_geometric.loader.DataLoader): Data loader for testing.\n\n    Returns:\n        list[Any]: A list of testing results.\n    \"\"\"\n    return self.evaluate(model, data_loader)\n</code></pre>"},{"location":"api/#QuantumGrav.evaluate.DefaultValidator","title":"<code>DefaultValidator</code>","text":"<p>               Bases: <code>DefaultEvaluator</code></p> <p>Default validator for model validation.</p> <p>Parameters:</p> Name Type Description Default <code>DefaultEvaluator</code> <code>Class</code> <p>Inherits from DefaultEvaluator and provides functionality for validating models</p> required <p>using a specified criterion and optional model application function.</p> Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>class DefaultValidator(DefaultEvaluator):\n    \"\"\"Default validator for model validation.\n\n    Args:\n        DefaultEvaluator (Class): Inherits from DefaultEvaluator and provides functionality for validating models\n    using a specified criterion and optional model application function.\n    \"\"\"\n\n    def __init__(\n        self,\n        device: str | torch.device | int,\n        criterion: Callable,\n        apply_model: Callable | None = None,\n    ):\n        \"\"\"Default validator for model validation.\n\n        Args:\n            device (str | torch.device | int,): The device to run the validation on.\n            criterion (Callable): The loss function to use for validation.\n            apply_model (Callable | None, optional): A function to apply the model to the data. Defaults to None.\n        \"\"\"\n        super().__init__(device, criterion, apply_model)\n\n    def validate(\n        self,\n        model: torch.nn.Module,\n        data_loader: torch_geometric.loader.DataLoader,  # type: ignore\n    ) -&gt; list[Any]:\n        \"\"\"Validate the model on the given data loader.\n\n        Args:\n            model (torch.nn.Module): Model to validate.\n            data_loader (torch_geometric.loader.DataLoader): Data loader for validation.\n        Returns:\n            list[Any]: A list of validation results.\n        \"\"\"\n        return self.evaluate(model, data_loader)\n</code></pre>"},{"location":"api/#QuantumGrav.evaluate.DefaultValidator.__init__","title":"<code>__init__(device, criterion, apply_model=None)</code>","text":"<p>Default validator for model validation.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>(str | device | int,)</code> <p>The device to run the validation on.</p> required <code>criterion</code> <code>Callable</code> <p>The loss function to use for validation.</p> required <code>apply_model</code> <code>Callable | None</code> <p>A function to apply the model to the data. Defaults to None.</p> <code>None</code> Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>def __init__(\n    self,\n    device: str | torch.device | int,\n    criterion: Callable,\n    apply_model: Callable | None = None,\n):\n    \"\"\"Default validator for model validation.\n\n    Args:\n        device (str | torch.device | int,): The device to run the validation on.\n        criterion (Callable): The loss function to use for validation.\n        apply_model (Callable | None, optional): A function to apply the model to the data. Defaults to None.\n    \"\"\"\n    super().__init__(device, criterion, apply_model)\n</code></pre>"},{"location":"api/#QuantumGrav.evaluate.DefaultValidator.validate","title":"<code>validate(model, data_loader)</code>","text":"<p>Validate the model on the given data loader.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to validate.</p> required <code>data_loader</code> <code>DataLoader</code> <p>Data loader for validation.</p> required <p>Returns:     list[Any]: A list of validation results.</p> Source code in <code>src/QuantumGrav/evaluate.py</code> <pre><code>def validate(\n    self,\n    model: torch.nn.Module,\n    data_loader: torch_geometric.loader.DataLoader,  # type: ignore\n) -&gt; list[Any]:\n    \"\"\"Validate the model on the given data loader.\n\n    Args:\n        model (torch.nn.Module): Model to validate.\n        data_loader (torch_geometric.loader.DataLoader): Data loader for validation.\n    Returns:\n        list[Any]: A list of validation results.\n    \"\"\"\n    return self.evaluate(model, data_loader)\n</code></pre>"},{"location":"api/#datasets","title":"Datasets","text":"<p>The package supports three kinds of datasets with a common baseclass <code>QGDatasetBase</code>. For the basics of how those work, check out the pytorch-geometric documentation of dataset</p> <p>These are:  - <code>QGDataset</code>: A dataset that relies on an on-disk storage of the processed data. It lazily loads csets from disk when needed.  - <code>QGDatasetInMemory</code>: A dataset that holds the entire processed dataset in memory at once.  - <code>QGDatasetOnthefly</code>: This dataset does not hold anything on disk or in memory, but creates the data on demand from some supplied Julia code. </p>"},{"location":"api/#dataset-base-class","title":"Dataset base class","text":""},{"location":"api/#QuantumGrav.dataset_base.QGDatasetBase","title":"<code>QGDatasetBase</code>","text":"<p>Mixin class that provides common functionality for the dataset classes. Works only for file-based datasets. Provides methods for processing data.</p> Source code in <code>src/QuantumGrav/dataset_base.py</code> <pre><code>class QGDatasetBase:\n    \"\"\"Mixin class that provides common functionality for the dataset classes. Works only for file-based datasets. Provides methods for processing data.\"\"\"\n\n    def __init__(\n        self,\n        input: list[str | Path],\n        output: str | Path,\n        reader: Callable[\n            [zarr.Group, torch.dtype, torch.dtype, bool],\n            list[Data],\n        ]\n        | None = None,\n        float_type: torch.dtype = torch.float32,\n        int_type: torch.dtype = torch.int64,\n        validate_data: bool = True,\n        n_processes: int = 1,\n        chunksize: int = 1000,\n        **kwargs,\n    ):\n        \"\"\"Initialize a DatasetMixin instance. This class is designed to handle the loading, processing, and writing of QuantumGrav datasets. It provides a common interface for both in-memory and on-disk datasets. It is not to be instantiated directly, but rather used as a mixin for other dataset classes.\n\n        Args:\n            input (list[str  |  Path] : The list of input files for the dataset, or a callable that generates a set of input files.\n            output (str | Path): The output directory where processed data will be stored.\n            reader (Callable[[zarr.Group, torch.dtype, torch.dtype, bool], list[Data]] | None, optional): A function to load data from a file. Defaults to None.\n            float_type (torch.dtype, optional): The data type to use for floating point values. Defaults to torch.float32.\n            int_type (torch.dtype, optional): The data type to use for integer values. Defaults to torch.int64.\n            validate_data (bool, optional): Whether to validate the data after loading. Defaults to True.\n            n_processes (int, optional): The number of processes to use for parallel processing of read data. Defaults to 1.\n            chunksize (int, optional): The size of the chunks to process in parallel. Defaults to 1000.\n\n        Raises:\n            ValueError: If one of the input data files does not exist\n            ValueError: If the metadata retrieval function is invalid.\n            FileNotFoundError: If an input file does not exist.\n        \"\"\"\n        if reader is None:\n            raise ValueError(\"A reader function must be provided to load the data.\")\n\n        self.input = input\n        for file in self.input:\n            if Path(file).exists() is False:\n                raise FileNotFoundError(f\"Input file {file} does not exist.\")\n\n        self.output = output\n        self.data_reader = reader\n        self.metadata = {}\n        self.float_type = float_type\n        self.int_type = int_type\n        self.validate_data = validate_data\n        self.n_processes = n_processes\n        self.chunksize = chunksize\n\n        # get the number of samples in the dataset\n        self._num_samples = 0\n\n        for filepath in self.input:\n            if not Path(filepath).exists():\n                raise FileNotFoundError(f\"Input file {filepath} does not exist.\")\n            self._num_samples += self._get_num_samples_per_file(filepath)\n\n        # ensure the input is a list of paths\n        if Path(self.processed_dir).exists():\n            with open(Path(self.processed_dir) / \"metadata.yaml\", \"r\") as f:\n                self.metadata = yaml.load(f, Loader=yaml.FullLoader)\n        else:\n            Path(self.processed_dir).mkdir(parents=True, exist_ok=True)\n            self.metadata = {\n                \"num_samples\": int(self._num_samples),\n                \"input\": [str(Path(f).resolve().absolute()) for f in self.input],\n                \"output\": str(Path(self.output).resolve().absolute()),\n                \"float_type\": str(self.float_type),\n                \"int_type\": str(self.int_type),\n                \"validate_data\": self.validate_data,\n                \"n_processes\": self.n_processes,\n                \"chunksize\": self.chunksize,\n            }\n\n            with open(Path(self.processed_dir) / \"metadata.yaml\", \"w\") as f:\n                yaml.dump(self.metadata, f)\n\n    def _get_num_samples_per_file(self, filepath: str | Path) -&gt; int:\n        \"\"\"Get the number of samples in a given file.\n\n        Args:\n            filepath (str | Path): The path to the file.\n\n        Raises:\n            ValueError: If the file is not a valid Zarr file.\n\n        Returns:\n            int: The number of samples in the file.\n        \"\"\"\n\n        # try to find the sample number from a dedicated dataset\n        def try_find_numsamples(f):\n            s = None\n            for name in [\"num_causal_sets\", \"num_samples\"]:\n                if name in f:\n                    s = f[name]\n                    break\n            return s\n\n        # ... if that fails, we try to read it from any scalar dataset.\n        # ... if we can\u00b4t because they are of unequal sizes, we return None\n        # ... to indicate an unresolvable state\n        def fallback(f) -&gt; int | None:\n            # find scalar datasets and use their sizes to determine size\n            shapes = [f[k].shape[0] for k in f.keys() if len(f[k].shape) == 1]\n            max_shape = max(shapes)\n            min_shape = min(shapes)\n            if max_shape != min_shape:\n                return None\n            else:\n                return max_shape\n\n        # same logic for Zarr\n        try:\n            group = zarr.open_group(\n                zarr.storage.LocalStore(filepath, read_only=True),\n                path=\"\",\n                mode=\"r\",\n            )\n            # note that fallback returns an int directly,\n            # while for try_find_numsamples we need to index into the result\n            s = try_find_numsamples(group)\n            if s is not None:\n                return s[0]\n            else:\n                s = fallback(group)\n                if s is not None:\n                    return s\n                else:\n                    raise RuntimeError(\"Unable to determine number of samples.\")\n        except Exception:\n            # we need an extra fallback for zarr b/c Julia Zarr and python Zarr\n            # can differ in layout - Julia Zarr does not have to have a group\n            try:\n                store = zarr.storage.LocalStore(filepath, read_only=True)\n                arr = zarr.open_array(store, path=\"adjacency_matrix\")\n                s = max(arr.shape)\n                return s\n            except Exception:\n                raise\n\n    @property\n    def processed_dir(self) -&gt; str:\n        \"\"\"Get the path to the processed directory.\n\n        Returns:\n            str: The path to the processed directory, or None if it doesn't exist.\n        \"\"\"\n        processed_path = Path(self.output).resolve().absolute() / \"processed\"\n        return str(processed_path)\n\n    @property\n    def raw_file_names(self) -&gt; list[str]:\n        \"\"\"Get the raw file paths from the input list.\n\n        Returns:\n            list[str]: A list of raw file paths.\n        \"\"\"\n        suf = \".zarr\"\n        return [str(Path(f).name) for f in self.input if Path(f).suffix == suf]\n\n    @property\n    def processed_file_names(self) -&gt; list[str]:\n        \"\"\"Get a list of processed files in the processed directory.\n\n        Returns:\n            list[str]: A list of processed file paths, excluding JSON files.\n        \"\"\"\n        if not Path(self.processed_dir).exists():\n            return []\n\n        return [\n            str(f.name)\n            for f in Path(self.processed_dir).iterdir()\n            if f.is_file() and f.suffix == \".pt\" and \"data\" in f.name\n        ]\n\n    def process_chunk(\n        self,\n        store: zarr.storage.LocalStore,\n        start: int,\n        pre_transform: Callable[[Data | Collection], Data] | None = None,\n        pre_filter: Callable[[Data | Collection], bool] | None = None,\n    ) -&gt; Sequence[Data]:\n        \"\"\"Process a chunk of data from the raw file. This method is intended to be used in the data loading pipeline to read a chunk of data, apply transformations, and filter the read data, and thus should not be called directly.\n\n        Args:\n            store (zarr.storage.LocalStore): local zarr storage\n            start (int): start index\n            pre_transform (Callable[[Data], Data] | None, optional): Transformation that adds additional features to the data. Defaults to None.\n            pre_filter (Callable[[Data], bool] | None, optional): A function that filters the data. Defaults to None.\n\n        Returns:\n            list[Data]: The processed data or None if the chunk is empty.\n        \"\"\"\n        N = self._get_num_samples_per_file(store.root)\n        rootgroup = zarr.open_group(store.root)\n\n        def process_item(i: int):\n            item = self.data_reader(\n                rootgroup,\n                i,\n                self.float_type,\n                self.int_type,\n                self.validate_data,\n            )\n            if pre_filter is not None and not pre_filter(item):\n                return None\n            if pre_transform is not None:\n                return pre_transform(item)\n            return item\n\n        if self.n_processes &gt; 1:\n            results = Parallel(n_jobs=self.n_processes)(\n                delayed(process_item)(i)\n                for i in range(start, min(start + self.chunksize, N))\n            )\n        else:\n            results = [\n                process_item(i) for i in range(start, min(start + self.chunksize, N))\n            ]\n\n        return [res for res in results if res is not None]\n</code></pre>"},{"location":"api/#QuantumGrav.dataset_base.QGDatasetBase.processed_dir","title":"<code>processed_dir</code>  <code>property</code>","text":"<p>Get the path to the processed directory.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the processed directory, or None if it doesn't exist.</p>"},{"location":"api/#QuantumGrav.dataset_base.QGDatasetBase.processed_file_names","title":"<code>processed_file_names</code>  <code>property</code>","text":"<p>Get a list of processed files in the processed directory.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of processed file paths, excluding JSON files.</p>"},{"location":"api/#QuantumGrav.dataset_base.QGDatasetBase.raw_file_names","title":"<code>raw_file_names</code>  <code>property</code>","text":"<p>Get the raw file paths from the input list.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of raw file paths.</p>"},{"location":"api/#QuantumGrav.dataset_base.QGDatasetBase.__init__","title":"<code>__init__(input, output, reader=None, float_type=torch.float32, int_type=torch.int64, validate_data=True, n_processes=1, chunksize=1000, **kwargs)</code>","text":"<p>Initialize a DatasetMixin instance. This class is designed to handle the loading, processing, and writing of QuantumGrav datasets. It provides a common interface for both in-memory and on-disk datasets. It is not to be instantiated directly, but rather used as a mixin for other dataset classes.</p> <p>Parameters:</p> Name Type Description Default <code>input (list[str  |  Path] </code> <p>The list of input files for the dataset, or a callable that generates a set of input files.</p> required <code>output</code> <code>str | Path</code> <p>The output directory where processed data will be stored.</p> required <code>reader</code> <code>Callable[[Group, dtype, dtype, bool], list[Data]] | None</code> <p>A function to load data from a file. Defaults to None.</p> <code>None</code> <code>float_type</code> <code>dtype</code> <p>The data type to use for floating point values. Defaults to torch.float32.</p> <code>float32</code> <code>int_type</code> <code>dtype</code> <p>The data type to use for integer values. Defaults to torch.int64.</p> <code>int64</code> <code>validate_data</code> <code>bool</code> <p>Whether to validate the data after loading. Defaults to True.</p> <code>True</code> <code>n_processes</code> <code>int</code> <p>The number of processes to use for parallel processing of read data. Defaults to 1.</p> <code>1</code> <code>chunksize</code> <code>int</code> <p>The size of the chunks to process in parallel. Defaults to 1000.</p> <code>1000</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If one of the input data files does not exist</p> <code>ValueError</code> <p>If the metadata retrieval function is invalid.</p> <code>FileNotFoundError</code> <p>If an input file does not exist.</p> Source code in <code>src/QuantumGrav/dataset_base.py</code> <pre><code>def __init__(\n    self,\n    input: list[str | Path],\n    output: str | Path,\n    reader: Callable[\n        [zarr.Group, torch.dtype, torch.dtype, bool],\n        list[Data],\n    ]\n    | None = None,\n    float_type: torch.dtype = torch.float32,\n    int_type: torch.dtype = torch.int64,\n    validate_data: bool = True,\n    n_processes: int = 1,\n    chunksize: int = 1000,\n    **kwargs,\n):\n    \"\"\"Initialize a DatasetMixin instance. This class is designed to handle the loading, processing, and writing of QuantumGrav datasets. It provides a common interface for both in-memory and on-disk datasets. It is not to be instantiated directly, but rather used as a mixin for other dataset classes.\n\n    Args:\n        input (list[str  |  Path] : The list of input files for the dataset, or a callable that generates a set of input files.\n        output (str | Path): The output directory where processed data will be stored.\n        reader (Callable[[zarr.Group, torch.dtype, torch.dtype, bool], list[Data]] | None, optional): A function to load data from a file. Defaults to None.\n        float_type (torch.dtype, optional): The data type to use for floating point values. Defaults to torch.float32.\n        int_type (torch.dtype, optional): The data type to use for integer values. Defaults to torch.int64.\n        validate_data (bool, optional): Whether to validate the data after loading. Defaults to True.\n        n_processes (int, optional): The number of processes to use for parallel processing of read data. Defaults to 1.\n        chunksize (int, optional): The size of the chunks to process in parallel. Defaults to 1000.\n\n    Raises:\n        ValueError: If one of the input data files does not exist\n        ValueError: If the metadata retrieval function is invalid.\n        FileNotFoundError: If an input file does not exist.\n    \"\"\"\n    if reader is None:\n        raise ValueError(\"A reader function must be provided to load the data.\")\n\n    self.input = input\n    for file in self.input:\n        if Path(file).exists() is False:\n            raise FileNotFoundError(f\"Input file {file} does not exist.\")\n\n    self.output = output\n    self.data_reader = reader\n    self.metadata = {}\n    self.float_type = float_type\n    self.int_type = int_type\n    self.validate_data = validate_data\n    self.n_processes = n_processes\n    self.chunksize = chunksize\n\n    # get the number of samples in the dataset\n    self._num_samples = 0\n\n    for filepath in self.input:\n        if not Path(filepath).exists():\n            raise FileNotFoundError(f\"Input file {filepath} does not exist.\")\n        self._num_samples += self._get_num_samples_per_file(filepath)\n\n    # ensure the input is a list of paths\n    if Path(self.processed_dir).exists():\n        with open(Path(self.processed_dir) / \"metadata.yaml\", \"r\") as f:\n            self.metadata = yaml.load(f, Loader=yaml.FullLoader)\n    else:\n        Path(self.processed_dir).mkdir(parents=True, exist_ok=True)\n        self.metadata = {\n            \"num_samples\": int(self._num_samples),\n            \"input\": [str(Path(f).resolve().absolute()) for f in self.input],\n            \"output\": str(Path(self.output).resolve().absolute()),\n            \"float_type\": str(self.float_type),\n            \"int_type\": str(self.int_type),\n            \"validate_data\": self.validate_data,\n            \"n_processes\": self.n_processes,\n            \"chunksize\": self.chunksize,\n        }\n\n        with open(Path(self.processed_dir) / \"metadata.yaml\", \"w\") as f:\n            yaml.dump(self.metadata, f)\n</code></pre>"},{"location":"api/#QuantumGrav.dataset_base.QGDatasetBase.process_chunk","title":"<code>process_chunk(store, start, pre_transform=None, pre_filter=None)</code>","text":"<p>Process a chunk of data from the raw file. This method is intended to be used in the data loading pipeline to read a chunk of data, apply transformations, and filter the read data, and thus should not be called directly.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>LocalStore</code> <p>local zarr storage</p> required <code>start</code> <code>int</code> <p>start index</p> required <code>pre_transform</code> <code>Callable[[Data], Data] | None</code> <p>Transformation that adds additional features to the data. Defaults to None.</p> <code>None</code> <code>pre_filter</code> <code>Callable[[Data], bool] | None</code> <p>A function that filters the data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Sequence[Data]</code> <p>list[Data]: The processed data or None if the chunk is empty.</p> Source code in <code>src/QuantumGrav/dataset_base.py</code> <pre><code>def process_chunk(\n    self,\n    store: zarr.storage.LocalStore,\n    start: int,\n    pre_transform: Callable[[Data | Collection], Data] | None = None,\n    pre_filter: Callable[[Data | Collection], bool] | None = None,\n) -&gt; Sequence[Data]:\n    \"\"\"Process a chunk of data from the raw file. This method is intended to be used in the data loading pipeline to read a chunk of data, apply transformations, and filter the read data, and thus should not be called directly.\n\n    Args:\n        store (zarr.storage.LocalStore): local zarr storage\n        start (int): start index\n        pre_transform (Callable[[Data], Data] | None, optional): Transformation that adds additional features to the data. Defaults to None.\n        pre_filter (Callable[[Data], bool] | None, optional): A function that filters the data. Defaults to None.\n\n    Returns:\n        list[Data]: The processed data or None if the chunk is empty.\n    \"\"\"\n    N = self._get_num_samples_per_file(store.root)\n    rootgroup = zarr.open_group(store.root)\n\n    def process_item(i: int):\n        item = self.data_reader(\n            rootgroup,\n            i,\n            self.float_type,\n            self.int_type,\n            self.validate_data,\n        )\n        if pre_filter is not None and not pre_filter(item):\n            return None\n        if pre_transform is not None:\n            return pre_transform(item)\n        return item\n\n    if self.n_processes &gt; 1:\n        results = Parallel(n_jobs=self.n_processes)(\n            delayed(process_item)(i)\n            for i in range(start, min(start + self.chunksize, N))\n        )\n    else:\n        results = [\n            process_item(i) for i in range(start, min(start + self.chunksize, N))\n        ]\n\n    return [res for res in results if res is not None]\n</code></pre>"},{"location":"api/#dataset-loading-data-from-disk","title":"Dataset loading data from disk","text":""},{"location":"api/#QuantumGrav.dataset_ondisk.QGDataset","title":"<code>QGDataset</code>","text":"<p>               Bases: <code>QGDatasetBase</code>, <code>Dataset</code></p> <p>A dataset class for QuantumGrav data that is designed to handle large datasets stored on disk. This class provides methods for loading, processing, and writing data that are common to both in-memory and on-disk datasets.</p> Source code in <code>src/QuantumGrav/dataset_ondisk.py</code> <pre><code>class QGDataset(QGDatasetBase, Dataset):\n    \"\"\"A dataset class for QuantumGrav data that is designed to handle large datasets stored on disk. This class provides methods for loading, processing, and writing data that are common to both in-memory and on-disk datasets.\"\"\"\n\n    def __init__(\n        self,\n        input: list[str | Path],\n        output: str | Path,\n        reader: Callable[[zarr.Group, torch.dtype, torch.dtype, bool], list[Data]]\n        | None = None,\n        float_type: torch.dtype = torch.float32,\n        int_type: torch.dtype = torch.int64,\n        validate_data: bool = True,\n        chunksize: int = 1000,\n        n_processes: int = 1,\n        # dataset properties\n        transform: Callable[[Data | Collection[Any]], Data] | None = None,\n        pre_transform: Callable[[Data | Collection[Any]], Data] | None = None,\n        pre_filter: Callable[[Data | Collection[Any]], bool] | None = None,\n    ):\n        \"\"\"Create a new QGDataset instance. This class is designed to handle the loading, processing, and writing of QuantumGrav datasets that are stored on disk.\n\n        Args:\n            input (list[str  |  Path] | Callable[[Any], dict]): List of input zarr file paths.\n            output (str | Path): Output directory where processed data will be stored.\n            reader (Callable[[zarr.Group, int], list[Data]] | None, optional): Function to read data from the zarr file. Defaults to None.\n            float_type (torch.dtype, optional): Data type for float tensors. Defaults to torch.float32.\n            int_type (torch.dtype, optional): Data type for int tensors. Defaults to torch.int64.\n            validate_data (bool, optional): Whether to validate the data. Defaults to True.\n            chunksize (int, optional): Size of data chunks to process at once. Defaults to 1000.\n            n_processes (int, optional): Number of processes to use for data loading. Defaults to 1.\n            transform (Callable[[Data], Data] | None, optional): Function to transform the data. Defaults to None.\n            pre_transform (Callable[[Data], Data] | None, optional): Function to pre-transform the data. Defaults to None.\n            pre_filter (Callable[[Data], bool] | None, optional): Function to pre-filter the data. Defaults to None.\n        \"\"\"\n\n        QGDatasetBase.__init__(\n            self,\n            input,\n            output,\n            reader=reader,\n            float_type=float_type,\n            int_type=int_type,\n            validate_data=validate_data,\n            chunksize=chunksize,\n            n_processes=n_processes,\n        )\n\n        Dataset.__init__(\n            self,\n            root=output,\n            transform=transform,\n            pre_transform=pre_transform,\n            pre_filter=pre_filter,\n        )\n\n    def write_data(self, data: list[Data], idx: int) -&gt; int:\n        \"\"\"Write the processed data to disk using `torch.save`. This is a default implementation that can be overridden by subclasses, and is intended to be used in the data loading pipeline. Thus, is not intended to be called directly.\n\n        Args:\n            data (list[Data]): The list of Data objects to write to disk.\n            idx (int): The index to use for naming the files.\n        \"\"\"\n        if not Path(self.processed_dir).exists():\n            Path(self.processed_dir).mkdir(parents=True, exist_ok=True)\n\n        for d in data:\n            if d is not None:\n                file_path = Path(self.processed_dir) / f\"data_{idx}.pt\"\n                torch.save(d, file_path)\n                idx += 1\n        return idx\n\n    def process(self) -&gt; None:\n        \"\"\"Process the dataset from the read rawdata into its final form.\"\"\"\n        # process data files\n        k = 0  # index to create the filenames for the processed data\n        for file in self.input:\n            N = self._get_num_samples_per_file(Path(file).resolve().absolute())\n\n            num_chunks = N // self.chunksize\n\n            raw_file = zarr.storage.LocalStore(\n                str(Path(file).resolve().absolute()), read_only=True\n            )\n\n            for i in range(0, num_chunks * self.chunksize, self.chunksize):\n                data = self.process_chunk(\n                    raw_file,\n                    i,\n                    pre_transform=self.pre_transform,\n                    pre_filter=self.pre_filter,\n                )\n\n                k = self.write_data(data, k)\n\n            # final chunk processing\n            data = self.process_chunk(\n                raw_file,\n                num_chunks * self.chunksize,\n                pre_transform=self.pre_transform,\n                pre_filter=self.pre_filter,\n            )\n\n            k = self.write_data(data, k)\n\n            raw_file.close()\n\n    def get(self, idx: int) -&gt; Data:\n        \"\"\"Get a single data sample by index.\"\"\"\n        if self._num_samples is None:\n            raise ValueError(\"Dataset has not been processed yet.\")\n\n        # if idx &lt; 0 or idx &gt;= self._num_samples:\n        #     raise IndexError(\"Index out of bounds.\")\n        # Load the data from the processed files\n        datapoint = torch.load(\n            Path(self.processed_dir) / f\"data_{idx}.pt\", weights_only=False\n        )\n        if self.transform is not None:\n            datapoint = self.transform(datapoint)\n        return datapoint\n\n    def __getitem__(self, idx: int | Sequence[int]) -&gt; Data | Sequence[Data]:\n        if isinstance(idx, int):\n            return self.get(idx)\n        else:\n            return [self.get(i) for i in idx]\n\n    def len(self) -&gt; int:\n        \"\"\"Get the number of samples in the dataset.\n\n        Returns:\n            int: The number of samples in the dataset.\n        \"\"\"\n        return len(self.processed_file_names)\n</code></pre>"},{"location":"api/#QuantumGrav.dataset_ondisk.QGDataset.__init__","title":"<code>__init__(input, output, reader=None, float_type=torch.float32, int_type=torch.int64, validate_data=True, chunksize=1000, n_processes=1, transform=None, pre_transform=None, pre_filter=None)</code>","text":"<p>Create a new QGDataset instance. This class is designed to handle the loading, processing, and writing of QuantumGrav datasets that are stored on disk.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>list[str | Path] | Callable[[Any], dict]</code> <p>List of input zarr file paths.</p> required <code>output</code> <code>str | Path</code> <p>Output directory where processed data will be stored.</p> required <code>reader</code> <code>Callable[[Group, int], list[Data]] | None</code> <p>Function to read data from the zarr file. Defaults to None.</p> <code>None</code> <code>float_type</code> <code>dtype</code> <p>Data type for float tensors. Defaults to torch.float32.</p> <code>float32</code> <code>int_type</code> <code>dtype</code> <p>Data type for int tensors. Defaults to torch.int64.</p> <code>int64</code> <code>validate_data</code> <code>bool</code> <p>Whether to validate the data. Defaults to True.</p> <code>True</code> <code>chunksize</code> <code>int</code> <p>Size of data chunks to process at once. Defaults to 1000.</p> <code>1000</code> <code>n_processes</code> <code>int</code> <p>Number of processes to use for data loading. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Callable[[Data], Data] | None</code> <p>Function to transform the data. Defaults to None.</p> <code>None</code> <code>pre_transform</code> <code>Callable[[Data], Data] | None</code> <p>Function to pre-transform the data. Defaults to None.</p> <code>None</code> <code>pre_filter</code> <code>Callable[[Data], bool] | None</code> <p>Function to pre-filter the data. Defaults to None.</p> <code>None</code> Source code in <code>src/QuantumGrav/dataset_ondisk.py</code> <pre><code>def __init__(\n    self,\n    input: list[str | Path],\n    output: str | Path,\n    reader: Callable[[zarr.Group, torch.dtype, torch.dtype, bool], list[Data]]\n    | None = None,\n    float_type: torch.dtype = torch.float32,\n    int_type: torch.dtype = torch.int64,\n    validate_data: bool = True,\n    chunksize: int = 1000,\n    n_processes: int = 1,\n    # dataset properties\n    transform: Callable[[Data | Collection[Any]], Data] | None = None,\n    pre_transform: Callable[[Data | Collection[Any]], Data] | None = None,\n    pre_filter: Callable[[Data | Collection[Any]], bool] | None = None,\n):\n    \"\"\"Create a new QGDataset instance. This class is designed to handle the loading, processing, and writing of QuantumGrav datasets that are stored on disk.\n\n    Args:\n        input (list[str  |  Path] | Callable[[Any], dict]): List of input zarr file paths.\n        output (str | Path): Output directory where processed data will be stored.\n        reader (Callable[[zarr.Group, int], list[Data]] | None, optional): Function to read data from the zarr file. Defaults to None.\n        float_type (torch.dtype, optional): Data type for float tensors. Defaults to torch.float32.\n        int_type (torch.dtype, optional): Data type for int tensors. Defaults to torch.int64.\n        validate_data (bool, optional): Whether to validate the data. Defaults to True.\n        chunksize (int, optional): Size of data chunks to process at once. Defaults to 1000.\n        n_processes (int, optional): Number of processes to use for data loading. Defaults to 1.\n        transform (Callable[[Data], Data] | None, optional): Function to transform the data. Defaults to None.\n        pre_transform (Callable[[Data], Data] | None, optional): Function to pre-transform the data. Defaults to None.\n        pre_filter (Callable[[Data], bool] | None, optional): Function to pre-filter the data. Defaults to None.\n    \"\"\"\n\n    QGDatasetBase.__init__(\n        self,\n        input,\n        output,\n        reader=reader,\n        float_type=float_type,\n        int_type=int_type,\n        validate_data=validate_data,\n        chunksize=chunksize,\n        n_processes=n_processes,\n    )\n\n    Dataset.__init__(\n        self,\n        root=output,\n        transform=transform,\n        pre_transform=pre_transform,\n        pre_filter=pre_filter,\n    )\n</code></pre>"},{"location":"api/#QuantumGrav.dataset_ondisk.QGDataset.get","title":"<code>get(idx)</code>","text":"<p>Get a single data sample by index.</p> Source code in <code>src/QuantumGrav/dataset_ondisk.py</code> <pre><code>def get(self, idx: int) -&gt; Data:\n    \"\"\"Get a single data sample by index.\"\"\"\n    if self._num_samples is None:\n        raise ValueError(\"Dataset has not been processed yet.\")\n\n    # if idx &lt; 0 or idx &gt;= self._num_samples:\n    #     raise IndexError(\"Index out of bounds.\")\n    # Load the data from the processed files\n    datapoint = torch.load(\n        Path(self.processed_dir) / f\"data_{idx}.pt\", weights_only=False\n    )\n    if self.transform is not None:\n        datapoint = self.transform(datapoint)\n    return datapoint\n</code></pre>"},{"location":"api/#QuantumGrav.dataset_ondisk.QGDataset.len","title":"<code>len()</code>","text":"<p>Get the number of samples in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of samples in the dataset.</p> Source code in <code>src/QuantumGrav/dataset_ondisk.py</code> <pre><code>def len(self) -&gt; int:\n    \"\"\"Get the number of samples in the dataset.\n\n    Returns:\n        int: The number of samples in the dataset.\n    \"\"\"\n    return len(self.processed_file_names)\n</code></pre>"},{"location":"api/#QuantumGrav.dataset_ondisk.QGDataset.process","title":"<code>process()</code>","text":"<p>Process the dataset from the read rawdata into its final form.</p> Source code in <code>src/QuantumGrav/dataset_ondisk.py</code> <pre><code>def process(self) -&gt; None:\n    \"\"\"Process the dataset from the read rawdata into its final form.\"\"\"\n    # process data files\n    k = 0  # index to create the filenames for the processed data\n    for file in self.input:\n        N = self._get_num_samples_per_file(Path(file).resolve().absolute())\n\n        num_chunks = N // self.chunksize\n\n        raw_file = zarr.storage.LocalStore(\n            str(Path(file).resolve().absolute()), read_only=True\n        )\n\n        for i in range(0, num_chunks * self.chunksize, self.chunksize):\n            data = self.process_chunk(\n                raw_file,\n                i,\n                pre_transform=self.pre_transform,\n                pre_filter=self.pre_filter,\n            )\n\n            k = self.write_data(data, k)\n\n        # final chunk processing\n        data = self.process_chunk(\n            raw_file,\n            num_chunks * self.chunksize,\n            pre_transform=self.pre_transform,\n            pre_filter=self.pre_filter,\n        )\n\n        k = self.write_data(data, k)\n\n        raw_file.close()\n</code></pre>"},{"location":"api/#QuantumGrav.dataset_ondisk.QGDataset.write_data","title":"<code>write_data(data, idx)</code>","text":"<p>Write the processed data to disk using <code>torch.save</code>. This is a default implementation that can be overridden by subclasses, and is intended to be used in the data loading pipeline. Thus, is not intended to be called directly.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[Data]</code> <p>The list of Data objects to write to disk.</p> required <code>idx</code> <code>int</code> <p>The index to use for naming the files.</p> required Source code in <code>src/QuantumGrav/dataset_ondisk.py</code> <pre><code>def write_data(self, data: list[Data], idx: int) -&gt; int:\n    \"\"\"Write the processed data to disk using `torch.save`. This is a default implementation that can be overridden by subclasses, and is intended to be used in the data loading pipeline. Thus, is not intended to be called directly.\n\n    Args:\n        data (list[Data]): The list of Data objects to write to disk.\n        idx (int): The index to use for naming the files.\n    \"\"\"\n    if not Path(self.processed_dir).exists():\n        Path(self.processed_dir).mkdir(parents=True, exist_ok=True)\n\n    for d in data:\n        if d is not None:\n            file_path = Path(self.processed_dir) / f\"data_{idx}.pt\"\n            torch.save(d, file_path)\n            idx += 1\n    return idx\n</code></pre>"},{"location":"api/#julia-python-integration","title":"Julia-Python integration","text":"<p>This class provides a bridge to some user-supplied Julia code and converts its output into something Python can work with. </p>"},{"location":"api/#QuantumGrav.julia_worker.JuliaWorker","title":"<code>JuliaWorker</code>","text":"<p>This class runs a given Julia callable object from a given Julia code file. It additionally imports the QuantumGrav julia module and installs given dependencies if provided. After creation, the wrapped julia callable can be called via the call method of this calls. Warning: This class requires the juliacall package to be installed in the Python environment. Warning: This class is in early development and may change in the future, be slow, or otherwise not ready for high performance production use.</p> Source code in <code>src/QuantumGrav/julia_worker.py</code> <pre><code>class JuliaWorker:\n    \"\"\"This class runs a given Julia callable object from a given Julia code file. It additionally imports the QuantumGrav julia module and installs given dependencies if provided. After creation, the wrapped julia callable can be called via the __call__ method of this calls.\n    **Warning**: This class requires the juliacall package to be installed in the Python environment.\n    **Warning**: This class is in early development and may change in the future, be slow, or otherwise not ready for high performance production use.\n    \"\"\"\n\n    jl_constructor_name = None\n\n    def __init__(\n        self,\n        jl_kwargs: dict[str, Any] | None = None,\n        jl_code_path: str | Path | None = None,\n        jl_constructor_name: str | None = None,\n        jl_base_module_path: str | Path | None = None,\n        jl_dependencies: list[str] | None = None,\n    ):\n        \"\"\"Initializes the JuliaWorker with the given parameters.\n\n        Args:\n            jl_kwargs (dict[str, Any] | None, optional): Keyword arguments to pass to the Julia callable object constructor. Defaults to None.\n            jl_code_path (str | Path | None, optional): Path to the Julia code file in which the callable object is defined. Defaults to None.\n            jl_constructor_name (str | None, optional): Name of the Julia constructor function. Defaults to None.\n            jl_base_module_path (str | Path | None, optional): Path to the base Julia module 'QuantumGrav.jl'. If not given, tries to load it via a default `using QuantumGrav` import. Defaults to None.\n            jl_dependencies (list[str] | None, optional): List of Julia package dependencies. Defaults to None. Will be installed via `Pkg.add` if provided upon first call.\n\n        Raises:\n            ValueError: If the Julia function name is not provided.\n            ValueError: If the Julia code path is not provided.\n            FileNotFoundError: If the Julia code path does not exist.\n            NotImplementedError: If the base module path is not provided.\n            RuntimeError: If there is an error loading the base module.\n            RuntimeError: If there is an error loading Julia dependencies.\n            RuntimeError: If there is an error loading the Julia code.\n        \"\"\"\n\n        # we test for a bunch of needed args first\n        if jl_constructor_name is None:\n            raise ValueError(\"Julia function name must be provided.\")\n\n        if jl_code_path is None:\n            raise ValueError(\"Julia code path must be provided.\")\n\n        jl_code_path = Path(jl_code_path).resolve().absolute()\n        if not jl_code_path.exists():\n            raise FileNotFoundError(f\"Julia code path {jl_code_path} does not exist.\")\n\n        self.jl_constructor_name = jl_constructor_name\n        self.jl_module_name = \"QuantumGravPy2Jl\"  # the module name is hardcoded here\n\n        # try to initialize the new Julia module, then later do every julia call through this module\n        try:\n            self.jl_module = jcall.newmodule(self.jl_module_name)\n\n        except jcall.JuliaError as e:\n            raise RuntimeError(f\"Error creating new julia module: {e}\") from e\n        except Exception as e:\n            raise RuntimeError(\n                f\"Unexpected exception while creating Julia module {self.jl_module_name}: {e}\"\n            ) from e\n\n        # add base module for dependencies if exists\n        if jl_base_module_path is not None:\n            jl_base_module_path = Path(jl_base_module_path).resolve().absolute()\n            try:\n                self.jl_module.seval(\n                    f'using Pkg; Pkg.develop(path=\"{str(jl_base_module_path)}\")'\n                )  # only for now -&gt; get from package index later\n            except jcall.JuliaError as e:\n                raise RuntimeError(\n                    f\"Error loading base module {str(jl_base_module_path)}: {e}\"\n                ) from e\n            except Exception as e:\n                raise RuntimeError(\n                    f\"Unexpected exception while initializing julia base module: {e}\"\n                ) from e\n\n        try:\n            # add dependencies if provided\n            if jl_dependencies is not None:\n                for dep in jl_dependencies:\n                    self.jl_module.seval(f'using Pkg; Pkg.add(\"{dep}\")')\n        except jcall.JuliaError as e:\n            raise RuntimeError(f\"Error processing Julia dependencies: {e}\") from e\n        except Exception as e:\n            raise RuntimeError(\n                f\"Unexpected exception while processing Julia dependencies: {e}\"\n            ) from e\n\n        try:\n            # load the julia data generation julia code\n            self.jl_module.seval(f'push!(LOAD_PATH, \"{jl_code_path}\")')\n            self.jl_module.seval(\"using QuantumGrav\")\n            self.jl_module.seval(f'include(\"{jl_code_path}\")')\n            constructor_name = getattr(self.jl_module, jl_constructor_name)\n            self.jl_generator = constructor_name(jl_kwargs)\n        except jcall.JuliaError as e:\n            raise RuntimeError(\n                f\"Error evaluating Julia code to activate base module: {e}\"\n            ) from e\n        except Exception as e:\n            raise RuntimeError(\n                f\"Unexpected exception while loading Julia base module: {e}\"\n            ) from e\n\n    def __call__(self, *args, **kwargs) -&gt; Any:\n        \"\"\"Calls the wrapped Julia generator with the given arguments.\n\n        Raises:\n            RuntimeError: If the Julia module is not initialized.\n        Args:\n            *args: Positional arguments to pass to the Julia generator.\n            **kwargs: Keyword arguments to pass to the Julia generator.\n        Returns:\n            Any: The raw data generated by the Julia generator.\n        \"\"\"\n        if self.jl_module is None:\n            raise RuntimeError(\"Julia module is not initialized.\")\n        raw_data = self.jl_generator(*args, **kwargs)\n        return raw_data\n</code></pre>"},{"location":"api/#QuantumGrav.julia_worker.JuliaWorker.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Calls the wrapped Julia generator with the given arguments.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the Julia module is not initialized.</p> <p>Args:     args: Positional arguments to pass to the Julia generator.     *kwargs: Keyword arguments to pass to the Julia generator. Returns:     Any: The raw data generated by the Julia generator.</p> Source code in <code>src/QuantumGrav/julia_worker.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; Any:\n    \"\"\"Calls the wrapped Julia generator with the given arguments.\n\n    Raises:\n        RuntimeError: If the Julia module is not initialized.\n    Args:\n        *args: Positional arguments to pass to the Julia generator.\n        **kwargs: Keyword arguments to pass to the Julia generator.\n    Returns:\n        Any: The raw data generated by the Julia generator.\n    \"\"\"\n    if self.jl_module is None:\n        raise RuntimeError(\"Julia module is not initialized.\")\n    raw_data = self.jl_generator(*args, **kwargs)\n    return raw_data\n</code></pre>"},{"location":"api/#QuantumGrav.julia_worker.JuliaWorker.__init__","title":"<code>__init__(jl_kwargs=None, jl_code_path=None, jl_constructor_name=None, jl_base_module_path=None, jl_dependencies=None)</code>","text":"<p>Initializes the JuliaWorker with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>jl_kwargs</code> <code>dict[str, Any] | None</code> <p>Keyword arguments to pass to the Julia callable object constructor. Defaults to None.</p> <code>None</code> <code>jl_code_path</code> <code>str | Path | None</code> <p>Path to the Julia code file in which the callable object is defined. Defaults to None.</p> <code>None</code> <code>jl_constructor_name</code> <code>str | None</code> <p>Name of the Julia constructor function. Defaults to None.</p> <code>None</code> <code>jl_base_module_path</code> <code>str | Path | None</code> <p>Path to the base Julia module 'QuantumGrav.jl'. If not given, tries to load it via a default <code>using QuantumGrav</code> import. Defaults to None.</p> <code>None</code> <code>jl_dependencies</code> <code>list[str] | None</code> <p>List of Julia package dependencies. Defaults to None. Will be installed via <code>Pkg.add</code> if provided upon first call.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Julia function name is not provided.</p> <code>ValueError</code> <p>If the Julia code path is not provided.</p> <code>FileNotFoundError</code> <p>If the Julia code path does not exist.</p> <code>NotImplementedError</code> <p>If the base module path is not provided.</p> <code>RuntimeError</code> <p>If there is an error loading the base module.</p> <code>RuntimeError</code> <p>If there is an error loading Julia dependencies.</p> <code>RuntimeError</code> <p>If there is an error loading the Julia code.</p> Source code in <code>src/QuantumGrav/julia_worker.py</code> <pre><code>def __init__(\n    self,\n    jl_kwargs: dict[str, Any] | None = None,\n    jl_code_path: str | Path | None = None,\n    jl_constructor_name: str | None = None,\n    jl_base_module_path: str | Path | None = None,\n    jl_dependencies: list[str] | None = None,\n):\n    \"\"\"Initializes the JuliaWorker with the given parameters.\n\n    Args:\n        jl_kwargs (dict[str, Any] | None, optional): Keyword arguments to pass to the Julia callable object constructor. Defaults to None.\n        jl_code_path (str | Path | None, optional): Path to the Julia code file in which the callable object is defined. Defaults to None.\n        jl_constructor_name (str | None, optional): Name of the Julia constructor function. Defaults to None.\n        jl_base_module_path (str | Path | None, optional): Path to the base Julia module 'QuantumGrav.jl'. If not given, tries to load it via a default `using QuantumGrav` import. Defaults to None.\n        jl_dependencies (list[str] | None, optional): List of Julia package dependencies. Defaults to None. Will be installed via `Pkg.add` if provided upon first call.\n\n    Raises:\n        ValueError: If the Julia function name is not provided.\n        ValueError: If the Julia code path is not provided.\n        FileNotFoundError: If the Julia code path does not exist.\n        NotImplementedError: If the base module path is not provided.\n        RuntimeError: If there is an error loading the base module.\n        RuntimeError: If there is an error loading Julia dependencies.\n        RuntimeError: If there is an error loading the Julia code.\n    \"\"\"\n\n    # we test for a bunch of needed args first\n    if jl_constructor_name is None:\n        raise ValueError(\"Julia function name must be provided.\")\n\n    if jl_code_path is None:\n        raise ValueError(\"Julia code path must be provided.\")\n\n    jl_code_path = Path(jl_code_path).resolve().absolute()\n    if not jl_code_path.exists():\n        raise FileNotFoundError(f\"Julia code path {jl_code_path} does not exist.\")\n\n    self.jl_constructor_name = jl_constructor_name\n    self.jl_module_name = \"QuantumGravPy2Jl\"  # the module name is hardcoded here\n\n    # try to initialize the new Julia module, then later do every julia call through this module\n    try:\n        self.jl_module = jcall.newmodule(self.jl_module_name)\n\n    except jcall.JuliaError as e:\n        raise RuntimeError(f\"Error creating new julia module: {e}\") from e\n    except Exception as e:\n        raise RuntimeError(\n            f\"Unexpected exception while creating Julia module {self.jl_module_name}: {e}\"\n        ) from e\n\n    # add base module for dependencies if exists\n    if jl_base_module_path is not None:\n        jl_base_module_path = Path(jl_base_module_path).resolve().absolute()\n        try:\n            self.jl_module.seval(\n                f'using Pkg; Pkg.develop(path=\"{str(jl_base_module_path)}\")'\n            )  # only for now -&gt; get from package index later\n        except jcall.JuliaError as e:\n            raise RuntimeError(\n                f\"Error loading base module {str(jl_base_module_path)}: {e}\"\n            ) from e\n        except Exception as e:\n            raise RuntimeError(\n                f\"Unexpected exception while initializing julia base module: {e}\"\n            ) from e\n\n    try:\n        # add dependencies if provided\n        if jl_dependencies is not None:\n            for dep in jl_dependencies:\n                self.jl_module.seval(f'using Pkg; Pkg.add(\"{dep}\")')\n    except jcall.JuliaError as e:\n        raise RuntimeError(f\"Error processing Julia dependencies: {e}\") from e\n    except Exception as e:\n        raise RuntimeError(\n            f\"Unexpected exception while processing Julia dependencies: {e}\"\n        ) from e\n\n    try:\n        # load the julia data generation julia code\n        self.jl_module.seval(f'push!(LOAD_PATH, \"{jl_code_path}\")')\n        self.jl_module.seval(\"using QuantumGrav\")\n        self.jl_module.seval(f'include(\"{jl_code_path}\")')\n        constructor_name = getattr(self.jl_module, jl_constructor_name)\n        self.jl_generator = constructor_name(jl_kwargs)\n    except jcall.JuliaError as e:\n        raise RuntimeError(\n            f\"Error evaluating Julia code to activate base module: {e}\"\n        ) from e\n    except Exception as e:\n        raise RuntimeError(\n            f\"Unexpected exception while loading Julia base module: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/#model-training","title":"Model training","text":"<p>This consists of two classes, one which provides the basic training functionality - <code>Trainer</code>, and a class derived from this, <code>TrainerDDP</code>, which provides functionality for distributed data parallel training. </p>"},{"location":"api/#trainer","title":"Trainer","text":"<p>This class provides wrapper functions for setting up a model and for training and evaluating it. The basic concept is that everything is defined in a yaml file and handed to this class together with evaluator classes. After construction, the <code>train</code> and <code>test</code> functions will take care of the training and testing of the model. </p>"},{"location":"api/#QuantumGrav.train.Trainer","title":"<code>Trainer</code>","text":"<p>Trainer class for training and evaluating GNN models.</p> Source code in <code>src/QuantumGrav/train.py</code> <pre><code>class Trainer:\n    \"\"\"Trainer class for training and evaluating GNN models.\"\"\"\n\n    def __init__(\n        self,\n        config: dict[str, Any],\n        # training and evaluation functions\n        criterion: Callable[[Any, Data, Any], torch.Tensor],\n        apply_model: Callable | None = None,\n        # training evaluation and reporting\n        early_stopping: Callable[[Collection[Any] | torch.Tensor], bool] | None = None,\n        validator: DefaultValidator | None = None,\n        tester: DefaultTester | None = None,\n    ):\n        \"\"\"Initialize the trainer.\n\n        Args:\n            config (dict[str, Any]): The configuration dictionary.\n            criterion (Callable): The loss function to use.\n            apply_model (Callable | None, optional): A function to apply the model. Defaults to None.\n            early_stopping (Callable[[Collection[Any]], bool] | None, optional): A function for early stopping. Defaults to None.\n            validator (DefaultValidator | None, optional): A validator for model evaluation. Defaults to None.\n            tester (DefaultTester | None, optional): A tester for model evaluation. Defaults to None.\n\n        Raises:\n            ValueError: If the configuration is invalid.\n        \"\"\"\n        if (\n            all(x in config for x in [\"training\", \"model\", \"validation\", \"testing\"])\n            is False\n        ):\n            raise ValueError(\n                \"Configuration must contain 'training', 'model', 'validation' and 'testing' sections.\"\n            )\n\n        self.config = config\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(config.get(\"log_level\", logging.INFO))\n        self.logger.info(\"Initializing Trainer instance\")\n\n        # functions for executing training and evaluation\n        self.criterion = criterion\n        self.apply_model = apply_model\n        self.early_stopping = early_stopping\n        self.seed = config[\"training\"][\"seed\"]\n        self.device = torch.device(config[\"training\"][\"device\"])\n\n        torch.manual_seed(self.seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(self.seed)\n\n        # parameters for finding out which model is best\n        self.best_score = None\n        self.best_epoch = 0\n        self.epoch = 0\n\n        # date and time of run:\n        run_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.data_path = (\n            Path(self.config[\"training\"][\"path\"])\n            / f\"{config['model'].get('name', 'run')}_{run_date}\"\n        )\n\n        if not self.data_path.exists():\n            self.data_path.mkdir(parents=True)\n        self.logger.info(f\"Data path set to: {self.data_path}\")\n\n        self.checkpoint_path = self.data_path / \"model_checkpoints\"\n        self.checkpoint_at = config[\"training\"].get(\"checkpoint_at\", None)\n        self.latest_checkpoint = None\n        # training and evaluation functions\n        self.validator = validator\n        self.tester = tester\n        self.model = None\n        self.optimizer = None\n\n        with open(self.data_path / \"config.yaml\", \"w\") as f:\n            yaml.dump(self.config, f)\n\n        self.logger.info(\"Trainer initialized\")\n        self.logger.debug(f\"Configuration: {self.config}\")\n\n    def initialize_model(self) -&gt; Any:\n        \"\"\"Initialize the model for training.\n\n        Returns:\n            Any: The initialized model.\n        \"\"\"\n        if self.model is not None:\n            return self.model\n        # try:\n        model = gnn_model.GNNModel.from_config(self.config[\"model\"])\n        model = model.to(self.device)\n        self.model = model\n        self.logger.info(\"Model initialized to device: {}\".format(self.device))\n        return self.model\n\n    def initialize_optimizer(self) -&gt; torch.optim.Optimizer | None:\n        \"\"\"Initialize the optimizer for training.\n\n        Raises:\n            RuntimeError: If the model is not initialized.\n\n        Returns:\n            torch.optim.Optimizer: The initialized optimizer.\n        \"\"\"\n\n        if self.model is None:\n            raise RuntimeError(\n                \"Model must be initialized before initializing optimizer.\"\n            )\n\n        if self.optimizer is not None:\n            return self.optimizer\n\n        try:\n            lr = self.config[\"training\"].get(\"learning_rate\", 0.001)\n            weight_decay = self.config[\"training\"].get(\"weight_decay\", 0.0001)\n            optimizer = torch.optim.Adam(\n                self.model.parameters(),\n                lr=lr,\n                weight_decay=weight_decay,\n            )\n            self.optimizer = optimizer\n            self.logger.info(\n                f\"Optimizer initialized with learning rate: {lr} and weight decay: {weight_decay}\"\n            )\n        except Exception as e:\n            self.logger.error(f\"Error initializing optimizer: {e}\")\n        return self.optimizer\n\n    def prepare_dataloaders(\n        self,\n        dataset: Dataset | None = None,\n        split: list[float] = [0.8, 0.1, 0.1],\n        train_dataset: torch.utils.data.Subset | None = None,\n        val_dataset: torch.utils.data.Subset | None = None,\n        test_dataset: torch.utils.data.Subset | None = None,\n        training_sampler: torch.utils.data.Sampler | None = None,\n    ) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:\n        \"\"\"Prepare the data loaders for training, validation, and testing.\n\n        Args:\n            dataset (Dataset): The dataset to prepare.\n            split (list[float], optional): The split ratios for training, validation, and test sets. Defaults to [0.8, 0.1, 0.1].\n            training_sampler (torch.utils.data.Sampler, optional): The sampler for the training data loader. Defaults to None.\n\n        Returns:\n            Tuple[DataLoader, DataLoader, DataLoader]: The data loaders for training, validation, and testing.\n        \"\"\"\n\n        if (\n            dataset is not None\n            and train_dataset is None\n            and val_dataset is None\n            and test_dataset is None\n        ):\n            train_size = int(len(dataset) * split[0])\n            val_size = int(len(dataset) * split[1])\n            test_size = len(dataset) - train_size - val_size\n\n            if not np.isclose(\n                np.sum(split), 1.0, rtol=1e-05, atol=1e-08, equal_nan=False\n            ):\n                raise ValueError(\n                    f\"Split ratios must sum to 1.0. Provided split: {split}\"\n                )\n\n            self.train_dataset, self.val_dataset, self.test_dataset = (\n                torch.utils.data.random_split(\n                    dataset, [train_size, val_size, test_size]\n                )\n            )\n        else:\n            self.train_dataset, self.val_dataset, self.test_dataset = (\n                train_dataset,\n                val_dataset,\n                test_dataset,\n            )\n\n        train_loader = DataLoader(\n            self.train_dataset,  # type: ignore\n            batch_size=self.config[\"training\"][\"batch_size\"],\n            num_workers=self.config[\"training\"].get(\"num_workers\", 0),\n            pin_memory=self.config[\"training\"].get(\"pin_memory\", True),\n            drop_last=self.config[\"training\"].get(\"drop_last\", False),\n            prefetch_factor=self.config[\"training\"].get(\"prefetch_factor\", None),\n            shuffle=self.config[\"training\"].get(\"shuffle\", True),\n            sampler=training_sampler,\n        )\n\n        val_loader = DataLoader(\n            self.val_dataset,  # type: ignore\n            batch_size=self.config[\"validation\"][\"batch_size\"],\n            num_workers=self.config[\"validation\"].get(\"num_workers\", 0),\n            pin_memory=self.config[\"validation\"].get(\"pin_memory\", True),\n            drop_last=self.config[\"validation\"].get(\"drop_last\", False),\n            prefetch_factor=self.config[\"validation\"].get(\"prefetch_factor\", None),\n            shuffle=self.config[\"validation\"].get(\"shuffle\", True),\n        )\n\n        test_loader = DataLoader(\n            self.test_dataset,  # type: ignore\n            batch_size=self.config[\"testing\"][\"batch_size\"],\n            num_workers=self.config[\"testing\"].get(\"num_workers\", 0),\n            pin_memory=self.config[\"testing\"].get(\"pin_memory\", True),\n            drop_last=self.config[\"testing\"].get(\"drop_last\", False),\n            prefetch_factor=self.config[\"testing\"].get(\"prefetch_factor\", None),\n            shuffle=self.config[\"testing\"].get(\"shuffle\", True),\n        )\n\n        if dataset is not None:\n            self.logger.info(\n                f\"Data loaders prepared with splits: {split} and dataset sizes: {len(self.train_dataset)}, {len(self.val_dataset)}, {len(self.test_dataset)}\"\n            )\n        else:\n            self.logger.info(\n                f\"Data loaders prepared with dataset sizes: {len(self.train_dataset)}, {len(self.val_dataset)}, {len(self.test_dataset)}\"\n            )\n        return train_loader, val_loader, test_loader\n\n    # training helper functions\n    def _evaluate_batch(\n        self,\n        model: torch.nn.Module,\n        data: Data,\n    ) -&gt; torch.Tensor | Collection[torch.Tensor]:\n        \"\"\"Evaluate a single batch of data using the model.\n\n        Args:\n            model (torch.nn.Module): The model to evaluate.\n            data (Data): The input data for the model.\n\n        Returns:\n            torch.Tensor | Collection[torch.Tensor]: The output of the model.\n        \"\"\"\n        self.logger.debug(f\"  Evaluating batch on device: {self.device}\")\n        if self.apply_model:\n            outputs = self.apply_model(model, data)\n        else:\n            outputs = model(data.x, data.edge_index, data.batch)\n\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        return outputs\n\n    def _run_train_epoch(\n        self,\n        model: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        train_loader: DataLoader,\n    ) -&gt; torch.Tensor:\n        \"\"\"Run a single training epoch.\n\n        Args:\n            model (torch.nn.Module): The model to train.\n            optimizer (torch.optim.Optimizer): The optimizer for the model.\n            train_loader (DataLoader): The data loader for the training set.\n        Raises:\n            RuntimeError: If the model is not initialized.\n            RuntimeError: If the optimizer is not initialized.\n\n        Returns:\n            torch.Tensor: The training loss for each batch stored in a torch.Tensor\n        \"\"\"\n\n        if model is None:\n            raise RuntimeError(\"Model must be initialized before training.\")\n\n        if optimizer is None:\n            raise RuntimeError(\"Optimizer must be initialized before training.\")\n\n        losses = torch.zeros(len(train_loader), dtype=torch.float32, device=self.device)\n        self.logger.info(f\"  Starting training epoch {self.epoch}\")\n        # training run\n        for i, batch in enumerate(\n            tqdm.tqdm(train_loader, desc=f\"Training Epoch {self.epoch}\")\n        ):\n            self.logger.debug(f\"    Moving batch {i} to device: {self.device}\")\n            optimizer.zero_grad()\n\n            data = batch.to(self.device)\n            outputs = self._evaluate_batch(model, data)\n\n            self.logger.debug(\"    Computing loss\")\n            loss = self.criterion(outputs, data, self)\n\n            self.logger.debug(f\"    Backpropagating loss: {loss.item()}\")\n            loss.backward()\n\n            optimizer.step()\n\n            losses[i] = loss\n\n        return losses\n\n    def _check_model_status(self, eval_data: list[Any] | torch.Tensor) -&gt; bool:\n        \"\"\"Check the status of the model during training.\n\n        Args:\n            eval_data (list[Any]): The evaluation data from the training epoch.\n\n        Returns:\n            bool: Whether the training should stop early.\n        \"\"\"\n        if (\n            self.checkpoint_at is not None\n            and self.epoch % self.checkpoint_at == 0\n            and self.epoch &gt; 0\n        ):\n            self.save_checkpoint()\n\n        if self.early_stopping is not None:\n            if self.early_stopping(eval_data):\n                self.logger.debug(f\"Early stopping at epoch {self.epoch}.\")\n                self.save_checkpoint(name_addition=f\"_{self.epoch}_early_stopping\")\n                return True\n\n            if self.early_stopping.found_better_model:\n                self.logger.debug(f\"Found better model at epoch {self.epoch}.\")\n                self.save_checkpoint(name_addition=f\"_{self.epoch}_current_best\")\n                # not returning true because this is not the end of training\n\n        return False\n\n    def run_training(\n        self,\n        train_loader: DataLoader,\n        val_loader: DataLoader,\n        trial: optuna.trial.Trial | None = None,\n    ) -&gt; Tuple[torch.Tensor | Collection[Any], torch.Tensor | Collection[Any]]:\n        \"\"\"Run the training process.\n\n        Args:\n            train_loader (DataLoader): The data loader for the training set.\n            val_loader (DataLoader): The data loader for the validation set.\n            trial (optuna.trial.Trial | None, optional): An Optuna trial\n                for hyperparameter tuning. Defaults to None.\n\n        Returns:\n            Tuple[torch.Tensor | Collection[Any], torch.Tensor | Collection[Any]]: The training and validation results.\n        \"\"\"\n        self.logger.info(\"Starting training process.\")\n        # training loop\n        num_epochs = self.config[\"training\"][\"num_epochs\"]\n\n        self.model = self.initialize_model()\n\n        optimizer = self.initialize_optimizer()\n\n        if optimizer is None:\n            raise AttributeError(\n                \"Error, optimizer must be successfully initialized before running training\"\n            )\n\n        total_training_data = torch.zeros(num_epochs, 2, dtype=torch.float32)\n\n        for epoch in range(0, num_epochs):\n            self.logger.info(f\"  Current epoch: {self.epoch}/{num_epochs}\")\n            self.model.train()\n\n            epoch_data = self._run_train_epoch(self.model, optimizer, train_loader)\n\n            # collect mean and std for each epoch\n            total_training_data[epoch, :] = torch.Tensor(\n                [epoch_data.mean(dim=0).item(), epoch_data.std(dim=0).item()]\n            )\n\n            self.logger.info(\n                f\"  Completed epoch {epoch}. training loss: {total_training_data[epoch, 0]:.8f} +/- {total_training_data[epoch, 1]:.8f}.\"\n            )\n\n            # evaluation run on validation set\n            if self.validator is not None:\n                validation_result = self.validator.validate(self.model, val_loader)\n                self.validator.report(validation_result)\n\n                # integrate Optuna here for hyperparameter tuning\n                if trial is not None:\n                    avg_sigma_loss = self.validator.data[self.epoch]\n                    avg_loss = avg_sigma_loss[0]\n                    trial.report(avg_loss, self.epoch)\n\n                    # Handle pruning based on the intermediate value.\n                    if trial.should_prune():\n                        raise optuna.exceptions.TrialPruned()\n\n            should_stop = self._check_model_status(\n                self.validator.data if self.validator else total_training_data,\n            )\n            if should_stop:\n                self.logger.info(\"Stopping training early.\")\n                break\n            self.epoch += 1\n\n        self.logger.info(\"Training process completed.\")\n        self.logger.info(\"Saving model\")\n\n        outpath = self.data_path / f\"final_model_epoch={self.epoch}.pt\"\n        self.model.save(outpath)\n\n        return total_training_data, self.validator.data if self.validator else []\n\n    def run_test(\n        self, test_loader: DataLoader, model_name_addition: str = \"current_best.pt\"\n    ) -&gt; Collection[Any]:\n        \"\"\"Run testing phase.\n\n        Args:\n            test_loader (DataLoader): The data loader for the test set.\n            model_name_addition (str): An optional string to append to the checkpoint filename.\n        Raises:\n            RuntimeError: If the model is not initialized.\n            RuntimeError: If the test data is not available.\n\n        Returns:\n            Collection[Any]: A collection of test results that can be scalars, tensors, lists, dictionaries or any other data type that the tester might return.\n        \"\"\"\n        self.logger.info(\"Starting testing process.\")\n        # get the best model again\n\n        saved_models = [\n            f\n            for f in Path(self.checkpoint_path).iterdir()\n            if f.is_file() and model_name_addition in str(f)\n        ]\n\n        if len(saved_models) == 0:\n            raise RuntimeError(\n                f\"No model with the name addition '{model_name_addition}' found, did training work?\"\n            )\n\n        # get the latest of the best models\n        best_of_the_best = (\n            max(saved_models, key=lambda f: f.stat().st_mtime) if saved_models else None\n        )\n\n        self.logger.info(f\"loading best model found: {str(best_of_the_best)}\")\n        self.model = gnn_model.GNNModel.load(best_of_the_best, device=self.device)\n\n        self.model.eval()\n        if self.tester is None:\n            raise RuntimeError(\"Tester must be initialized before testing.\")\n        test_result = self.tester.test(self.model, test_loader)\n        self.tester.report(test_result)\n        self.logger.info(\"Testing process completed.\")\n        self.save_checkpoint(name_addition=\"best_model_found\")\n        return self.tester.data\n\n    def save_checkpoint(self, name_addition: str = \"\"):\n        \"\"\"Save model checkpoint.\n\n        Raises:\n            ValueError: If the model is not initialized.\n            ValueError: If the model configuration does not contain 'name'.\n            ValueError: If the training configuration does not contain 'checkpoint_path'.\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model must be initialized before saving checkpoint.\")\n\n        self.logger.info(\n            f\"Saving checkpoint for model {self.config['model'].get('name', ' model')} at epoch {self.epoch} to {self.checkpoint_path}\"\n        )\n        outpath = (\n            self.checkpoint_path\n            / f\"{self.config['model'].get('name', 'model')}_{name_addition}.pt\"\n        )\n\n        if outpath.exists() is False:\n            outpath.parent.mkdir(parents=True, exist_ok=True)\n            self.logger.debug(f\"Created directory {outpath.parent} for checkpoint.\")\n\n        self.latest_checkpoint = outpath\n        self.model.save(outpath)\n\n    def load_checkpoint(self, name_addition: str = \"\") -&gt; None:\n        \"\"\"Load model checkpoint to the device given\n\n        Args:\n            name_addition (str): An optional string to append to the checkpoint filename.\n\n        Raises:\n            RuntimeError: If the model is not initialized.\n        \"\"\"\n\n        if self.model is None:\n            raise RuntimeError(\"Model must be initialized before loading checkpoint.\")\n\n        if Path(self.checkpoint_path).exists() is False:\n            raise RuntimeError(\"Checkpoint path does not exist.\")\n\n        self.logger.info(\n            \"available checkpoints: %s\", list(Path(self.checkpoint_path).iterdir())\n        )\n\n        loadpath = (\n            Path(self.checkpoint_path)\n            / f\"{self.config['model'].get('name', 'model')}_{name_addition}.pt\"\n        )\n\n        if not loadpath.exists():\n            raise FileNotFoundError(f\"Checkpoint file {loadpath} does not exist.\")\n\n        self.model = gnn_model.GNNModel.load(loadpath)\n</code></pre>"},{"location":"api/#QuantumGrav.train.Trainer.__init__","title":"<code>__init__(config, criterion, apply_model=None, early_stopping=None, validator=None, tester=None)</code>","text":"<p>Initialize the trainer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>The configuration dictionary.</p> required <code>criterion</code> <code>Callable</code> <p>The loss function to use.</p> required <code>apply_model</code> <code>Callable | None</code> <p>A function to apply the model. Defaults to None.</p> <code>None</code> <code>early_stopping</code> <code>Callable[[Collection[Any]], bool] | None</code> <p>A function for early stopping. Defaults to None.</p> <code>None</code> <code>validator</code> <code>DefaultValidator | None</code> <p>A validator for model evaluation. Defaults to None.</p> <code>None</code> <code>tester</code> <code>DefaultTester | None</code> <p>A tester for model evaluation. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the configuration is invalid.</p> Source code in <code>src/QuantumGrav/train.py</code> <pre><code>def __init__(\n    self,\n    config: dict[str, Any],\n    # training and evaluation functions\n    criterion: Callable[[Any, Data, Any], torch.Tensor],\n    apply_model: Callable | None = None,\n    # training evaluation and reporting\n    early_stopping: Callable[[Collection[Any] | torch.Tensor], bool] | None = None,\n    validator: DefaultValidator | None = None,\n    tester: DefaultTester | None = None,\n):\n    \"\"\"Initialize the trainer.\n\n    Args:\n        config (dict[str, Any]): The configuration dictionary.\n        criterion (Callable): The loss function to use.\n        apply_model (Callable | None, optional): A function to apply the model. Defaults to None.\n        early_stopping (Callable[[Collection[Any]], bool] | None, optional): A function for early stopping. Defaults to None.\n        validator (DefaultValidator | None, optional): A validator for model evaluation. Defaults to None.\n        tester (DefaultTester | None, optional): A tester for model evaluation. Defaults to None.\n\n    Raises:\n        ValueError: If the configuration is invalid.\n    \"\"\"\n    if (\n        all(x in config for x in [\"training\", \"model\", \"validation\", \"testing\"])\n        is False\n    ):\n        raise ValueError(\n            \"Configuration must contain 'training', 'model', 'validation' and 'testing' sections.\"\n        )\n\n    self.config = config\n    self.logger = logging.getLogger(__name__)\n    self.logger.setLevel(config.get(\"log_level\", logging.INFO))\n    self.logger.info(\"Initializing Trainer instance\")\n\n    # functions for executing training and evaluation\n    self.criterion = criterion\n    self.apply_model = apply_model\n    self.early_stopping = early_stopping\n    self.seed = config[\"training\"][\"seed\"]\n    self.device = torch.device(config[\"training\"][\"device\"])\n\n    torch.manual_seed(self.seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(self.seed)\n\n    # parameters for finding out which model is best\n    self.best_score = None\n    self.best_epoch = 0\n    self.epoch = 0\n\n    # date and time of run:\n    run_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    self.data_path = (\n        Path(self.config[\"training\"][\"path\"])\n        / f\"{config['model'].get('name', 'run')}_{run_date}\"\n    )\n\n    if not self.data_path.exists():\n        self.data_path.mkdir(parents=True)\n    self.logger.info(f\"Data path set to: {self.data_path}\")\n\n    self.checkpoint_path = self.data_path / \"model_checkpoints\"\n    self.checkpoint_at = config[\"training\"].get(\"checkpoint_at\", None)\n    self.latest_checkpoint = None\n    # training and evaluation functions\n    self.validator = validator\n    self.tester = tester\n    self.model = None\n    self.optimizer = None\n\n    with open(self.data_path / \"config.yaml\", \"w\") as f:\n        yaml.dump(self.config, f)\n\n    self.logger.info(\"Trainer initialized\")\n    self.logger.debug(f\"Configuration: {self.config}\")\n</code></pre>"},{"location":"api/#QuantumGrav.train.Trainer.initialize_model","title":"<code>initialize_model()</code>","text":"<p>Initialize the model for training.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The initialized model.</p> Source code in <code>src/QuantumGrav/train.py</code> <pre><code>def initialize_model(self) -&gt; Any:\n    \"\"\"Initialize the model for training.\n\n    Returns:\n        Any: The initialized model.\n    \"\"\"\n    if self.model is not None:\n        return self.model\n    # try:\n    model = gnn_model.GNNModel.from_config(self.config[\"model\"])\n    model = model.to(self.device)\n    self.model = model\n    self.logger.info(\"Model initialized to device: {}\".format(self.device))\n    return self.model\n</code></pre>"},{"location":"api/#QuantumGrav.train.Trainer.initialize_optimizer","title":"<code>initialize_optimizer()</code>","text":"<p>Initialize the optimizer for training.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the model is not initialized.</p> <p>Returns:</p> Type Description <code>Optimizer | None</code> <p>torch.optim.Optimizer: The initialized optimizer.</p> Source code in <code>src/QuantumGrav/train.py</code> <pre><code>def initialize_optimizer(self) -&gt; torch.optim.Optimizer | None:\n    \"\"\"Initialize the optimizer for training.\n\n    Raises:\n        RuntimeError: If the model is not initialized.\n\n    Returns:\n        torch.optim.Optimizer: The initialized optimizer.\n    \"\"\"\n\n    if self.model is None:\n        raise RuntimeError(\n            \"Model must be initialized before initializing optimizer.\"\n        )\n\n    if self.optimizer is not None:\n        return self.optimizer\n\n    try:\n        lr = self.config[\"training\"].get(\"learning_rate\", 0.001)\n        weight_decay = self.config[\"training\"].get(\"weight_decay\", 0.0001)\n        optimizer = torch.optim.Adam(\n            self.model.parameters(),\n            lr=lr,\n            weight_decay=weight_decay,\n        )\n        self.optimizer = optimizer\n        self.logger.info(\n            f\"Optimizer initialized with learning rate: {lr} and weight decay: {weight_decay}\"\n        )\n    except Exception as e:\n        self.logger.error(f\"Error initializing optimizer: {e}\")\n    return self.optimizer\n</code></pre>"},{"location":"api/#QuantumGrav.train.Trainer.load_checkpoint","title":"<code>load_checkpoint(name_addition='')</code>","text":"<p>Load model checkpoint to the device given</p> <p>Parameters:</p> Name Type Description Default <code>name_addition</code> <code>str</code> <p>An optional string to append to the checkpoint filename.</p> <code>''</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the model is not initialized.</p> Source code in <code>src/QuantumGrav/train.py</code> <pre><code>def load_checkpoint(self, name_addition: str = \"\") -&gt; None:\n    \"\"\"Load model checkpoint to the device given\n\n    Args:\n        name_addition (str): An optional string to append to the checkpoint filename.\n\n    Raises:\n        RuntimeError: If the model is not initialized.\n    \"\"\"\n\n    if self.model is None:\n        raise RuntimeError(\"Model must be initialized before loading checkpoint.\")\n\n    if Path(self.checkpoint_path).exists() is False:\n        raise RuntimeError(\"Checkpoint path does not exist.\")\n\n    self.logger.info(\n        \"available checkpoints: %s\", list(Path(self.checkpoint_path).iterdir())\n    )\n\n    loadpath = (\n        Path(self.checkpoint_path)\n        / f\"{self.config['model'].get('name', 'model')}_{name_addition}.pt\"\n    )\n\n    if not loadpath.exists():\n        raise FileNotFoundError(f\"Checkpoint file {loadpath} does not exist.\")\n\n    self.model = gnn_model.GNNModel.load(loadpath)\n</code></pre>"},{"location":"api/#QuantumGrav.train.Trainer.prepare_dataloaders","title":"<code>prepare_dataloaders(dataset=None, split=[0.8, 0.1, 0.1], train_dataset=None, val_dataset=None, test_dataset=None, training_sampler=None)</code>","text":"<p>Prepare the data loaders for training, validation, and testing.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to prepare.</p> <code>None</code> <code>split</code> <code>list[float]</code> <p>The split ratios for training, validation, and test sets. Defaults to [0.8, 0.1, 0.1].</p> <code>[0.8, 0.1, 0.1]</code> <code>training_sampler</code> <code>Sampler</code> <p>The sampler for the training data loader. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[DataLoader, DataLoader, DataLoader]</code> <p>Tuple[DataLoader, DataLoader, DataLoader]: The data loaders for training, validation, and testing.</p> Source code in <code>src/QuantumGrav/train.py</code> <pre><code>def prepare_dataloaders(\n    self,\n    dataset: Dataset | None = None,\n    split: list[float] = [0.8, 0.1, 0.1],\n    train_dataset: torch.utils.data.Subset | None = None,\n    val_dataset: torch.utils.data.Subset | None = None,\n    test_dataset: torch.utils.data.Subset | None = None,\n    training_sampler: torch.utils.data.Sampler | None = None,\n) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"Prepare the data loaders for training, validation, and testing.\n\n    Args:\n        dataset (Dataset): The dataset to prepare.\n        split (list[float], optional): The split ratios for training, validation, and test sets. Defaults to [0.8, 0.1, 0.1].\n        training_sampler (torch.utils.data.Sampler, optional): The sampler for the training data loader. Defaults to None.\n\n    Returns:\n        Tuple[DataLoader, DataLoader, DataLoader]: The data loaders for training, validation, and testing.\n    \"\"\"\n\n    if (\n        dataset is not None\n        and train_dataset is None\n        and val_dataset is None\n        and test_dataset is None\n    ):\n        train_size = int(len(dataset) * split[0])\n        val_size = int(len(dataset) * split[1])\n        test_size = len(dataset) - train_size - val_size\n\n        if not np.isclose(\n            np.sum(split), 1.0, rtol=1e-05, atol=1e-08, equal_nan=False\n        ):\n            raise ValueError(\n                f\"Split ratios must sum to 1.0. Provided split: {split}\"\n            )\n\n        self.train_dataset, self.val_dataset, self.test_dataset = (\n            torch.utils.data.random_split(\n                dataset, [train_size, val_size, test_size]\n            )\n        )\n    else:\n        self.train_dataset, self.val_dataset, self.test_dataset = (\n            train_dataset,\n            val_dataset,\n            test_dataset,\n        )\n\n    train_loader = DataLoader(\n        self.train_dataset,  # type: ignore\n        batch_size=self.config[\"training\"][\"batch_size\"],\n        num_workers=self.config[\"training\"].get(\"num_workers\", 0),\n        pin_memory=self.config[\"training\"].get(\"pin_memory\", True),\n        drop_last=self.config[\"training\"].get(\"drop_last\", False),\n        prefetch_factor=self.config[\"training\"].get(\"prefetch_factor\", None),\n        shuffle=self.config[\"training\"].get(\"shuffle\", True),\n        sampler=training_sampler,\n    )\n\n    val_loader = DataLoader(\n        self.val_dataset,  # type: ignore\n        batch_size=self.config[\"validation\"][\"batch_size\"],\n        num_workers=self.config[\"validation\"].get(\"num_workers\", 0),\n        pin_memory=self.config[\"validation\"].get(\"pin_memory\", True),\n        drop_last=self.config[\"validation\"].get(\"drop_last\", False),\n        prefetch_factor=self.config[\"validation\"].get(\"prefetch_factor\", None),\n        shuffle=self.config[\"validation\"].get(\"shuffle\", True),\n    )\n\n    test_loader = DataLoader(\n        self.test_dataset,  # type: ignore\n        batch_size=self.config[\"testing\"][\"batch_size\"],\n        num_workers=self.config[\"testing\"].get(\"num_workers\", 0),\n        pin_memory=self.config[\"testing\"].get(\"pin_memory\", True),\n        drop_last=self.config[\"testing\"].get(\"drop_last\", False),\n        prefetch_factor=self.config[\"testing\"].get(\"prefetch_factor\", None),\n        shuffle=self.config[\"testing\"].get(\"shuffle\", True),\n    )\n\n    if dataset is not None:\n        self.logger.info(\n            f\"Data loaders prepared with splits: {split} and dataset sizes: {len(self.train_dataset)}, {len(self.val_dataset)}, {len(self.test_dataset)}\"\n        )\n    else:\n        self.logger.info(\n            f\"Data loaders prepared with dataset sizes: {len(self.train_dataset)}, {len(self.val_dataset)}, {len(self.test_dataset)}\"\n        )\n    return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"api/#QuantumGrav.train.Trainer.run_test","title":"<code>run_test(test_loader, model_name_addition='current_best.pt')</code>","text":"<p>Run testing phase.</p> <p>Parameters:</p> Name Type Description Default <code>test_loader</code> <code>DataLoader</code> <p>The data loader for the test set.</p> required <code>model_name_addition</code> <code>str</code> <p>An optional string to append to the checkpoint filename.</p> <code>'current_best.pt'</code> <p>Raises:     RuntimeError: If the model is not initialized.     RuntimeError: If the test data is not available.</p> <p>Returns:</p> Type Description <code>Collection[Any]</code> <p>Collection[Any]: A collection of test results that can be scalars, tensors, lists, dictionaries or any other data type that the tester might return.</p> Source code in <code>src/QuantumGrav/train.py</code> <pre><code>def run_test(\n    self, test_loader: DataLoader, model_name_addition: str = \"current_best.pt\"\n) -&gt; Collection[Any]:\n    \"\"\"Run testing phase.\n\n    Args:\n        test_loader (DataLoader): The data loader for the test set.\n        model_name_addition (str): An optional string to append to the checkpoint filename.\n    Raises:\n        RuntimeError: If the model is not initialized.\n        RuntimeError: If the test data is not available.\n\n    Returns:\n        Collection[Any]: A collection of test results that can be scalars, tensors, lists, dictionaries or any other data type that the tester might return.\n    \"\"\"\n    self.logger.info(\"Starting testing process.\")\n    # get the best model again\n\n    saved_models = [\n        f\n        for f in Path(self.checkpoint_path).iterdir()\n        if f.is_file() and model_name_addition in str(f)\n    ]\n\n    if len(saved_models) == 0:\n        raise RuntimeError(\n            f\"No model with the name addition '{model_name_addition}' found, did training work?\"\n        )\n\n    # get the latest of the best models\n    best_of_the_best = (\n        max(saved_models, key=lambda f: f.stat().st_mtime) if saved_models else None\n    )\n\n    self.logger.info(f\"loading best model found: {str(best_of_the_best)}\")\n    self.model = gnn_model.GNNModel.load(best_of_the_best, device=self.device)\n\n    self.model.eval()\n    if self.tester is None:\n        raise RuntimeError(\"Tester must be initialized before testing.\")\n    test_result = self.tester.test(self.model, test_loader)\n    self.tester.report(test_result)\n    self.logger.info(\"Testing process completed.\")\n    self.save_checkpoint(name_addition=\"best_model_found\")\n    return self.tester.data\n</code></pre>"},{"location":"api/#QuantumGrav.train.Trainer.run_training","title":"<code>run_training(train_loader, val_loader, trial=None)</code>","text":"<p>Run the training process.</p> <p>Parameters:</p> Name Type Description Default <code>train_loader</code> <code>DataLoader</code> <p>The data loader for the training set.</p> required <code>val_loader</code> <code>DataLoader</code> <p>The data loader for the validation set.</p> required <code>trial</code> <code>Trial | None</code> <p>An Optuna trial for hyperparameter tuning. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Tensor | Collection[Any], Tensor | Collection[Any]]</code> <p>Tuple[torch.Tensor | Collection[Any], torch.Tensor | Collection[Any]]: The training and validation results.</p> Source code in <code>src/QuantumGrav/train.py</code> <pre><code>def run_training(\n    self,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    trial: optuna.trial.Trial | None = None,\n) -&gt; Tuple[torch.Tensor | Collection[Any], torch.Tensor | Collection[Any]]:\n    \"\"\"Run the training process.\n\n    Args:\n        train_loader (DataLoader): The data loader for the training set.\n        val_loader (DataLoader): The data loader for the validation set.\n        trial (optuna.trial.Trial | None, optional): An Optuna trial\n            for hyperparameter tuning. Defaults to None.\n\n    Returns:\n        Tuple[torch.Tensor | Collection[Any], torch.Tensor | Collection[Any]]: The training and validation results.\n    \"\"\"\n    self.logger.info(\"Starting training process.\")\n    # training loop\n    num_epochs = self.config[\"training\"][\"num_epochs\"]\n\n    self.model = self.initialize_model()\n\n    optimizer = self.initialize_optimizer()\n\n    if optimizer is None:\n        raise AttributeError(\n            \"Error, optimizer must be successfully initialized before running training\"\n        )\n\n    total_training_data = torch.zeros(num_epochs, 2, dtype=torch.float32)\n\n    for epoch in range(0, num_epochs):\n        self.logger.info(f\"  Current epoch: {self.epoch}/{num_epochs}\")\n        self.model.train()\n\n        epoch_data = self._run_train_epoch(self.model, optimizer, train_loader)\n\n        # collect mean and std for each epoch\n        total_training_data[epoch, :] = torch.Tensor(\n            [epoch_data.mean(dim=0).item(), epoch_data.std(dim=0).item()]\n        )\n\n        self.logger.info(\n            f\"  Completed epoch {epoch}. training loss: {total_training_data[epoch, 0]:.8f} +/- {total_training_data[epoch, 1]:.8f}.\"\n        )\n\n        # evaluation run on validation set\n        if self.validator is not None:\n            validation_result = self.validator.validate(self.model, val_loader)\n            self.validator.report(validation_result)\n\n            # integrate Optuna here for hyperparameter tuning\n            if trial is not None:\n                avg_sigma_loss = self.validator.data[self.epoch]\n                avg_loss = avg_sigma_loss[0]\n                trial.report(avg_loss, self.epoch)\n\n                # Handle pruning based on the intermediate value.\n                if trial.should_prune():\n                    raise optuna.exceptions.TrialPruned()\n\n        should_stop = self._check_model_status(\n            self.validator.data if self.validator else total_training_data,\n        )\n        if should_stop:\n            self.logger.info(\"Stopping training early.\")\n            break\n        self.epoch += 1\n\n    self.logger.info(\"Training process completed.\")\n    self.logger.info(\"Saving model\")\n\n    outpath = self.data_path / f\"final_model_epoch={self.epoch}.pt\"\n    self.model.save(outpath)\n\n    return total_training_data, self.validator.data if self.validator else []\n</code></pre>"},{"location":"api/#QuantumGrav.train.Trainer.save_checkpoint","title":"<code>save_checkpoint(name_addition='')</code>","text":"<p>Save model checkpoint.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not initialized.</p> <code>ValueError</code> <p>If the model configuration does not contain 'name'.</p> <code>ValueError</code> <p>If the training configuration does not contain 'checkpoint_path'.</p> Source code in <code>src/QuantumGrav/train.py</code> <pre><code>def save_checkpoint(self, name_addition: str = \"\"):\n    \"\"\"Save model checkpoint.\n\n    Raises:\n        ValueError: If the model is not initialized.\n        ValueError: If the model configuration does not contain 'name'.\n        ValueError: If the training configuration does not contain 'checkpoint_path'.\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model must be initialized before saving checkpoint.\")\n\n    self.logger.info(\n        f\"Saving checkpoint for model {self.config['model'].get('name', ' model')} at epoch {self.epoch} to {self.checkpoint_path}\"\n    )\n    outpath = (\n        self.checkpoint_path\n        / f\"{self.config['model'].get('name', 'model')}_{name_addition}.pt\"\n    )\n\n    if outpath.exists() is False:\n        outpath.parent.mkdir(parents=True, exist_ok=True)\n        self.logger.debug(f\"Created directory {outpath.parent} for checkpoint.\")\n\n    self.latest_checkpoint = outpath\n    self.model.save(outpath)\n</code></pre>"},{"location":"api/#distributed-data-parallel-trainer-class","title":"Distributed data parallel Trainer class","text":"<p>This is based on this part of the pytorch documentation and is untested at the time of writing. </p>"},{"location":"api/#QuantumGrav.train_ddp.TrainerDDP","title":"<code>TrainerDDP</code>","text":"<p>               Bases: <code>Trainer</code></p> Source code in <code>src/QuantumGrav/train_ddp.py</code> <pre><code>class TrainerDDP(train.Trainer):\n    def __init__(\n        self,\n        rank: int,\n        config: dict[str, Any],\n        # training and evaluation functions\n        criterion: Callable,\n        apply_model: Callable | None = None,\n        # training evaluation and reporting\n        early_stopping: Callable[[Collection[Any] | torch.Tensor], bool] | None = None,\n        validator: DefaultValidator | None = None,\n        tester: DefaultTester | None = None,\n    ):\n        \"\"\"Initialize the distributed data parallel (DDP) trainer.\n\n        Args:\n            rank (int): The rank of the current process.\n            config (dict[str, Any]): The configuration dictionary.\n            criterion (Callable): The loss function.\n            apply_model (Callable | None, optional): The function to apply the model. Defaults to None.\n            early_stopping (Callable[[list[dict[str, Any]]], bool] | None, optional): The early stopping function. Defaults to None.\n            validator (DefaultValidator | None, optional): The validator for model evaluation. Defaults to None.\n            tester (DefaultTester | None, optional): The tester for model testing. Defaults to None.\n\n        Raises:\n            ValueError: If the configuration is invalid.\n        \"\"\"\n        if \"parallel\" not in config:\n            raise ValueError(\"Configuration must contain 'parallel' section for DDP.\")\n\n        super().__init__(\n            config,\n            criterion,\n            apply_model,\n            early_stopping,\n            validator,\n            tester,\n        )\n        # initialize the systems differently on each process/rank\n        torch.manual_seed(self.seed + rank)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(self.seed + rank)\n\n        if torch.cuda.is_available() and config[\"training\"][\"device\"] != \"cpu\":\n            torch.cuda.set_device(rank)\n            self.device = torch.device(f\"cuda:{rank}\")\n        else:\n            self.device = torch.device(\"cpu\")\n\n        self.rank = rank\n        self.world_size = config[\"parallel\"][\"world_size\"]\n        self.logger.info(\"Initialized DDP trainer\")\n\n    def initialize_model(self) -&gt; DDP:\n        \"\"\"Initialize the model for training.\n\n        Returns:\n            DDP: The initialized model.\n        \"\"\"\n        model = gnn_model.GNNModel.from_config(self.config[\"model\"])\n\n        if self.device.type == \"cpu\" or (\n            isinstance(self.device, torch.device) and self.device.type == \"cpu\"\n        ):\n            d_id = None\n            o_id = None\n        else:\n            d_id = [\n                self.device,\n            ]\n            o_id = self.config[\"parallel\"].get(\"output_device\", None)\n        model = DDP(\n            model,\n            device_ids=d_id,\n            output_device=o_id,\n            find_unused_parameters=self.config[\"parallel\"].get(\n                \"find_unused_parameters\", False\n            ),\n        )\n        self.model = model.to(self.device, non_blocking=True)\n        self.logger.info(f\"Model initialized on device: {self.device}\")\n        return self.model\n\n    def prepare_dataloaders(\n        self, dataset: Dataset, split: list[float] = [0.8, 0.1, 0.1]\n    ) -&gt; Tuple[\n        DataLoader,\n        DataLoader,\n        DataLoader,\n    ]:\n        \"\"\"Prepare the data loaders for training, validation, and testing.\n\n        Args:\n            dataset (Dataset): The dataset to split.\n            split (list[float], optional): The proportions for train/val/test split. Defaults to [0.8, 0.1, 0.1].\n\n        Returns:\n            Tuple[ DataLoader, DataLoader, DataLoader, ]: The data loaders for training, validation, and testing.\n        \"\"\"\n        train_size = int(len(dataset) * split[0])\n        val_size = int(len(dataset) * split[1])\n        test_size = len(dataset) - train_size - val_size\n        self.logger.info(\n            f\"Preparing data loaders with split: {split}, train size: {train_size}, val size: {val_size}, test size: {test_size}\"\n        )\n        if (\n            np.isclose(np.sum(split), 1.0, rtol=1e-05, atol=1e-08, equal_nan=False)\n            is False\n        ):\n            raise ValueError(\n                \"Split ratios must sum to 1.0. Provided split: {}\".format(split)\n            )\n\n        self.train_dataset, self.val_dataset, self.test_dataset = (\n            torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n        )\n\n        # samplers are needed to distribute the data across processes in such a way that each process gets a unique subset of the data\n        self.train_sampler = torch.utils.data.DistributedSampler(\n            self.train_dataset,\n            num_replicas=self.world_size,\n            rank=self.rank,\n            shuffle=True,\n        )\n\n        self.val_sampler = torch.utils.data.DistributedSampler(\n            self.val_dataset,\n            num_replicas=self.world_size,\n            rank=self.rank,\n            shuffle=False,\n        )\n\n        self.test_sampler = torch.utils.data.DistributedSampler(\n            self.test_dataset,\n            num_replicas=self.world_size,\n            rank=self.rank,\n            shuffle=False,\n        )\n\n        # make the data loaders\n        train_loader = DataLoader(\n            self.train_dataset,\n            batch_size=self.config[\"training\"][\"batch_size\"],\n            sampler=self.train_sampler,\n            num_workers=self.config[\"training\"].get(\"num_workers\", 0),\n            pin_memory=self.config[\"training\"].get(\"pin_memory\", True),\n            drop_last=self.config[\"training\"].get(\"drop_last\", False),\n            prefetch_factor=self.config[\"training\"].get(\"prefetch_factor\", None),\n        )\n\n        val_loader = DataLoader(\n            self.val_dataset,\n            sampler=self.val_sampler,\n            batch_size=self.config[\"validation\"][\"batch_size\"],\n            num_workers=self.config[\"validation\"].get(\"num_workers\", 0),\n            pin_memory=self.config[\"validation\"].get(\"pin_memory\", True),\n            drop_last=self.config[\"validation\"].get(\"drop_last\", False),\n            prefetch_factor=self.config[\"validation\"].get(\"prefetch_factor\", None),\n        )\n\n        test_loader = DataLoader(\n            self.test_dataset,\n            sampler=self.test_sampler,\n            batch_size=self.config[\"testing\"][\"batch_size\"],\n            num_workers=self.config[\"testing\"].get(\"num_workers\", 0),\n            pin_memory=self.config[\"testing\"].get(\"pin_memory\", True),\n            drop_last=self.config[\"testing\"].get(\"drop_last\", False),\n            prefetch_factor=self.config[\"testing\"].get(\"prefetch_factor\", None),\n        )\n        self.logger.info(\n            f\"Data loaders prepared with splits: {split} and dataset sizes: {len(self.train_dataset)}, {len(self.val_dataset)}, {len(self.test_dataset)}\"\n        )\n        return train_loader, val_loader, test_loader\n\n    def _check_model_status(self, eval_data: list[Any] | torch.Tensor) -&gt; bool:\n        \"\"\"Check the status of the model during evaluation.\n\n        Args:\n            eval_data (list[Any] | torch.Tensor): The evaluation data to check.\n\n        Returns:\n            bool: Whether the model training should stop.\n        \"\"\"\n        should_stop = False\n        if self.rank == 0:\n            should_stop = super()._check_model_status(eval_data)\n        return should_stop\n\n    def save_checkpoint(self, name_addition: str = \"\"):\n        \"\"\"Save model checkpoint.\n\n        Raises:\n            ValueError: If the model is not initialized.\n            ValueError: If the model configuration does not contain 'name'.\n            ValueError: If the training configuration does not contain 'checkpoint_path'.\n        \"\"\"\n        if self.rank == 0:\n            if self.model is None:\n                raise ValueError(\"Model must be initialized before saving checkpoint.\")\n\n            if \"name\" not in self.config[\"model\"]:\n                raise ValueError(\n                    \"Model configuration must contain 'name' to save checkpoint.\"\n                )\n\n            self.logger.info(\n                f\"Saving checkpoint for model {self.config['model']['name']} at epoch {self.epoch} to {self.checkpoint_path}\"\n            )\n            outpath = (\n                self.checkpoint_path\n                / f\"{self.config['model']['name']}_epoch_{self.epoch}_{name_addition}.pt\"\n            )\n\n            if outpath.exists() is False:\n                outpath.parent.mkdir(parents=True, exist_ok=True)\n                self.logger.info(f\"Created directory {outpath.parent} for checkpoint.\")\n\n            self.latest_checkpoint = outpath\n            torch.save(self.model, outpath)\n\n    def run_training(\n        self,\n        train_loader: DataLoader,\n        val_loader: DataLoader,\n        trial: optuna.trial.Trial | None = None,\n    ) -&gt; Tuple[torch.Tensor | Collection[Any], torch.Tensor | Collection[Any]]:\n        \"\"\"\n        Run the training loop for the distributed model. This will synchronize for validation. No testing is performed in this function. The model will only be checkpointed and early stopped on the 'rank' 0 process.\n\n        Args:\n            train_loader (DataLoader): The training data loader.\n            val_loader (DataLoader): The validation data loader.\n            trial (optuna.trial.Trial | None, optional): An Optuna trial for hyperparameter optimization.\n                Defaults to None.\n\n        Returns:\n            Tuple[torch.Tensor | Collection[Any], torch.Tensor | Collection[Any]]: The training and validation results.\n        \"\"\"\n\n        self.model = self.initialize_model()\n        self.optimizer = self.initialize_optimizer()\n\n        num_epochs = self.config[\"training\"][\"num_epochs\"]\n        self.logger.info(\"Starting training process.\")\n\n        total_training_data = []\n        all_training_data: list[Any] = [None for _ in range(self.world_size)]\n        all_validation_data: list[Any] = [None for _ in range(self.world_size)]\n        for _ in range(0, num_epochs):\n            self.logger.info(f\"  Current epoch: {self.epoch}/{num_epochs}\")\n            self.model.train()\n            train_loader.sampler.set_epoch(self.epoch)\n            epoch_data = self._run_train_epoch(self.model, self.optimizer, train_loader)\n            total_training_data.append(epoch_data)\n\n            # evaluation run on validation set\n            self.model.eval()\n            if self.validator is not None:\n                validation_result = self.validator.validate(self.model, val_loader)\n                if self.rank == 0:\n                    self.validator.report(validation_result)\n\n                    # integrate Optuna here for hyperparameter tuning\n                    if trial is not None:\n                        avg_sigma_loss = self.validator.data[self.epoch]\n                        avg_loss = avg_sigma_loss[0]\n                        trial.report(avg_loss, self.epoch)\n\n                        # Handle pruning based on the intermediate value.\n                        if trial.should_prune():\n                            raise optuna.exceptions.TrialPruned()\n\n            dist.barrier()  # Ensure all processes have completed the epoch before checking status\n            should_stop = self._check_model_status(\n                self.validator.data if self.validator else total_training_data,\n            )\n\n            object_list = [should_stop]\n\n            should_stop = dist.broadcast_object_list(\n                object_list, src=0, device=self.device\n            )\n            should_stop = object_list[0]\n\n            if should_stop:\n                break\n\n            self.epoch += 1\n\n        dist.barrier()\n        dist.all_gather_object(all_training_data, total_training_data)\n        dist.all_gather_object(\n            all_validation_data, self.validator.data if self.validator else []\n        )\n        self.logger.info(\"Training process completed.\")\n\n        return all_training_data, all_validation_data\n</code></pre>"},{"location":"api/#QuantumGrav.train_ddp.TrainerDDP.__init__","title":"<code>__init__(rank, config, criterion, apply_model=None, early_stopping=None, validator=None, tester=None)</code>","text":"<p>Initialize the distributed data parallel (DDP) trainer.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>The rank of the current process.</p> required <code>config</code> <code>dict[str, Any]</code> <p>The configuration dictionary.</p> required <code>criterion</code> <code>Callable</code> <p>The loss function.</p> required <code>apply_model</code> <code>Callable | None</code> <p>The function to apply the model. Defaults to None.</p> <code>None</code> <code>early_stopping</code> <code>Callable[[list[dict[str, Any]]], bool] | None</code> <p>The early stopping function. Defaults to None.</p> <code>None</code> <code>validator</code> <code>DefaultValidator | None</code> <p>The validator for model evaluation. Defaults to None.</p> <code>None</code> <code>tester</code> <code>DefaultTester | None</code> <p>The tester for model testing. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the configuration is invalid.</p> Source code in <code>src/QuantumGrav/train_ddp.py</code> <pre><code>def __init__(\n    self,\n    rank: int,\n    config: dict[str, Any],\n    # training and evaluation functions\n    criterion: Callable,\n    apply_model: Callable | None = None,\n    # training evaluation and reporting\n    early_stopping: Callable[[Collection[Any] | torch.Tensor], bool] | None = None,\n    validator: DefaultValidator | None = None,\n    tester: DefaultTester | None = None,\n):\n    \"\"\"Initialize the distributed data parallel (DDP) trainer.\n\n    Args:\n        rank (int): The rank of the current process.\n        config (dict[str, Any]): The configuration dictionary.\n        criterion (Callable): The loss function.\n        apply_model (Callable | None, optional): The function to apply the model. Defaults to None.\n        early_stopping (Callable[[list[dict[str, Any]]], bool] | None, optional): The early stopping function. Defaults to None.\n        validator (DefaultValidator | None, optional): The validator for model evaluation. Defaults to None.\n        tester (DefaultTester | None, optional): The tester for model testing. Defaults to None.\n\n    Raises:\n        ValueError: If the configuration is invalid.\n    \"\"\"\n    if \"parallel\" not in config:\n        raise ValueError(\"Configuration must contain 'parallel' section for DDP.\")\n\n    super().__init__(\n        config,\n        criterion,\n        apply_model,\n        early_stopping,\n        validator,\n        tester,\n    )\n    # initialize the systems differently on each process/rank\n    torch.manual_seed(self.seed + rank)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(self.seed + rank)\n\n    if torch.cuda.is_available() and config[\"training\"][\"device\"] != \"cpu\":\n        torch.cuda.set_device(rank)\n        self.device = torch.device(f\"cuda:{rank}\")\n    else:\n        self.device = torch.device(\"cpu\")\n\n    self.rank = rank\n    self.world_size = config[\"parallel\"][\"world_size\"]\n    self.logger.info(\"Initialized DDP trainer\")\n</code></pre>"},{"location":"api/#QuantumGrav.train_ddp.TrainerDDP.initialize_model","title":"<code>initialize_model()</code>","text":"<p>Initialize the model for training.</p> <p>Returns:</p> Name Type Description <code>DDP</code> <code>DistributedDataParallel</code> <p>The initialized model.</p> Source code in <code>src/QuantumGrav/train_ddp.py</code> <pre><code>def initialize_model(self) -&gt; DDP:\n    \"\"\"Initialize the model for training.\n\n    Returns:\n        DDP: The initialized model.\n    \"\"\"\n    model = gnn_model.GNNModel.from_config(self.config[\"model\"])\n\n    if self.device.type == \"cpu\" or (\n        isinstance(self.device, torch.device) and self.device.type == \"cpu\"\n    ):\n        d_id = None\n        o_id = None\n    else:\n        d_id = [\n            self.device,\n        ]\n        o_id = self.config[\"parallel\"].get(\"output_device\", None)\n    model = DDP(\n        model,\n        device_ids=d_id,\n        output_device=o_id,\n        find_unused_parameters=self.config[\"parallel\"].get(\n            \"find_unused_parameters\", False\n        ),\n    )\n    self.model = model.to(self.device, non_blocking=True)\n    self.logger.info(f\"Model initialized on device: {self.device}\")\n    return self.model\n</code></pre>"},{"location":"api/#QuantumGrav.train_ddp.TrainerDDP.prepare_dataloaders","title":"<code>prepare_dataloaders(dataset, split=[0.8, 0.1, 0.1])</code>","text":"<p>Prepare the data loaders for training, validation, and testing.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to split.</p> required <code>split</code> <code>list[float]</code> <p>The proportions for train/val/test split. Defaults to [0.8, 0.1, 0.1].</p> <code>[0.8, 0.1, 0.1]</code> <p>Returns:</p> Type Description <code>Tuple[DataLoader, DataLoader, DataLoader]</code> <p>Tuple[ DataLoader, DataLoader, DataLoader, ]: The data loaders for training, validation, and testing.</p> Source code in <code>src/QuantumGrav/train_ddp.py</code> <pre><code>def prepare_dataloaders(\n    self, dataset: Dataset, split: list[float] = [0.8, 0.1, 0.1]\n) -&gt; Tuple[\n    DataLoader,\n    DataLoader,\n    DataLoader,\n]:\n    \"\"\"Prepare the data loaders for training, validation, and testing.\n\n    Args:\n        dataset (Dataset): The dataset to split.\n        split (list[float], optional): The proportions for train/val/test split. Defaults to [0.8, 0.1, 0.1].\n\n    Returns:\n        Tuple[ DataLoader, DataLoader, DataLoader, ]: The data loaders for training, validation, and testing.\n    \"\"\"\n    train_size = int(len(dataset) * split[0])\n    val_size = int(len(dataset) * split[1])\n    test_size = len(dataset) - train_size - val_size\n    self.logger.info(\n        f\"Preparing data loaders with split: {split}, train size: {train_size}, val size: {val_size}, test size: {test_size}\"\n    )\n    if (\n        np.isclose(np.sum(split), 1.0, rtol=1e-05, atol=1e-08, equal_nan=False)\n        is False\n    ):\n        raise ValueError(\n            \"Split ratios must sum to 1.0. Provided split: {}\".format(split)\n        )\n\n    self.train_dataset, self.val_dataset, self.test_dataset = (\n        torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n    )\n\n    # samplers are needed to distribute the data across processes in such a way that each process gets a unique subset of the data\n    self.train_sampler = torch.utils.data.DistributedSampler(\n        self.train_dataset,\n        num_replicas=self.world_size,\n        rank=self.rank,\n        shuffle=True,\n    )\n\n    self.val_sampler = torch.utils.data.DistributedSampler(\n        self.val_dataset,\n        num_replicas=self.world_size,\n        rank=self.rank,\n        shuffle=False,\n    )\n\n    self.test_sampler = torch.utils.data.DistributedSampler(\n        self.test_dataset,\n        num_replicas=self.world_size,\n        rank=self.rank,\n        shuffle=False,\n    )\n\n    # make the data loaders\n    train_loader = DataLoader(\n        self.train_dataset,\n        batch_size=self.config[\"training\"][\"batch_size\"],\n        sampler=self.train_sampler,\n        num_workers=self.config[\"training\"].get(\"num_workers\", 0),\n        pin_memory=self.config[\"training\"].get(\"pin_memory\", True),\n        drop_last=self.config[\"training\"].get(\"drop_last\", False),\n        prefetch_factor=self.config[\"training\"].get(\"prefetch_factor\", None),\n    )\n\n    val_loader = DataLoader(\n        self.val_dataset,\n        sampler=self.val_sampler,\n        batch_size=self.config[\"validation\"][\"batch_size\"],\n        num_workers=self.config[\"validation\"].get(\"num_workers\", 0),\n        pin_memory=self.config[\"validation\"].get(\"pin_memory\", True),\n        drop_last=self.config[\"validation\"].get(\"drop_last\", False),\n        prefetch_factor=self.config[\"validation\"].get(\"prefetch_factor\", None),\n    )\n\n    test_loader = DataLoader(\n        self.test_dataset,\n        sampler=self.test_sampler,\n        batch_size=self.config[\"testing\"][\"batch_size\"],\n        num_workers=self.config[\"testing\"].get(\"num_workers\", 0),\n        pin_memory=self.config[\"testing\"].get(\"pin_memory\", True),\n        drop_last=self.config[\"testing\"].get(\"drop_last\", False),\n        prefetch_factor=self.config[\"testing\"].get(\"prefetch_factor\", None),\n    )\n    self.logger.info(\n        f\"Data loaders prepared with splits: {split} and dataset sizes: {len(self.train_dataset)}, {len(self.val_dataset)}, {len(self.test_dataset)}\"\n    )\n    return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"api/#QuantumGrav.train_ddp.TrainerDDP.run_training","title":"<code>run_training(train_loader, val_loader, trial=None)</code>","text":"<p>Run the training loop for the distributed model. This will synchronize for validation. No testing is performed in this function. The model will only be checkpointed and early stopped on the 'rank' 0 process.</p> <p>Parameters:</p> Name Type Description Default <code>train_loader</code> <code>DataLoader</code> <p>The training data loader.</p> required <code>val_loader</code> <code>DataLoader</code> <p>The validation data loader.</p> required <code>trial</code> <code>Trial | None</code> <p>An Optuna trial for hyperparameter optimization. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Tensor | Collection[Any], Tensor | Collection[Any]]</code> <p>Tuple[torch.Tensor | Collection[Any], torch.Tensor | Collection[Any]]: The training and validation results.</p> Source code in <code>src/QuantumGrav/train_ddp.py</code> <pre><code>def run_training(\n    self,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    trial: optuna.trial.Trial | None = None,\n) -&gt; Tuple[torch.Tensor | Collection[Any], torch.Tensor | Collection[Any]]:\n    \"\"\"\n    Run the training loop for the distributed model. This will synchronize for validation. No testing is performed in this function. The model will only be checkpointed and early stopped on the 'rank' 0 process.\n\n    Args:\n        train_loader (DataLoader): The training data loader.\n        val_loader (DataLoader): The validation data loader.\n        trial (optuna.trial.Trial | None, optional): An Optuna trial for hyperparameter optimization.\n            Defaults to None.\n\n    Returns:\n        Tuple[torch.Tensor | Collection[Any], torch.Tensor | Collection[Any]]: The training and validation results.\n    \"\"\"\n\n    self.model = self.initialize_model()\n    self.optimizer = self.initialize_optimizer()\n\n    num_epochs = self.config[\"training\"][\"num_epochs\"]\n    self.logger.info(\"Starting training process.\")\n\n    total_training_data = []\n    all_training_data: list[Any] = [None for _ in range(self.world_size)]\n    all_validation_data: list[Any] = [None for _ in range(self.world_size)]\n    for _ in range(0, num_epochs):\n        self.logger.info(f\"  Current epoch: {self.epoch}/{num_epochs}\")\n        self.model.train()\n        train_loader.sampler.set_epoch(self.epoch)\n        epoch_data = self._run_train_epoch(self.model, self.optimizer, train_loader)\n        total_training_data.append(epoch_data)\n\n        # evaluation run on validation set\n        self.model.eval()\n        if self.validator is not None:\n            validation_result = self.validator.validate(self.model, val_loader)\n            if self.rank == 0:\n                self.validator.report(validation_result)\n\n                # integrate Optuna here for hyperparameter tuning\n                if trial is not None:\n                    avg_sigma_loss = self.validator.data[self.epoch]\n                    avg_loss = avg_sigma_loss[0]\n                    trial.report(avg_loss, self.epoch)\n\n                    # Handle pruning based on the intermediate value.\n                    if trial.should_prune():\n                        raise optuna.exceptions.TrialPruned()\n\n        dist.barrier()  # Ensure all processes have completed the epoch before checking status\n        should_stop = self._check_model_status(\n            self.validator.data if self.validator else total_training_data,\n        )\n\n        object_list = [should_stop]\n\n        should_stop = dist.broadcast_object_list(\n            object_list, src=0, device=self.device\n        )\n        should_stop = object_list[0]\n\n        if should_stop:\n            break\n\n        self.epoch += 1\n\n    dist.barrier()\n    dist.all_gather_object(all_training_data, total_training_data)\n    dist.all_gather_object(\n        all_validation_data, self.validator.data if self.validator else []\n    )\n    self.logger.info(\"Training process completed.\")\n\n    return all_training_data, all_validation_data\n</code></pre>"},{"location":"api/#QuantumGrav.train_ddp.TrainerDDP.save_checkpoint","title":"<code>save_checkpoint(name_addition='')</code>","text":"<p>Save model checkpoint.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not initialized.</p> <code>ValueError</code> <p>If the model configuration does not contain 'name'.</p> <code>ValueError</code> <p>If the training configuration does not contain 'checkpoint_path'.</p> Source code in <code>src/QuantumGrav/train_ddp.py</code> <pre><code>def save_checkpoint(self, name_addition: str = \"\"):\n    \"\"\"Save model checkpoint.\n\n    Raises:\n        ValueError: If the model is not initialized.\n        ValueError: If the model configuration does not contain 'name'.\n        ValueError: If the training configuration does not contain 'checkpoint_path'.\n    \"\"\"\n    if self.rank == 0:\n        if self.model is None:\n            raise ValueError(\"Model must be initialized before saving checkpoint.\")\n\n        if \"name\" not in self.config[\"model\"]:\n            raise ValueError(\n                \"Model configuration must contain 'name' to save checkpoint.\"\n            )\n\n        self.logger.info(\n            f\"Saving checkpoint for model {self.config['model']['name']} at epoch {self.epoch} to {self.checkpoint_path}\"\n        )\n        outpath = (\n            self.checkpoint_path\n            / f\"{self.config['model']['name']}_epoch_{self.epoch}_{name_addition}.pt\"\n        )\n\n        if outpath.exists() is False:\n            outpath.parent.mkdir(parents=True, exist_ok=True)\n            self.logger.info(f\"Created directory {outpath.parent} for checkpoint.\")\n\n        self.latest_checkpoint = outpath\n        torch.save(self.model, outpath)\n</code></pre>"},{"location":"api/#QuantumGrav.train_ddp.cleanup_ddp","title":"<code>cleanup_ddp()</code>","text":"<p>Clean up the distributed process group.</p> Source code in <code>src/QuantumGrav/train_ddp.py</code> <pre><code>def cleanup_ddp() -&gt; None:\n    \"\"\"Clean up the distributed process group.\"\"\"\n    if dist.is_initialized():\n        dist.destroy_process_group()\n        os.environ.pop(\"MASTER_ADDR\", None)\n        os.environ.pop(\"MASTER_PORT\", None)\n</code></pre>"},{"location":"api/#QuantumGrav.train_ddp.initialize_ddp","title":"<code>initialize_ddp(rank, worldsize, master_addr='localhost', master_port='12345', backend='nccl')</code>","text":"<p>Initialize the distributed process group. This assumes one process per GPU.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>The rank of the current process.</p> required <code>worldsize</code> <code>int</code> <p>The total number of processes.</p> required <code>master_addr</code> <code>str</code> <p>The address of the master process. Defaults to \"localhost\". This needs to be the ip of the master node if you are running on a cluster.</p> <code>'localhost'</code> <code>master_port</code> <code>str</code> <p>The port of the master process. Defaults to \"12345\". Choose a high port if you are running multiple jobs on the same machine to avoid conflicts. If running on a cluster, this should be the port that the master node is listening on.</p> <code>'12345'</code> <code>backend</code> <code>str</code> <p>The backend to use for distributed training. Defaults to \"nccl\".</p> <code>'nccl'</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the environment variables MASTER_ADDR and MASTER_PORT are already set.</p> Source code in <code>src/QuantumGrav/train_ddp.py</code> <pre><code>def initialize_ddp(\n    rank: int,\n    worldsize: int,\n    master_addr: str = \"localhost\",\n    master_port: str = \"12345\",\n    backend: str = \"nccl\",\n) -&gt; None:\n    \"\"\"Initialize the distributed process group. This assumes one process per GPU.\n\n    Args:\n        rank (int): The rank of the current process.\n        worldsize (int): The total number of processes.\n        master_addr (str, optional): The address of the master process. Defaults to \"localhost\". This needs to be the ip of the master node if you are running on a cluster.\n        master_port (str, optional): The port of the master process. Defaults to \"12345\". Choose a high port if you are running multiple jobs on the same machine to avoid conflicts. If running on a cluster, this should be the port that the master node is listening on.\n        backend (str, optional): The backend to use for distributed training. Defaults to \"nccl\".\n\n    Raises:\n        RuntimeError: If the environment variables MASTER_ADDR and MASTER_PORT are already set.\n    \"\"\"\n    if dist.is_initialized():\n        raise RuntimeError(\"The distributed process group is already initialized.\")\n    else:\n        os.environ[\"MASTER_ADDR\"] = master_addr\n        os.environ[\"MASTER_PORT\"] = master_port\n        dist.init_process_group(backend=backend, rank=rank, world_size=worldsize)\n</code></pre>"},{"location":"api/#utilities","title":"Utilities","text":"<p>General utilities that are used throughout this package. </p>"},{"location":"api/#QuantumGrav.utils.assign_at_path","title":"<code>assign_at_path(cfg, path, value)</code>","text":"<p>Assign a value to a key in a nested dictionary 'dict'. The path to follow through this nested structure is given by 'path'.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>dict</code> <p>The configuration dictionary to modify.</p> required <code>path</code> <code>Sequence[Any]</code> <p>The path to the key to modify as a list of nodes to traverse.</p> required <code>value</code> <code>Any</code> <p>The value to assign to the key.</p> required Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def assign_at_path(cfg: dict, path: Sequence[Any], value: Any) -&gt; None:\n    \"\"\"Assign a value to a key in a nested dictionary 'dict'. The path to follow through this nested structure is given by 'path'.\n\n    Args:\n        cfg (dict): The configuration dictionary to modify.\n        path (Sequence[Any]): The path to the key to modify as a list of nodes to traverse.\n        value (Any): The value to assign to the key.\n    \"\"\"\n    for p in path[:-1]:\n        cfg = cfg[p]\n    cfg[path[-1]] = value\n</code></pre>"},{"location":"api/#QuantumGrav.utils.get_at_path","title":"<code>get_at_path(cfg, path, default=None)</code>","text":"<p>Get the value at a key in a nested dictionary. The path to follow through this nested structure is given by 'path'.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>dict</code> <p>The configuration dictionary to modify.</p> required <code>path</code> <code>Sequence[Any]</code> <p>The path to the key to get as a list of nodes to traverse.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value at the specified key, or None if not found.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def get_at_path(cfg: dict, path: Sequence[Any], default: Any = None) -&gt; Any:\n    \"\"\"Get the value at a key in a nested dictionary. The path to follow through this nested structure is given by 'path'.\n\n    Args:\n        cfg (dict): The configuration dictionary to modify.\n        path (Sequence[Any]): The path to the key to get as a list of nodes to traverse.\n\n    Returns:\n        Any: The value at the specified key, or None if not found.\n    \"\"\"\n    for p in path[:-1]:\n        cfg = cfg[p]\n\n    return cfg.get(path[-1], default)\n</code></pre>"},{"location":"api/#QuantumGrav.utils.get_evaluation_function","title":"<code>get_evaluation_function(name)</code>","text":"<p>Get a registered evaluation function by name</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the function to get</p> required <p>Returns:</p> Type Description <code>Callable | type | None</code> <p>Callable | type | None: a callable or type if registered function is found, None otherwise</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def get_evaluation_function(name: str) -&gt; Callable | type | None:\n    \"\"\"Get a registered evaluation function by name\n\n    Args:\n        name (str): name of the function to get\n\n    Returns:\n        Callable | type | None: a callable or type if registered function is found, None otherwise\n    \"\"\"\n    return evaluation_funcs.get(name, None)\n</code></pre>"},{"location":"api/#QuantumGrav.utils.get_graph_features_aggregation","title":"<code>get_graph_features_aggregation(name)</code>","text":"<p>Get a registered graph features aggregation function by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the graph features aggregation function.</p> required <p>Returns:</p> Type Description <code>type[Module] | Callable | None</code> <p>type[torch.nn.Module] | Callable | None: Function to aggregate graph features outputs, or None if not found.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def get_graph_features_aggregation(\n    name: str,\n) -&gt; type[torch.nn.Module] | Callable | None:\n    \"\"\"Get a registered graph features aggregation function by name.\n\n    Args:\n        name (str): The name of the graph features aggregation function.\n\n    Returns:\n        type[torch.nn.Module] | Callable | None: Function to aggregate graph features outputs, or None if not found.\n    \"\"\"\n    return (\n        graph_features_aggregations[name]\n        if name in graph_features_aggregations\n        else None\n    )\n</code></pre>"},{"location":"api/#QuantumGrav.utils.get_pooling_aggregation","title":"<code>get_pooling_aggregation(name)</code>","text":"<p>Get a registered pooling aggregation function by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the pooling aggregation function.</p> required <p>Returns:</p> Type Description <code>type[Module] | Callable | None</code> <p>type[torch.nn.Module] | Callable: Function to aggregate pooling layer outputs.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def get_pooling_aggregation(name: str) -&gt; type[torch.nn.Module] | Callable | None:\n    \"\"\"Get a registered pooling aggregation function by name.\n\n    Args:\n        name (str): The name of the pooling aggregation function.\n\n    Returns:\n        type[torch.nn.Module] | Callable: Function to aggregate pooling layer outputs.\n    \"\"\"\n    return pooling_aggregations[name] if name in pooling_aggregations else None\n</code></pre>"},{"location":"api/#QuantumGrav.utils.get_registered_activation","title":"<code>get_registered_activation(name)</code>","text":"<p>Get a registered activation layer by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the activation layer.</p> required <p>Returns:</p> Type Description <code>type[Module] | None</code> <p>type[torch.nn.Module] | None: The registered activation layer named <code>name</code>, or None if not found.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def get_registered_activation(name: str) -&gt; type[torch.nn.Module] | None:\n    \"\"\"Get a registered activation layer by name.\n\n    Args:\n        name (str): The name of the activation layer.\n\n    Returns:\n        type[torch.nn.Module] | None: The registered activation layer named `name`, or None if not found.\n    \"\"\"\n    return activation_layers[name] if name in activation_layers else None\n</code></pre>"},{"location":"api/#QuantumGrav.utils.get_registered_gnn_layer","title":"<code>get_registered_gnn_layer(name)</code>","text":"<p>Get a registered GNN layer by name. Args:     name (str): The name of the GNN layer.</p> <p>Returns:</p> Type Description <code>type[Module] | None</code> <p>type[torch.nn.Module] | None: The registered GNN layer named <code>name</code>, or None if not found.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def get_registered_gnn_layer(name: str) -&gt; type[torch.nn.Module] | None:\n    \"\"\"Get a registered GNN layer by name.\n    Args:\n        name (str): The name of the GNN layer.\n\n    Returns:\n        type[torch.nn.Module] | None: The registered GNN layer named `name`, or None if not found.\n    \"\"\"\n    return gnn_layers[name] if name in gnn_layers else None\n</code></pre>"},{"location":"api/#QuantumGrav.utils.get_registered_normalizer","title":"<code>get_registered_normalizer(name)</code>","text":"<p>Get a registered normalizer layer by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the normalizer layer.</p> required <p>Returns:</p> Type Description <code>type[Module] | None</code> <p>type[torch.nn.Module]| None: The registered normalizer layer named <code>name</code>, or None if not found.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def get_registered_normalizer(name: str) -&gt; type[torch.nn.Module] | None:\n    \"\"\"Get a registered normalizer layer by name.\n\n    Args:\n        name (str): The name of the normalizer layer.\n\n    Returns:\n        type[torch.nn.Module]| None: The registered normalizer layer named `name`, or None if not found.\n    \"\"\"\n    return normalizer_layers[name] if name in normalizer_layers else None\n</code></pre>"},{"location":"api/#QuantumGrav.utils.get_registered_pooling_layer","title":"<code>get_registered_pooling_layer(name)</code>","text":"<p>Get a registered pooling layer by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the pooling layer.</p> required <p>Returns:</p> Type Description <code>type[Module] | Callable | None</code> <p>type[torch.nn.Module] | Callable | None: The registered pooling layer named <code>name</code>, or None if not found.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def get_registered_pooling_layer(name: str) -&gt; type[torch.nn.Module] | Callable | None:\n    \"\"\"Get a registered pooling layer by name.\n\n    Args:\n        name (str): The name of the pooling layer.\n\n    Returns:\n        type[torch.nn.Module] | Callable | None: The registered pooling layer named `name`, or None if not found.\n    \"\"\"\n    return pooling_layers[name] if name in pooling_layers else None\n</code></pre>"},{"location":"api/#QuantumGrav.utils.import_and_get","title":"<code>import_and_get(importpath)</code>","text":"<p>Import a module and get an object from it.</p> <p>Parameters:</p> Name Type Description Default <code>importpath</code> <code>str</code> <p>The import path of the object to get.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The name as imported from the module.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>When the module indicated by the path is not found</p> <code>KeyError</code> <p>When the object name indidcated by the path is not found in the module</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def import_and_get(importpath: str) -&gt; Any:\n    \"\"\"Import a module and get an object from it.\n\n    Args:\n        importpath (str): The import path of the object to get.\n\n    Returns:\n        Any: The name as imported from the module.\n\n    Raises:\n        KeyError: When the module indicated by the path is not found\n        KeyError: When the object name indidcated by the path is not found in the module\n    \"\"\"\n    parts = importpath.split(\".\")\n    module_name = \".\".join(parts[:-1])\n    object_name = parts[-1]\n\n    try:\n        module = importlib.import_module(module_name)\n    except Exception as e:\n        raise KeyError(f\"Importing module {module_name} unsuccessful\") from e\n    try:\n        return getattr(module, object_name)\n    except Exception as e:\n        raise KeyError(f\"Could not load name {object_name} from {module_name}\") from e\n</code></pre>"},{"location":"api/#QuantumGrav.utils.list_evaluation_functions","title":"<code>list_evaluation_functions()</code>","text":"<p>list the registered evaluation functions</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: list of names of eval functions</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def list_evaluation_functions() -&gt; list[str]:\n    \"\"\"list the registered evaluation functions\n\n    Returns:\n        list[str]: list of names of eval functions\n    \"\"\"\n    return list(evaluation_funcs.keys())\n</code></pre>"},{"location":"api/#QuantumGrav.utils.list_registered_activations","title":"<code>list_registered_activations()</code>","text":"<p>List all registered activation layers.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def list_registered_activations() -&gt; list[str]:\n    \"\"\"List all registered activation layers.\"\"\"\n    return list(activation_layers.keys())\n</code></pre>"},{"location":"api/#QuantumGrav.utils.list_registered_gnn_layers","title":"<code>list_registered_gnn_layers()</code>","text":"<p>List all registered GNN layers.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def list_registered_gnn_layers() -&gt; list[str]:\n    \"\"\"List all registered GNN layers.\"\"\"\n    return list(gnn_layers.keys())\n</code></pre>"},{"location":"api/#QuantumGrav.utils.list_registered_graph_features_aggregations","title":"<code>list_registered_graph_features_aggregations()</code>","text":"<p>List all registered graph features aggregation functions.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of registered graph features aggregation function names.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def list_registered_graph_features_aggregations() -&gt; list[str]:\n    \"\"\"List all registered graph features aggregation functions.\n\n    Returns:\n        list[str]: A list of registered graph features aggregation function names.\n    \"\"\"\n    return list(graph_features_aggregations.keys())\n</code></pre>"},{"location":"api/#QuantumGrav.utils.list_registered_normalizers","title":"<code>list_registered_normalizers()</code>","text":"<p>List all registered normalizer layers.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def list_registered_normalizers() -&gt; list[str]:\n    \"\"\"List all registered normalizer layers.\"\"\"\n    return list(normalizer_layers.keys())\n</code></pre>"},{"location":"api/#QuantumGrav.utils.list_registered_pooling_aggregations","title":"<code>list_registered_pooling_aggregations()</code>","text":"<p>List all registered pooling aggregation functions.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of registered pooling aggregation function names.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def list_registered_pooling_aggregations() -&gt; list[str]:\n    \"\"\"List all registered pooling aggregation functions.\n\n    Returns:\n        list[str]: A list of registered pooling aggregation function names.\n    \"\"\"\n    return list(pooling_aggregations.keys())\n</code></pre>"},{"location":"api/#QuantumGrav.utils.list_registered_pooling_layers","title":"<code>list_registered_pooling_layers()</code>","text":"<p>List all registered pooling layers.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def list_registered_pooling_layers() -&gt; list[str]:\n    \"\"\"List all registered pooling layers.\"\"\"\n    return list(pooling_layers.keys())\n</code></pre>"},{"location":"api/#QuantumGrav.utils.register_activation","title":"<code>register_activation(activation_name, activation_layer)</code>","text":"<p>Register an activation layer with the module</p> <p>Parameters:</p> Name Type Description Default <code>activation_name</code> <code>str</code> <p>The name of the activation layer.</p> required <code>activation_layer</code> <code>type[Module]</code> <p>The activation layer to register.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the activation layer is already registered.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def register_activation(\n    activation_name: str, activation_layer: type[torch.nn.Module]\n) -&gt; None:\n    \"\"\"Register an activation layer with the module\n\n    Args:\n        activation_name (str): The name of the activation layer.\n        activation_layer (type[torch.nn.Module]): The activation layer to register.\n\n    Raises:\n        ValueError: If the activation layer is already registered.\n    \"\"\"\n    if activation_name in activation_layers:\n        raise ValueError(f\"Activation '{activation_name}' is already registered.\")\n    activation_layers[activation_name] = activation_layer\n    activation_layers_names[activation_layer] = activation_name\n</code></pre>"},{"location":"api/#QuantumGrav.utils.register_evaluation_function","title":"<code>register_evaluation_function(name, func)</code>","text":"<p>register a new evaluation function by name</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the function</p> required <code>func</code> <code>Callable | type</code> <p>function to register</p> required Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def register_evaluation_function(name: str, func: Callable | type) -&gt; None:\n    \"\"\"register a new evaluation function by name\n\n    Args:\n        name (str): name of the function\n        func (Callable | type): function to register\n    \"\"\"\n    if name in evaluation_funcs:\n        raise ValueError(f\"Evaluation function '{name}' is already registered.\")\n    evaluation_funcs[name] = func\n</code></pre>"},{"location":"api/#QuantumGrav.utils.register_gnn_layer","title":"<code>register_gnn_layer(gnn_layer_name, gnn_layer)</code>","text":"<p>Register a GNN layer with the module</p> <p>Parameters:</p> Name Type Description Default <code>gnn_layer_name</code> <code>str</code> <p>The name of the GNN layer.</p> required <code>gnn_layer</code> <code>type[Module]</code> <p>The GNN layer to register.</p> required Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def register_gnn_layer(gnn_layer_name: str, gnn_layer: type[torch.nn.Module]) -&gt; None:\n    \"\"\"Register a GNN layer with the module\n\n    Args:\n        gnn_layer_name (str): The name of the GNN layer.\n        gnn_layer (type[torch.nn.Module]): The GNN layer to register.\n    \"\"\"\n    if gnn_layer_name in gnn_layers:\n        raise ValueError(f\"GNN layer '{gnn_layer_name}' is already registered.\")\n    gnn_layers[gnn_layer_name] = gnn_layer\n    gnn_layers_names[gnn_layer] = gnn_layer_name\n</code></pre>"},{"location":"api/#QuantumGrav.utils.register_graph_features_aggregation","title":"<code>register_graph_features_aggregation(aggregation_name, aggregation_function)</code>","text":"<p>Register a graph features aggregation function with the module</p> <p>Parameters:</p> Name Type Description Default <code>aggregation_name</code> <code>str</code> <p>The name of the graph features aggregation function.</p> required <code>aggregation_function</code> <code>Callable</code> <p>The graph features aggregation function to register.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the graph features aggregation function is already registered.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def register_graph_features_aggregation(\n    aggregation_name: str, aggregation_function: type[torch.nn.Module] | Callable\n) -&gt; None:\n    \"\"\"Register a graph features aggregation function with the module\n\n    Args:\n        aggregation_name (str): The name of the graph features aggregation function.\n        aggregation_function (Callable): The graph features aggregation function to register.\n\n    Raises:\n        ValueError: If the graph features aggregation function is already registered.\n    \"\"\"\n    if aggregation_name in graph_features_aggregations:\n        raise ValueError(\n            f\"Graph features aggregation '{aggregation_name}' is already registered.\"\n        )\n    graph_features_aggregations[aggregation_name] = aggregation_function\n    graph_features_aggregations_names[aggregation_function] = aggregation_name\n</code></pre>"},{"location":"api/#QuantumGrav.utils.register_normalizer","title":"<code>register_normalizer(normalizer_name, normalizer_layer)</code>","text":"<p>Register a normalizer layer with the module</p> <p>Parameters:</p> Name Type Description Default <code>normalizer_name</code> <code>str</code> <p>The name of the normalizer.</p> required <code>normalizer_layer</code> <code>type[Module]</code> <p>The normalizer layer to register.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the normalizer layer is already registered.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def register_normalizer(\n    normalizer_name: str, normalizer_layer: type[torch.nn.Module]\n) -&gt; None:\n    \"\"\"Register a normalizer layer with the module\n\n    Args:\n        normalizer_name (str): The name of the normalizer.\n        normalizer_layer (type[torch.nn.Module]): The normalizer layer to register.\n\n    Raises:\n        ValueError: If the normalizer layer is already registered.\n    \"\"\"\n    if normalizer_name in normalizer_layers:\n        raise ValueError(f\"Normalizer '{normalizer_name}' is already registered.\")\n    normalizer_layers[normalizer_name] = normalizer_layer\n    normalizer_layers_names[normalizer_layer] = normalizer_name\n</code></pre>"},{"location":"api/#QuantumGrav.utils.register_pooling_aggregation","title":"<code>register_pooling_aggregation(aggregation_name, aggregation_function)</code>","text":"<p>Register a pooling aggregation function with the module</p> <p>Parameters:</p> Name Type Description Default <code>aggregation_name</code> <code>str</code> <p>The name of the pooling aggregation function.</p> required <code>aggregation_function</code> <code>Callable</code> <p>The pooling aggregation function to register.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pooling aggregation function is already registered.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def register_pooling_aggregation(\n    aggregation_name: str, aggregation_function: Callable\n) -&gt; None:\n    \"\"\"Register a pooling aggregation function with the module\n\n    Args:\n        aggregation_name (str): The name of the pooling aggregation function.\n        aggregation_function (Callable): The pooling aggregation function to register.\n\n    Raises:\n        ValueError: If the pooling aggregation function is already registered.\n    \"\"\"\n    if aggregation_name in pooling_aggregations:\n        raise ValueError(\n            f\"Pooling aggregation '{aggregation_name}' is already registered.\"\n        )\n    pooling_aggregations[aggregation_name] = aggregation_function\n    pooling_aggregations_names[aggregation_function] = aggregation_name\n</code></pre>"},{"location":"api/#QuantumGrav.utils.register_pooling_layer","title":"<code>register_pooling_layer(pooling_layer_name, pooling_layer)</code>","text":"<p>Register a pooling layer with the module</p> <p>Parameters:</p> Name Type Description Default <code>pooling_layer_name</code> <code>str</code> <p>The name of the pooling layer.</p> required <code>pooling_layer</code> <code>Module</code> <p>The pooling layer to register.</p> required Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def register_pooling_layer(\n    pooling_layer_name: str, pooling_layer: type[torch.nn.Module] | Callable\n) -&gt; None:\n    \"\"\"Register a pooling layer with the module\n\n    Args:\n        pooling_layer_name (str): The name of the pooling layer.\n        pooling_layer (torch.nn.Module): The pooling layer to register.\n    \"\"\"\n    if pooling_layer_name in pooling_layers:\n        raise ValueError(f\"Pooling layer '{pooling_layer_name}' is already registered.\")\n    pooling_layers[pooling_layer_name] = pooling_layer\n    pooling_layers_names[pooling_layer] = pooling_layer_name\n</code></pre>"},{"location":"api/#QuantumGrav.utils.verify_config_node","title":"<code>verify_config_node(cfg)</code>","text":"<p>Verify that a config node has the required keys.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>dict</code> <p>The config node to verify.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the config node is valid, False otherwise.</p> Source code in <code>src/QuantumGrav/utils.py</code> <pre><code>def verify_config_node(cfg) -&gt; bool:\n    # TODO: remove when schemas are there\n    \"\"\"Verify that a config node has the required keys.\n\n    Args:\n        cfg (dict): The config node to verify.\n\n    Returns:\n        bool: True if the config node is valid, False otherwise.\n    \"\"\"\n    required_keys = {\"type\", \"args\", \"kwargs\"}\n    if not isinstance(cfg, dict):\n        return False\n    if not required_keys.issubset(cfg.keys()):\n        return False\n    if not isinstance(cfg[\"args\"], list):\n        return False\n    if not isinstance(cfg[\"kwargs\"], dict):\n        return False\n    return True\n</code></pre>"},{"location":"datasets_and_preprocessing/","title":"Using Datasets for data processing and batching","text":""},{"location":"datasets_and_preprocessing/#raw-data","title":"Raw data","text":"<p><code>QuantumGrav</code> supports Zarr as a data format for storing the raw data. Each cset, by default, is stored in it's own group in the file. In most cases, you want one index, typically the first or the last, to be used as the sample index, such that each array has $$N = n_{sample} + 1$$ dimensions, with $$n_{sample}$$ being the dimensionality of the data for a single sample. For practicality, this should be the same in each stored array.</p>"},{"location":"datasets_and_preprocessing/#concept","title":"Concept","text":"<p>The various Dataset all work by the same principle:</p> <ul> <li>read in all data that is needed to construct a complete <code>pytorch_geometric.data.Data</code> object, which represents a single cset graph, its feature data and its target data. The former will be processed by the graph neural network models this package is designed to build, the latter are the targets for supervised learning tasks.</li> <li>Build the features (node-level, graph-level or edge-level) and targets from the raw data.</li> <li>Make a <code>Data</code> object and save it to disk via <code>torch.save</code>.</li> </ul> <p>Becausse this package does not make assumptions about the structure and character of the input raw data, you need to supply a set of functions yourself:</p> <ul> <li> <p>A <code>reader</code> function that reads the raw data to construct a single cset/graph/sample.</p> </li> <li> <p>A <code>pre_transform</code> function which builds the actual <code>Data</code> object. This will be executed only once when you open the dataset path.  Internally, the dataset will create a directory named <code>processed</code> which will contain the processed files, one for each cset. The precence of this directory is used to determine if <code>pre_transform</code> is executed again, so you can go to the directory and delete <code>processed</code> or rename it to trigger a new processing run.</p> </li> <li> <p>A <code>pre_filter</code> function which filters out undesired raw samples and only lets a subset through to be processed by <code>pre_transform</code>. The semantics is the same as <code>pre_transform</code>, and the two will always be executed together.</p> </li> <li> <p>A <code>transform</code> function which is executed each time the dataset path on disk is opened, and can be used to execute all data transformations that would need to be carried out each time a dataset is loaded.</p> </li> </ul> <p>The last three are part of <code>pytorch</code>/<code>pytorch_geometric</code>'s <code>Dataset</code> API, so check out the respective documentation to learn more about them.</p>"},{"location":"datasets_and_preprocessing/#how-to-create-a-dataset-object","title":"How to create a dataset object","text":"<p>Each dataset must first know where the raw data is stored. A set of .zarr files can be passed as a list.</p> <p>Next, it needs to know where to store the processed data. This is given by a single <code>pathlib.Path</code> or <code>string</code> object. A <code>processed</code> directory will be created there and the result of <code>pretransform</code> for each sample will be stored there. Let's inspect the signature of the constructor:</p> <pre><code>class QGDataset(QGDatasetBase, Dataset):\n    def __init__(\n        self,\n        input: list[str | Path],\n        output: str | Path,\n        reader: Callable[[zarr.Group, torch.dtype, torch.dtype, bool], list[Data]] | None = None,\n        float_type: torch.dtype = torch.float32,\n        int_type: torch.dtype = torch.int64,\n        validate_data: bool = True,\n        chunksize: int = 1000,\n        n_processes: int = 1,\n        # dataset properties\n        transform: Callable[[Data | Collection], Data] | None = None,\n        pre_transform: Callable[[Data | Collection], Data] | None = None,\n        pre_filter: Callable[[Data | Collection], bool] | None = None,\n    )\n</code></pre> <p>First, we need to define the list of input files. We also need to choose the output directory.  If we used <code>zarr</code> files, we would put <code>zarr</code> here.</p> <pre><code>  input = ['path/to/file1.zarr', 'path/to/file2.zarr', 'path/to/file3.zarr']\n  output = 'path/to/output'\n</code></pre> <p>Then, we have to define the function that reads data from the file. This eats a zarr file, a float and int type, and whether to validate the data or not. For example:</p> <pre><code>def reader(\n        f: zarr.Group, idx: int, float_dtype: torch.dtype, int_dtype: torch.dtype, validate: bool\n    ) -&gt; Data:\n\n        # get the adjacency matrix\n        adj_raw = f[\"adjacency_matrix\"][idx, :, :]\n        adj_matrix = torch.tensor(adj_raw, dtype=float_dtype)\n        node_features = []\n\n        # Path lengths\n        max_path_future = torch.tensor(\n            f[\"max_pathlen_future\"][idx, :], dtype=float_dtype\n        ).unsqueeze(1)  # make this a (num_nodes, 1) tensor\n\n        max_path_past = torch.tensor(\n            f[\"max_pathlen_past\"][idx, :], dtype=float_dtype\n        ).unsqueeze(1)  # make this a (num_nodes, 1) tensor\n        node_features.extend([max_path_future, max_path_past])\n\n        # make the targets\n        manifold = f[\"manifold\"][idx]\n        boundary = f[\"boundary\"][idx]\n        dimension = f[\"dimension\"][idx]\n\n        if (\n            isinstance(manifold, np.ndarray)\n            and isinstance(boundary, np.ndarray)\n            and isinstance(dimension, np.ndarray)\n        ):\n            value_list = [manifold.item(), boundary.item(), dimension.item()]\n        else:\n            value_list = [manifold, boundary, dimension]\n\n        return {\n            \"adj\": adj_matrix,\n            \"max_pathlen_future\": max_path_future,\n            \"max_pathlen_past\": max_path_past,\n            \"manifold\": manifold,\n            \"boundary\": boundary,\n            \"dimension\": dimension\n        }\n</code></pre> <p>Now we have a function that turns raw data into a dictionary. Next, we need the <code>pre_filter</code> and <code>pre_transform</code> functions. We want to retain all data, so <code>pre_filter</code> can just return true all the time:</p> <p><pre><code>pre_filter = lambda x: true\n</code></pre> or we can filter out some targets:</p> <pre><code>pre_filter = lambda data: data.y[2] != 2\n</code></pre> <p>Then, we need the <code>pre_transform</code> function which truns the returned dict into a <code>torch_geometric.data.Data</code> object. Here, we want to fix the adjacency matrix because Julia uses a different convention. we could have done this right away in the reader function, too, but it's a good way to show what <code>pre_transform</code> can do.</p> <pre><code>def pre_transform(data: Data) -&gt; Data:\n    \"\"\"Pre-transform the data dictionary into a  Data object.\"\"\"\n    adjacency_matrix = data[\"adjacency_matrix\"]\n    cset_size = data[\"cset_size\"]\n    # this is a workaround for the fact that the adjacency matrix is stored in a transposed form when going from julia to hdf5\n    adjacency_matrix = np.transpose(adjacency_matrix)\n    adjacency_matrix = adjacency_matrix[0:cset_size, 0:cset_size]\n    edge_index, edge_weight = dense_to_sparse(\n        torch.tensor(adjacency_matrix, dtype=torch.float32)\n    )\n\n    node_features = []\n    for feature_name in data[\"feature_names\"]:\n        node_features.append(data[feature_name])\n    x = torch.cat(node_features, dim=1).to(torch.float32)\n    y = torch.tensor(data[\"manifold_like\"]).to(torch.long)\n\n    # make data object\n    tgdata = Data(\n        x=x,\n        edge_index=edge_index,\n        edge_attr=edge_weight,\n        y=y,\n    )\n\n    if not tgdata.validate():\n        raise ValueError(f\"Data validation failed for index {idx}.\")\n    return tgdata\n</code></pre> <p>The <code>transform</code> function follows the same principle, so we don't show it explicitly here and just set it to a no-op: <pre><code>transform = lambda x: x\n</code></pre> Now, we can put together our dataset. Upon first instantiation, it will pre-process the data in the files given as <code>input</code> into <code>Data</code> objects and store them individually in a directory <code>output/processed</code>. As long as it sees this directory, it will not process any files again when another dataset is opend with the same output path. Data procesessing will be parallelized over the number of processes given as <code>n_processes</code>, and <code>chunk_size</code> many samples will be processed at once.</p> <p><pre><code>dataset = QGDataset(\n    input,\n    output,\n    reader = reader,\n    validate_data = True,\n    chunksize = 5000,\n    n_processes= 12,\n    transform = transform,\n    pre_transform = pre_transform,\n    pre_filter = pre_filter,\n)\n</code></pre> Here we use 12 processes which process the data in chunks of 5000 samples before loading the next 5000 using the <code>reader</code> function, processing them and so on.</p>"},{"location":"datasets_and_preprocessing/#onthefly-dataset","title":"OntheFly dataset","text":"<p>We will rarely use this, so no explicit example is provided. You can check out the <code>test_ontheflydataset.py</code> test file to see how it is used in principle.</p>"},{"location":"getting_started/","title":"Getting started","text":"<p>This document explains how to get the project running for development and for usage. It covers the two supported ecosystems used in this repository: the Julia package (data generation and low-level causal-set utilities) and the Python package (model code, training and evaluation).</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<p>WARNING: Currently, QG only supports CUDA 12.8</p> <p>The Python package lives in <code>QuantumGravPy/</code> and follows a standard packaging layout (sources under <code>QuantumGravPy/src/QuantumGrav</code>). The project uses PyTorch and PyTorch Geometric; installation of those dependencies depends on your OS and available hardware (CUDA version).</p> <p>Basic steps (UNIX-like shells):</p> <ol> <li>Create and activate a virtual environment:</li> </ol> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\n</code></pre> <ol> <li>Upgrade pip:</li> </ol> <pre><code>python -m pip install --upgrade pip\n</code></pre> <ol> <li>Install PyTorch (follow the instructions at https://pytorch.org/get-started/locally/ for the correct wheel for your platform and CUDA). Example (CPU-only wheel).</li> </ol> <pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n</code></pre> <ol> <li> <p>For PyTorch Geometric, follow the project documentation for the correct platform-specific wheels: https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html</p> </li> <li> <p>Install the Python package (recommended editable install for development):</p> </li> </ol> <pre><code>cd QuantumGravPy\npython -m pip install -e .\n</code></pre> <p>After installation you can import the package as <code>import QuantumGrav</code> from Python code or a REPL.</p>"},{"location":"getting_started/#installation-as-a-developer","title":"Installation as a developer","text":"<ol> <li>Clone the repository from here.</li> </ol> <p>Do an editable install so local changes in <code>QuantumGravPy/src/QuantumGrav</code> are available immediately:</p> <pre><code>cd QuantumGravPy\npython -m pip install -e .[dev,docs]\n</code></pre> <p>Run the Python unit tests from the repository root (with the virtualenv activated):</p> <pre><code>cd QuantumGravPy\npytest test\n</code></pre>"},{"location":"getting_started/#building-the-documentation-locally","title":"Building the documentation locally","text":"<p>The documentation is generated with MkDocs and <code>mkdocstrings</code>. MkDocs needs to be able to import the <code>QuantumGrav</code> package so either install the package in the same environment (editable install above) or add the <code>src</code> path to <code>PYTHONPATH</code> before running mkdocs.</p> <p>Quick serve (from repository root):</p> <p><pre><code>cd QuantumGravPy\n# if you didn't install the package, export PYTHONPATH to include the src dir\nmkdocs serve\n</code></pre> Follow the instructions on screen to open the documentation. More on <code>mkdocs</code> can be found here, and on the <code>Documenter.jl</code> package used in the Julia package here.</p> <p>Training and evaluation scripts live under <code>QuantumGravPy/src/QuantumGrav/</code> (<code>train.py</code>, <code>train_ddp.py</code>) and can be run once dependencies are installed. See the training section for more.</p>"},{"location":"getting_started/#notes-and-troubleshooting","title":"Notes and troubleshooting","text":"<ul> <li>PyTorch and PyTorch-Geometric installation is platform and CUDA-version specific; consult the official installation docs if you encounter wheel or binary compatibility errors.</li> </ul>"},{"location":"getting_started/#contribution-guide","title":"Contribution guide","text":"<p>Tbd.</p>"},{"location":"hparam_tuning/","title":"Hyperparameter Optimization","text":"<p>The <code>QuantumGrav</code> Python package lets users customize hyperparameters when building, training, validating, and testing GNN models. Choosing the right values is crucial for model performance.</p> <p>To accelerate this process, we developed two ways to handle possible values of hyperparameters:</p> <ul> <li>Using custom YAML tags to generate a list of configs based on the cartesian product of possible values of hyperparameters, then run training on each config.<ul> <li>Pros: Fully deterministic, exhaustive, simple, transparent</li> <li>Cons: Explodes combinatorially, no guided search</li> <li>Useful if:<ul> <li>Our search space is small.</li> <li>We need absolute transparency and determinism.</li> <li>We are debugging or doing systematic experiments.</li> <li>We want full coverage of combinations.</li> <li>We avoid extra dependencies or complexity.</li> </ul> </li> </ul> </li> <li>Using Optuna to create hyperparameter search space on a config file then automatically find optimal hyperparameter values for specific objectives (e.g. minimizing loss or maximizing accuracy).<ul> <li>Pros: Support of intelligent search and pruning algorithms, visualization (when using with database), scalable running</li> <li>Cons: Stochastic sampling, less exhaustive, more complex as we need to integrate the training loop into Optuna's objective function</li> <li>Useful if:<ul> <li>The search space is moderately large or huge.</li> <li>We want efficient, guided optimization.</li> <li>We need early stopping for poor configs.</li> <li>We run on clusters or need parallel trials.</li> <li>We want good results with limited compute budget.</li> </ul> </li> </ul> </li> </ul>"},{"location":"hparam_tuning/#config-handling-with-custom-yaml-tags","title":"Config handling with custom YAML tags","text":"<p>We developed the following custom YAML tags to specify possible values of hyperparamters:</p> <ul> <li> <p><code>!sweep</code> tag: Used to define possible categorical values for a hyperparameter. For example, if we want to experiment with two different values for the number of epochs (32 and 64), we can specify them as follows:     <pre><code>training:\n    ...\n    epochs: !sweep\n        values: [32, 64]\n</code></pre></p> </li> <li> <p><code>!coupled-sweep</code> tag: If a hyperparameter\u2019s values depend on another sweep hyperparameter, we can use coupled-sweep to link them. For example, if we want the batch size to be 64 when training for 32 epochs and 128 when training for 64 epochs, we can specify the batch size as follows:     <pre><code>training:\n    ...\n    batch_size: !coupled-sweep\n        target: training.epochs\n        values: [64, 128]\n</code></pre></p> </li> <li> <p><code>!range</code> tag: If a hyperparameter takes values within an int or float range, the range can be specified as below:     <pre><code>epochs: !range # int range\n    start: 10\n    stop: 30\n    step: 5\ndrop_rate: !range # float range\n    start: 0.1\n    stop: 0.5\n    step: 0.2\n</code></pre> Note: The stop value is included.</p> </li> <li> <p><code>!random_uniform</code> tag: This tag generates a set of float values uniformly sampled from a specified range. We can define the <code>size</code> (the number of values to produce); if not provided, it defaults to 5. When <code>log=True</code>, values are sampled in the log domain; otherwise, they are sampled linearly.     <pre><code>lr: !random_uniform # float values in log domain\n    start: 1e-5\n    stop: 1e-2\n    log: true\n    size: 7\n</code></pre></p> </li> <li> <p><code>!reference</code> tag: In some cases, a hyperparameter must share the same value as another. For instance, the input dimension of one layer and the output dimension of the previous layer. The <code>!reference</code> tag handles such cases:     <pre><code>model:\n    layers:\n        -\n            in_dim: 728\n            out_dim: 64\n        -\n            in_dim: !reference\n                target: model.layers[0].out_dim\n</code></pre></p> </li> <li> <p><code>!pyobject</code> tag: Ultimately, to specify a Python object in the config file, we use the <code>!pyobject</code> tag. This is useful for assigning a Python object to a model or layer type. For example:     <pre><code>model:\n    conv_layer: !pyobject torch_geometric.nn.conv.sage_conv.SAGEConv\n</code></pre></p> </li> </ul>"},{"location":"hparam_tuning/#confighandler-class","title":"ConfigHandler class","text":"<p>TODO: brief explanation.</p>"},{"location":"hparam_tuning/#optimization-with-optuna","title":"Optimization with Optuna","text":"<p>The second option for finding optimal hyperparameter values is to use Optuna together with the custom YAML tag.</p> <p>We developed a subpackage <code>QGTune</code> to convert a config file with custom YAML tags to hyperparameter search space for Optuna. Main purpose of the GQTune subpackage includes:</p> <ul> <li>Build Optuna search space from a model/trainer config YAML file (with above custom tags)</li> <li>Create an Optuna study from a tuning config file (preferably YAML file)</li> <li>Save the best config with optimal hyperparameter values back to a YAML file.</li> </ul>"},{"location":"hparam_tuning/#define-an-optuna-search-space-using-qgtune","title":"Define an Optuna search space using QGTune","text":""},{"location":"hparam_tuning/#optuna-suggestion-with-custom-yaml-tag","title":"Optuna suggestion with custom YAML tag","text":"<p>Optuna use optuna.trial.Trial to define hyperparameter search space for each study. Main functions include:</p> <ul> <li><code>suggest_categorical()</code>: suggest a value for a categorical parameter, which is a list of <code>bool</code>, <code>string</code>, <code>float</code>, or <code>int</code></li> <li><code>suggest_float()</code>: suggest a value within a range for a floating point parameter, in linear or log domain</li> <li><code>suggest_int()</code>: suggest a value within a range for an integer parameter</li> </ul> <p>The first suggestion function corresponds to the <code>!sweep</code> tag described above, while <code>suggest_int()</code> maps to the <code>!range</code> tag (when <code>start</code>, <code>stop</code>, and <code>step</code> are all integers). Float suggestions are handled through the <code>!range</code> and <code>!random_uniform</code> tags.</p> <p>For example, the following configuration:</p> <pre><code>epochs: !range\n    start: 10\n    stop: 30\n    step: 5\n</code></pre> <p>is equivalent to calling <code>optuna.trial.Trial.suggest_int(param_name, start, stop, step)</code>. </p> <p>Here, <code>param_name</code> represents the parameter name stored in the Optuna study. We define this name as the path from the root node to the current node in the configuration file, e.g. <code>training.epochs</code> or <code>model.layer.0.in_dim</code>. Note that in this naming scheme, all parts are separated by dots (<code>.</code>). Unlike the <code>target</code> field in the <code>!reference</code> tag, no square brackets (<code>[]</code>) are used.</p>"},{"location":"hparam_tuning/#create-optuna-search-space-from-a-modeltrainer-config-file","title":"Create Optuna search space from a model/trainer config file","text":"<p>To build an Optuna search space with <code>QGTune</code>, we can use <code>build_search_space()</code> function as in the following example:</p> <pre><code>from QuantumGrav.QGTune import tune\n\ndef objective(trial: optuna.trial.Trial, config_file: Path):\n\n    search_space = tune.build_search_space(\n        config_file=config_file,\n        trial=trial,\n    )\n  ...\n</code></pre> <ul> <li><code>objective</code> is the function that will be used later by an Optuna study for optimization</li> <li><code>trial</code> is an object of <code>optuna.trial.Trial</code>, representing trials of an Optuna study</li> <li><code>config_file</code> is path to the model/trainer configuration file (see the configuration <code>dict</code> for an example)</li> <li> <p><code>search_space</code> is a dictionary constructed from the model/trainer config file, where values of hyperparameters defined with custom YAML tags are replaced by their corresponding Optuna trial suggestions. For example, the <code>epochs</code> configuration shown earlier would be transformed into:     <pre><code>training:\n    epochs: optuna.trial.Trial.suggest_int(\"training.epochs\", start, stop, step)\n</code></pre></p> <p>Subsequently, this <code>search_space</code> dictionary would be used for creating a GNN model and training the model as described in The Trainer class.</p> </li> </ul>"},{"location":"hparam_tuning/#create-an-optuna-study","title":"Create an Optuna study","text":"<p>The input for creating an Optuna study is a tuning configuration dictionary that should include essential keys like <code>\"storage\"</code>, <code>\"study_name\"</code>, and <code>\"direction\"</code>, for example:</p> <pre><code>{\n  \"study_name\": \"quantum_grav_study\", # name of the Optuna study\n  \"storage\": \"experiments/results.log\", # only supports JournalStorage for multi-processing\n  \"direction\": \"minimize\" # direction of optimization (\"minimize\" or \"maximize\")\n}\n</code></pre> <p>If <code>storage</code> is assigned to <code>None</code> (or <code>null</code> in YAML file), the study will be saved with <code>optuna.storages.InMemoryStorage</code>, i.e. in RAM only until the Python session ends.</p> <p>For simplicity while working with multi-processing, we only support storage with Optuna's JournalStorage.</p>"},{"location":"hparam_tuning/#save-the-best-config-to-a-yaml-file","title":"Save the best config to a YAML file","text":"<p>After completing all trials, the hyperparameter values from the best trial, along with other fixed parameters, can be saved to a YAML file using the <code>save_best_config()</code> function.</p> <pre><code>def save_best_config(\n    config_file: Path,\n    best_trial: optuna.trial.FrozenTrial,\n    output_file: Path,\n):\n</code></pre> <ul> <li><code>config_file</code> is the path to the model/trainer configuration file</li> <li><code>best_trial</code> is the trial with the optimal results, obtained from an Optuna study.</li> <li><code>output_file</code> is the path to the YAML output file</li> </ul>"},{"location":"hparam_tuning/#an-example-of-tuning-with-qgtune","title":"An example of tuning with QGTune","text":"<p>We present here an example to demonstrate the functionality of <code>QGTune</code>.</p>"},{"location":"hparam_tuning/#example-explanation","title":"Example explanation","text":"<p>In this example, we would:</p> <ul> <li>create sample configuration files for tuning and model/trainer settings</li> <li>define a small model based on Optuna's PyTorch example.</li> <li>load dat from Fashion-MNIST dataset. </li> <li> <p>define the Optuna <code>objective</code> function.</p> <ul> <li>The task is to classify each 28\u00d728 grayscale image into one of 10 fashion categories, such as T-shirt, coat, or sneaker.</li> <li>To enable Optuna to monitor training progress, <code>trial.report()</code> should be called after each epoch to record the metric being optimized. In this example, we use <code>accuracy</code>.</li> </ul> <pre><code>def objective(trial: optuna.trial.Trial, config_file: Path):\n  ...\n  search_space = ...\n  ...\n  # prepare model\n  ...\n  # prepare optimizer\n  ...\n  # prepare data\n  ...\n  for epoch in range(epochs):\n    # train the model\n    ...\n    # validate the model\n    ...\n    accuracy = ...\n\n    trial.report(accuracy, epoch)\n    if trial.should_prune():\n        raise optuna.exceptions.TrialPruned()\n</code></pre> </li> <li> <p>run the optimization process for a specified number of trials (<code>n_trials</code>)</p> </li> <li>use Optuna's multi-process optimization by setting the number of iterations (<code>n_iterations</code>) and processes (<code>n_processes</code>).</li> </ul>"},{"location":"hparam_tuning/#example-code","title":"Example code","text":"<pre><code># tune_example.py\n\nimport os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # make sure no GPU is used\n\nfrom QuantumGrav.QGTune import tune\nimport optuna\nimport yaml\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\nfrom pathlib import Path\nfrom multiprocessing import Pool\nfrom functools import partial\nfrom optuna.storages.journal import JournalStorage, JournalFileBackend\n\nDEVICE = torch.device(\"cpu\")\n\ncurrent_dir = Path(__file__).parent\ntmp_dir = current_dir / \"tmp\"\n\n\ndef create_tuning_config_file(tmp_path: Path) -&gt; Path:\n    \"\"\"Create a tuning configuration YAML file for testing purposes.\"\"\"\n    # Note on n_jobs: Multi-thread optimization has traditionally been inefficient\n    # in Python due to the Global Interpreter Lock (GIL) (Python &lt; 3.14)\n    tuning_config = {\n        \"config_file\": str(tmp_path / \"config.yaml\"),\n        \"study_name\": \"test_study\",\n        \"storage\": str(tmp_path / \"test_study.log\"),\n        \"direction\": \"maximize\",\n        \"n_trials\": 15,\n        \"timeout\": 600,\n        \"n_jobs\": 1,  # set to &gt;1 to enable multi-threading,\n        \"best_config_file\": str(tmp_path / \"best_config.yaml\"),\n        \"n_processes\": 2,\n        \"n_iterations\": 2,\n    }\n    tuning_config_file = tmp_path / \"tuning_config.yaml\"\n    with open(tuning_config_file, \"w\") as f:\n        yaml.safe_dump(tuning_config, f)\n\n    return tuning_config_file\n\n\ndef create_config_file(file_path: Path) -&gt; Path:\n    \"\"\"Create a base configuration YAML file with tuning parameters for testing purposes.\"\"\"\n    yaml_text = \"\"\"\n        model:\n            n_layers: 3\n            nn:\n                -\n                    in_dim: 784\n                    out_dim: !sweep\n                        values: [128, 256]\n                    dropout: !range\n                        start: 0.2\n                        stop: 0.5\n                        step: 0.1\n                -\n                    in_dim: !reference\n                        target: model.nn[0].out_dim\n                    out_dim: !sweep\n                        values: [16, 32]\n                    dropout: !range\n                        start: 0.2\n                        stop: 0.5\n                        step: 0.1\n                -\n                    in_dim: !reference\n                        target: model.nn[1].out_dim\n                    out_dim: !sweep\n                        values: [16, 32]\n                    dropout: !range\n                        start: 0.2\n                        stop: 0.5\n                        step: 0.1\n        training:\n            batch_size: !sweep\n                values: [16, 32]\n            optimizer: !sweep\n                values: [\"Adam\", \"SGD\"]\n            lr: !random_uniform\n                start: 1e-5\n                stop: 1e-2\n                log: true\n            epochs: !sweep\n                values: [2, 5, 7]\n    \"\"\"\n\n    with open(file_path, \"w\") as f:\n        f.write(yaml_text)\n\n\ndef define_small_model(config):\n    n_layers = config[\"model\"][\"n_layers\"]\n    layers = []\n\n    for i in range(n_layers):\n        in_dim = config[\"model\"][\"nn\"][i][\"in_dim\"]\n        out_dim = config[\"model\"][\"nn\"][i][\"out_dim\"]\n        layers.append(nn.Linear(in_dim, out_dim))\n        layers.append(nn.ReLU())\n        dropout = config[\"model\"][\"nn\"][i][\"dropout\"]\n        layers.append(nn.Dropout(dropout))\n\n    layers.append(nn.Linear(out_dim, 10))  # classification of 10 classes\n    layers.append(nn.LogSoftmax(dim=1))\n\n    return nn.Sequential(*layers)\n\n\ndef load_data(config, dir_path):\n    batch_size = config[\"training\"][\"batch_size\"]\n    # Load FashionMNIST dataset.\n    train_loader = torch.utils.data.DataLoader(\n        datasets.FashionMNIST(\n            dir_path, train=True, download=True, transform=transforms.ToTensor()\n        ),\n        batch_size=batch_size,\n        shuffle=True,\n    )\n    valid_loader = torch.utils.data.DataLoader(\n        datasets.FashionMNIST(dir_path, train=False, transform=transforms.ToTensor()),\n        batch_size=batch_size,\n        shuffle=True,\n    )\n\n    return train_loader, valid_loader\n\n\ndef objective(trial: optuna.trial.Trial, config_file: Path):\n\n    search_space = tune.build_search_space(\n        config_file=config_file,\n        trial=trial,\n    )\n\n    # prepare model\n    model = define_small_model(search_space).to(DEVICE)\n\n    # prepare optimizer\n    optimizer_name = search_space[\"training\"][\"optimizer\"]\n    lr = search_space[\"training\"][\"lr\"]\n    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=lr)\n\n    # prepare data\n    data_dir = tmp_dir / \"data\"\n    train_loader, valid_loader = load_data(search_space, data_dir)\n\n    # train the model\n    epochs = search_space[\"training\"][\"epochs\"]\n    batch_size = search_space[\"training\"][\"batch_size\"]\n    n_train_examples = batch_size * 30\n    n_valid_examples = batch_size * 10\n\n    # training loop\n    for epoch in range(epochs):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            if batch_idx * batch_size &gt; n_train_examples:\n                break\n\n            data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n\n            optimizer.zero_grad()\n            output = model(data)\n            loss = nn.NLLLoss()(output, target)\n            loss.backward()\n            optimizer.step()\n\n        # validate the model\n        model.eval()\n        correct = 0\n        with torch.no_grad():\n            for batch_idx, (data, target) in enumerate(valid_loader):\n                if batch_idx * batch_size &gt; n_valid_examples:\n                    break\n\n                data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n                output = model(data)\n                pred = output.argmax(dim=1, keepdim=True)\n                correct += pred.eq(target.view_as(pred)).sum().item()\n\n        accuracy = correct / min(len(valid_loader.dataset), n_valid_examples)\n\n        # report value to Optuna\n        trial.report(accuracy, epoch)\n\n        # prune trial if needed\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n    return accuracy\n\n\ndef tune_integration(run_idx, tuning_config):  # run_idx is the iteration index\n\n    study = tune.create_study(tuning_config)\n    study.optimize(\n        partial(objective, config_file=Path(tuning_config[\"config_file\"])),\n        n_trials=tuning_config[\"n_trials\"],\n        timeout=tuning_config[\"timeout\"],\n        n_jobs=tuning_config[\"n_jobs\"],\n    )\n\n    pruned_trials = study.get_trials(\n        deepcopy=False, states=[optuna.trial.TrialState.PRUNED]\n    )\n    complete_trials = study.get_trials(\n        deepcopy=False, states=[optuna.trial.TrialState.COMPLETE]\n    )\n\n    return (\n        len(study.trials),\n        len(pruned_trials),\n        len(complete_trials),\n        study.best_trial.value,\n    )\n\n\nif __name__ == \"__main__\":\n    if not tmp_dir.exists():\n        tmp_dir.mkdir(parents=True, exist_ok=True)\n\n    # create tuning config\n    tuning_config = tune.load_yaml(create_tuning_config_file(tmp_dir))\n    n_processes = tuning_config.get(\"n_processes\", 1)\n    n_iterations = tuning_config.get(\"n_iterations\", 1)\n\n    # create config file\n    create_config_file(Path(tuning_config[\"config_file\"]))\n\n    print(\"Starting the tuning integration process...\")\n    with Pool(processes=n_processes) as pool:\n        local_results = pool.map(\n            partial(tune_integration, tuning_config=tuning_config), range(n_iterations)\n        )\n\n    print(\"Tuning results for each run:------------------\")\n    for i, result in enumerate(local_results):\n        n_trials, n_pruned, n_complete, best_value = result\n        print(f\"Study statistics for run {i}: \")\n        print(\"  Number of finished trials: \", n_trials)\n        print(\"  Number of pruned trials: \", n_pruned)\n        print(\"  Number of complete trials: \", n_complete)\n        print(\"  Best trial value: \", best_value)\n\n    storage = JournalStorage(\n        JournalFileBackend(tuning_config[\"storage\"])  # use uncompressed journal\n    )\n    study = optuna.load_study(study_name=tuning_config[\"study_name\"], storage=storage)\n\n    print(\"Best trial global:----------------------------\")\n    trial = study.best_trial\n\n    print(\"  Value: \", trial.value)\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n\n    print(\"Saving the best configuration...\")\n    tune.save_best_config(\n        config_file=Path(tuning_config[\"config_file\"]),\n        best_trial=trial,\n        output_file=Path(tuning_config[\"best_config_file\"]),\n    )\n</code></pre>"},{"location":"hparam_tuning/#example-result","title":"Example result","text":"<pre><code>Starting the tuning integration process...\n[I 2025-11-07 11:24:36,912] A new study created in Journal with name: test_study\nStudy test_study was created and saved to /QuantumGrav/QuantumGravPy/docs/tmp/test_study.log.\n[I 2025-11-07 11:24:36,946] Using an existing study with name 'test_study' instead of creating a new one.\nStudy test_study was created and saved to /QuantumGrav/QuantumGravPy/docs/tmp/test_study.log.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.4M/26.4M [00:00&lt;00:00, 57.6MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.4M/26.4M [00:00&lt;00:00, 54.9MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29.5k/29.5k [00:00&lt;00:00, 4.39MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29.5k/29.5k [00:00&lt;00:00, 4.53MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.42M/4.42M [00:00&lt;00:00, 43.7MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.42M/4.42M [00:00&lt;00:00, 42.4MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.15k/5.15k [00:00&lt;00:00, 88.9MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.15k/5.15k [00:00&lt;00:00, 40.9MB/s]\n[I 2025-11-07 11:24:38,433] Trial 1 finished with value: 0.625 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.4, 'model.nn.1.out_dim': 32, 'model.nn.1.dropout': 0.5, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.30000000000000004, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.0016765190716563534, 'training.epochs': 5}. Best is trial 1 with value: 0.625.\n[I 2025-11-07 11:24:38,632] Trial 2 finished with value: 0.075 and parameters: {'model.nn.0.out_dim': 256, 'model.nn.0.dropout': 0.30000000000000004, 'model.nn.1.out_dim': 32, 'model.nn.1.dropout': 0.5, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.30000000000000004, 'training.batch_size': 16, 'training.optimizer': 'SGD', 'training.lr': 0.0002146791042032724, 'training.epochs': 2}. Best is trial 1 with value: 0.625.\n[I 2025-11-07 11:24:38,666] Trial 0 finished with value: 0.08125 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.2, 'model.nn.1.out_dim': 32, 'model.nn.1.dropout': 0.30000000000000004, 'model.nn.2.out_dim': 16, 'model.nn.2.dropout': 0.5, 'training.batch_size': 32, 'training.optimizer': 'SGD', 'training.lr': 0.0003945248169221987, 'training.epochs': 7}. Best is trial 1 with value: 0.625.\n[I 2025-11-07 11:24:38,894] Trial 3 finished with value: 0.0875 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.5, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.2, 'model.nn.2.out_dim': 16, 'model.nn.2.dropout': 0.4, 'training.batch_size': 16, 'training.optimizer': 'SGD', 'training.lr': 0.00018633239421692437, 'training.epochs': 5}. Best is trial 1 with value: 0.625.\n[I 2025-11-07 11:24:38,898] Trial 4 finished with value: 0.090625 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.4, 'model.nn.1.out_dim': 32, 'model.nn.1.dropout': 0.2, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.4, 'training.batch_size': 32, 'training.optimizer': 'SGD', 'training.lr': 0.00379972665577471, 'training.epochs': 2}. Best is trial 1 with value: 0.625.\n[I 2025-11-07 11:24:39,314] Trial 6 finished with value: 0.13125 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.2, 'model.nn.1.out_dim': 32, 'model.nn.1.dropout': 0.4, 'model.nn.2.out_dim': 16, 'model.nn.2.dropout': 0.2, 'training.batch_size': 16, 'training.optimizer': 'SGD', 'training.lr': 0.0008484799816474145, 'training.epochs': 7}. Best is trial 1 with value: 0.625.\n[I 2025-11-07 11:24:39,435] Trial 5 finished with value: 0.1375 and parameters: {'model.nn.0.out_dim': 256, 'model.nn.0.dropout': 0.30000000000000004, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.30000000000000004, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.30000000000000004, 'training.batch_size': 16, 'training.optimizer': 'SGD', 'training.lr': 8.354945492960286e-05, 'training.epochs': 7}. Best is trial 1 with value: 0.625.\n[I 2025-11-07 11:24:39,641] Trial 8 finished with value: 0.09375 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.30000000000000004, 'model.nn.1.out_dim': 32, 'model.nn.1.dropout': 0.5, 'model.nn.2.out_dim': 16, 'model.nn.2.dropout': 0.5, 'training.batch_size': 32, 'training.optimizer': 'SGD', 'training.lr': 8.08272287524503e-05, 'training.epochs': 2}. Best is trial 1 with value: 0.625.\n[I 2025-11-07 11:24:39,718] Trial 7 finished with value: 0.11875 and parameters: {'model.nn.0.out_dim': 256, 'model.nn.0.dropout': 0.30000000000000004, 'model.nn.1.out_dim': 32, 'model.nn.1.dropout': 0.2, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.5, 'training.batch_size': 16, 'training.optimizer': 'SGD', 'training.lr': 0.0006188877342998116, 'training.epochs': 7}. Best is trial 1 with value: 0.625.\n[I 2025-11-07 11:24:39,822] Trial 9 finished with value: 0.103125 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.5, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.4, 'model.nn.2.out_dim': 16, 'model.nn.2.dropout': 0.5, 'training.batch_size': 32, 'training.optimizer': 'SGD', 'training.lr': 0.0008168448454213926, 'training.epochs': 2}. Best is trial 1 with value: 0.625.\n[I 2025-11-07 11:24:39,913] Trial 10 finished with value: 0.134375 and parameters: {'model.nn.0.out_dim': 256, 'model.nn.0.dropout': 0.30000000000000004, 'model.nn.1.out_dim': 32, 'model.nn.1.dropout': 0.2, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.5, 'training.batch_size': 32, 'training.optimizer': 'SGD', 'training.lr': 0.00021660920841439104, 'training.epochs': 2}. Best is trial 1 with value: 0.625.\n[I 2025-11-07 11:24:40,157] Trial 11 finished with value: 0.18125 and parameters: {'model.nn.0.out_dim': 256, 'model.nn.0.dropout': 0.4, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.5, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.2, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.00883615551351022, 'training.epochs': 5}. Best is trial 1 with value: 0.625.\n[I 2025-11-07 11:24:40,269] Trial 12 finished with value: 0.15625 and parameters: {'model.nn.0.out_dim': 256, 'model.nn.0.dropout': 0.4, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.30000000000000004, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.30000000000000004, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 1.5473312399729955e-05, 'training.epochs': 5}. Best is trial 1 with value: 0.625.\n[I 2025-11-07 11:24:40,531] Trial 13 finished with value: 0.29375 and parameters: {'model.nn.0.out_dim': 256, 'model.nn.0.dropout': 0.4, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.5, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.2, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.009542442262415065, 'training.epochs': 5}. Best is trial 1 with value: 0.625.\n[I 2025-11-07 11:24:40,610] Trial 14 finished with value: 0.40625 and parameters: {'model.nn.0.out_dim': 256, 'model.nn.0.dropout': 0.4, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.5, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.2, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.008577489282530437, 'training.epochs': 5}. Best is trial 1 with value: 0.625.\n[I 2025-11-07 11:24:40,855] Trial 15 finished with value: 0.6375 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.5, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.4, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.2, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.002801697167722077, 'training.epochs': 5}. Best is trial 15 with value: 0.6375.\n[I 2025-11-07 11:24:40,924] Trial 16 finished with value: 0.5375 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.5, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.4, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.2, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.0024869223253056747, 'training.epochs': 5}. Best is trial 15 with value: 0.6375.\n[I 2025-11-07 11:24:41,217] Trial 17 finished with value: 0.5 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.5, 'model.nn.1.out_dim': 32, 'model.nn.1.dropout': 0.4, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.30000000000000004, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.0017531394038905789, 'training.epochs': 5}. Best is trial 15 with value: 0.6375.\n[I 2025-11-07 11:24:41,311] Trial 18 finished with value: 0.55625 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.5, 'model.nn.1.out_dim': 32, 'model.nn.1.dropout': 0.4, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.30000000000000004, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.0020300569180315316, 'training.epochs': 5}. Best is trial 15 with value: 0.6375.\n[I 2025-11-07 11:24:41,545] Trial 19 finished with value: 0.54375 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.5, 'model.nn.1.out_dim': 32, 'model.nn.1.dropout': 0.4, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.4, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.0015706471695032163, 'training.epochs': 5}. Best is trial 15 with value: 0.6375.\n[I 2025-11-07 11:24:41,628] Trial 20 finished with value: 0.5875 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.5, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.5, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.4, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.0035034963024678494, 'training.epochs': 5}. Best is trial 15 with value: 0.6375.\n[I 2025-11-07 11:24:41,852] Trial 21 finished with value: 0.60625 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.4, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.5, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.2, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.004692563836368309, 'training.epochs': 5}. Best is trial 15 with value: 0.6375.\n[I 2025-11-07 11:24:41,949] Trial 22 finished with value: 0.475 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.5, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.5, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.4, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.00413377148937998, 'training.epochs': 5}. Best is trial 15 with value: 0.6375.\n[I 2025-11-07 11:24:42,158] Trial 23 finished with value: 0.5375 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.4, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.5, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.2, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.004366383387329541, 'training.epochs': 5}. Best is trial 15 with value: 0.6375.\n[I 2025-11-07 11:24:42,268] Trial 24 finished with value: 0.55625 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.4, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.5, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.2, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.005176248318654213, 'training.epochs': 5}. Best is trial 15 with value: 0.6375.\n[I 2025-11-07 11:24:42,493] Trial 25 finished with value: 0.51875 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.4, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.4, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.2, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.0011242281028770243, 'training.epochs': 5}. Best is trial 15 with value: 0.6375.\n[I 2025-11-07 11:24:42,577] Trial 26 finished with value: 0.50625 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.4, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.4, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.30000000000000004, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.001187102582494059, 'training.epochs': 5}. Best is trial 15 with value: 0.6375.\n[I 2025-11-07 11:24:43,038] Trial 27 finished with value: 0.44375 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.4, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.30000000000000004, 'model.nn.2.out_dim': 16, 'model.nn.2.dropout': 0.30000000000000004, 'training.batch_size': 32, 'training.optimizer': 'Adam', 'training.lr': 0.0013102961720892286, 'training.epochs': 5}. Best is trial 15 with value: 0.6375.\n[I 2025-11-07 11:24:43,132] Trial 28 finished with value: 0.465625 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.4, 'model.nn.1.out_dim': 16, 'model.nn.1.dropout': 0.30000000000000004, 'model.nn.2.out_dim': 16, 'model.nn.2.dropout': 0.30000000000000004, 'training.batch_size': 32, 'training.optimizer': 'Adam', 'training.lr': 0.0005325671025809822, 'training.epochs': 5}. Best is trial 15 with value: 0.6375.\n[I 2025-11-07 11:24:43,504] Trial 29 finished with value: 0.7125 and parameters: {'model.nn.0.out_dim': 128, 'model.nn.0.dropout': 0.2, 'model.nn.1.out_dim': 32, 'model.nn.1.dropout': 0.5, 'model.nn.2.out_dim': 32, 'model.nn.2.dropout': 0.2, 'training.batch_size': 16, 'training.optimizer': 'Adam', 'training.lr': 0.0026496075439612654, 'training.epochs': 7}. Best is trial 29 with value: 0.7125.\nTuning results for each run:------------------\nStudy statistics for run 0: \n  Number of finished trials:  29\n  Number of pruned trials:  0\n  Number of complete trials:  28\n  Best trial value:  0.6375\nStudy statistics for run 1: \n  Number of finished trials:  30\n  Number of pruned trials:  0\n  Number of complete trials:  30\n  Best trial value:  0.7125\nBest trial global:----------------------------\n  Value:  0.7125\n  Params: \n    model.nn.0.out_dim: 128\n    model.nn.0.dropout: 0.2\n    model.nn.1.out_dim: 32\n    model.nn.1.dropout: 0.5\n    model.nn.2.out_dim: 32\n    model.nn.2.dropout: 0.2\n    training.batch_size: 16\n    training.optimizer: Adam\n    training.lr: 0.0026496075439612654\n    training.epochs: 7\nSaving the best configuration...\nBest configuration saved to /QuantumGrav/QuantumGravPy/docs/tmp/best_config.yaml.\n</code></pre>"},{"location":"hparam_tuning/#notes-on-pruner-and-parallelization","title":"Notes on pruner and parallelization","text":"<p>We used optuna.pruners.MedianPruner when creating an Optuna study (<code>QGTune.tune.create_study()</code>). Support for additional pruners may be added in the future if required.</p> <p>Although users can specify <code>n_jobs</code> (for multi-threading) when running a study optimization, we recommend keeping <code>n_jobs</code> set to <code>1</code>, according to Optuna's Multi-thread Optimization.</p>"},{"location":"license/","title":"License","text":"<pre><code>                GNU GENERAL PUBLIC LICENSE\n                   Version 3, 29 June 2007\n</code></pre> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/  Everyone is permitted to copy and distribute verbatim copies  of this license document, but changing it is not allowed.</p> <pre><code>                        Preamble\n</code></pre> <p>The GNU General Public License is a free, copyleft license for software and other kinds of works.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works.  By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.  We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors.  You can apply it to your programs, too.</p> <p>When we speak of free software, we are referring to freedom, not price.  Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights.  Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.</p> <p>For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received.  You must make sure that they, too, receive or can get the source code.  And you must show them these terms so they know their rights.</p> <p>Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.</p> <p>For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software.  For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.</p> <p>Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so.  This is fundamentally incompatible with the aim of protecting users' freedom to change the software.  The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable.  Therefore, we have designed this version of the GPL to prohibit the practice for those products.  If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.</p> <p>Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary.  To prevent this, the GPL assures that patents cannot be used to render the program non-free.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p> <pre><code>                   TERMS AND CONDITIONS\n</code></pre> <ol> <li>Definitions.</li> </ol> <p>\"This License\" refers to version 3 of the GNU General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License.  Each licensee is addressed as \"you\".  \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy.  The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy.  Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies.  Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License.  If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p> <ol> <li>Source Code.</li> </ol> <p>The \"source code\" for a work means the preferred form of the work for making modifications to it.  \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form.  A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities.  However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work.  For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p> <ol> <li>Basic Permissions.</li> </ol> <p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met.  This License explicitly affirms your unlimited permission to run the unmodified Program.  The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work.  This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force.  You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright.  Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below.  Sublicensing is not allowed; section 10 makes it unnecessary.</p> <ol> <li>Protecting Users' Legal Rights From Anti-Circumvention Law.</li> </ol> <p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p> <ol> <li>Conveying Verbatim Copies.</li> </ol> <p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p> <ol> <li>Conveying Modified Source Versions.</li> </ol> <p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <pre><code>a) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\n</code></pre> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit.  Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p> <ol> <li>Conveying Non-Source Forms.</li> </ol> <p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <pre><code>a) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\n</code></pre> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling.  In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage.  For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product.  A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source.  The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information.  But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed.  Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p> <ol> <li>Additional Terms.</li> </ol> <p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law.  If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it.  (Additional permissions may be written to require their own removal in certain cases when you modify the work.)  You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <pre><code>a) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\n</code></pre> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10.  If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term.  If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p> <ol> <li>Termination.</li> </ol> <p>You may not propagate or modify a covered work except as expressly provided under this License.  Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License.  If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p> <ol> <li>Acceptance Not Required for Having Copies.</li> </ol> <p>You are not required to accept this License in order to receive or run a copy of the Program.  Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance.  However, nothing other than this License grants you permission to propagate or modify any covered work.  These actions infringe copyright if you do not accept this License.  Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p> <ol> <li>Automatic Licensing of Downstream Recipients.</li> </ol> <p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License.  You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations.  If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License.  For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p> <ol> <li>Patents.</li> </ol> <p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based.  The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version.  For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement).  To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients.  \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License.  You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p> <ol> <li>No Surrender of Others' Freedom.</li> </ol> <p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License.  If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all.  For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p> <ol> <li>Use with the GNU Affero General Public License.</li> </ol> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work.  The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.</p> <ol> <li>Revised Versions of this License.</li> </ol> <p>The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time.  Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number.  If the Program specifies that a certain numbered version of the GNU General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation.  If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions.  However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p> <ol> <li>Disclaimer of Warranty.</li> </ol> <p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p> <ol> <li>Limitation of Liability.</li> </ol> <p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p> <ol> <li>Interpretation of Sections 15 and 16.</li> </ol> <p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <pre><code>                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\n</code></pre> <p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it  free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program.  It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:</p> <pre><code>&lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\nThis program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\nThis is free software, and you are welcome to redistribute it\nunder certain conditions; type `show c' for details.\n</code></pre> <p>The hypothetical commands <code>show w' and</code>show c' should show the appropriate parts of the General Public License.  Of course, your program's commands might be different; for a GUI interface, you would use an \"about box\".</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see https://www.gnu.org/licenses/.</p> <p>The GNU General Public License does not permit incorporating your program into proprietary programs.  If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library.  If this is what you want to do, use the GNU Lesser General Public License instead of this License.  But first, please read https://www.gnu.org/licenses/why-not-lgpl.html.</p>"},{"location":"models/","title":"Graph Neural Network models","text":"<p>In this section, we'll explore the architecture of the Graph Neural Network (GNN) models used in QuantumGravPy. These models are designed to learn meaningful representations from causal set graphs, which can then be used for various downstream tasks such as classification or regression.</p>"},{"location":"models/#structure","title":"Structure","text":"<p>The GNN models in QuantumGravPy follow a modular architecture that consists of three main components:</p> <ol> <li>Backbone: A sequence of GNN blocks that processes node features and graph connectivity to produce node embeddings.</li> <li>Optional addition: A network for processing additional graph-level features.</li> <li>Frontend: A linear block that takes the embeddings from the backbone (and optional graph features) and produces predictions for specific tasks. This can be replaced with another task specific frontend by overwriting the respective parts of <code>GNNModel</code> with your own logic. </li> </ol> <p>This modular design allows for flexibility in model configuration, making it easy to adapt the architecture to different types of causal set data and analysis tasks. The separation of the backbone from the frontend enables transfer learning approaches, where a pre-trained backbone can be reused for multiple different downstream tasks: </p> <pre><code>Input Graph (Node features + topology)\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Backbone   \u2502    \u2502  Graph Features \u2502\n\u2502  (GNN Blocks)\u2502    \u2502     Network     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                     \u2502\n       \u25bc                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            Concatenate             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               Frontend             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n                   \u25bc\n                 Output\n</code></pre>"},{"location":"models/#backbone","title":"Backbone","text":"<p>The backbone consists of a sequence of <code>GNNBlock</code> modules that transform the input node features through multiple graph convolutional layers. Each GNNBlock includes:</p> <ol> <li>A graph convolution layer (GCN, GraphConv, SageConv, etc.)</li> <li>A batch normalization layer</li> <li>An activation function</li> <li>A residual connection</li> <li>Dropout for regularization</li> </ol> <pre><code>    Input graph (Node features + topology)\n                \u2502              \u2502\n                \u25bc              \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n        \u2502   GNN layer   \u2502      \u2502\n        \u2502  (GCNConv,...)\u2502      \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n                \u2502              \u2502\n                \u25bc              \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n        \u2502   Normalize   \u2502      \u2502\n        \u2502 (BatchNorm..) \u2502      \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n                \u2502              \u2502\n                \u25bc              \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n        \u2502   Normalize   \u2502      \u2502\n        \u2502 (BatchNorm..) \u2502      \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n                \u2502              \u2502\n                \u25bc              \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n        \u2502   Activation   \u2502     \u2502\n        \u2502  (ReLu...)     \u2502     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n                \u2502              \u2502\n                \u25bc              \u25bc \n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \n             \u2502          +          \u2502  residual connection\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n                        \u2502\n                        \u25bc\n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \n             \u2502      Dropout        \u2502  only active during training    \n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n                        \u2502\n                        \u25bc\n                      Output\n</code></pre> <p>This implementation follows modern deep learning practices, with residual connections helping to mitigate the vanishing gradient problem and enabling the training of deeper networks. Each block is set up with a set of parameters:</p> <pre><code># Example GNNBlock configuration\n{\n    \"in_dim\": 64,\n    \"out_dim\": 64,\n    \"dropout\": 0.3,\n    \"gnn_layer_type\": \"GCNConv\",\n    \"normalizer\": \"BatchNorm\",\n    \"activation\": \"ReLU\", \n    \"gnn_layer_args\": [arg1, arg2, ...]\n    \"gnn_layer_kwargs\":{\n        key1: kwarg1, \n        key2: kwarg2,\n        key3: ...\n    }\n    \"norm_args\": [arg1, arg2, ...]\n    \"norm_kwargs\": {\n        key1: kwarg1, \n        key2: kwarg2,\n        key3: ...\n    }\n    \"activation_args\": [arg1, arg2, ...]\n    \"activation_kwargs\": {\n        key1: kwarg1, \n        key2: kwarg2,\n        key3: ...\n    }\n}\n</code></pre> <p>The backbone processes the node features and graph connectivity through these blocks sequentially, and the final node embeddings are pooled (using a configurable pooling operation such as mean, sum, or max pooling) to obtain a graph-level representation. It makes sense to familiarize yourself with the <code>pytorch_geometric</code> documentation if you don't have already, to learn how the GNN layers, activation functions etc work.</p> <p>The <code>gnn_layer_type</code>, <code>activation</code> and <code>normalizer</code> can be chosen from a set of predefined layers:  <pre><code>gnn_layers: dict[str, torch.nn.Module] = {\n    \"gcn\": tgnn.conv.GCNConv,\n    \"gat\": tgnn.conv.GATConv,\n    \"sage\": tgnn.conv.SAGEConv,\n    \"gco\": tgnn.conv.GraphConv,\n}\n\nnormalizer_layers: dict[str, torch.nn.Module] = {\n    \"identity\": torch.nn.Identity,\n    \"batch_norm\": torch.nn.BatchNorm1d,\n    \"layer_norm\": torch.nn.LayerNorm,\n}\n\nactivation_layers: dict[str, torch.nn.Module] = {\n    \"relu\": torch.nn.ReLU,\n    \"leaky_relu\": torch.nn.LeakyReLU,\n    \"sigmoid\": torch.nn.Sigmoid,\n    \"tanh\": torch.nn.Tanh,\n    \"identity\": torch.nn.Identity,\n}\n\n\npooling_layers: dict[str, torch.nn.Module] = {\n    \"mean\": tgnn.global_mean_pool,\n    \"max\": tgnn.global_max_pool,\n    \"sum\": tgnn.global_add_pool,\n}\n</code></pre> You can add new layers by using the supplied <code>register_</code> functions in the <code>utils</code> module. Consult the API documentation for more details.</p>"},{"location":"models/#optional-graph-level-features","title":"Optional graph-level features","text":"<p>In many real-world scenarios, we have additional graph-level features that can't be naturally represented as node features. The <code>GraphFeaturesBlock</code> allows for processing these auxiliary features and integrating them with the learned graph representation from the backbone.</p> <p>The <code>GraphFeaturesBlock</code> is a sequence of linear layers with activation functions that processes these global graph features. The outputs of the GNN backbone and the graph features network are concatenated before being passed to the frontend classifier. It's construction parameters are similar to the <code>GNNBlock</code>:</p> <pre><code># Example GraphFeaturesBlock configuration\n{\n    \"input_dim\": 10,\n    \"output_dim\": 32,\n    \"hidden_dims\": [64, 64],\n    \"activation\": \"ReLU\", \n    \"norm_args\": [arg1, arg2, ...]\n    \"norm_kwargs\": {\n        key1: kwarg1, \n        key2: kwarg2,\n        key3: ...\n    }\n    \"activation_args\": [arg1, arg2, ...]\n    \"activation_kwargs\": {\n        key1: kwarg1, \n        key2: kwarg2,\n        key3: ...\n    }\n}\n</code></pre> <p>This approach allows the model to incorporate both local (node-level) and global (graph-level) information when making predictions. By customizing the <code>hidden_dims</code> you can make the network shallower or deeper or wider or narrower.</p> <p>Edges features are currently not supported out of the box. </p>"},{"location":"models/#frontends","title":"Frontends","text":"<p>The frontend model takes the combined embeddings from the backbone and optional graph features network and produces predictions for specific tasks. The <code>ClassifierBlock</code> is based on the <code>LinearSequential</code> module, which supports multi-objective classification by allowing multiple output layers, each corresponding to a different task. It's construction parameters are once more similar to the <code>GNNBlock</code> and <code>GraphFeaturesBlock</code> models, e.g.: </p> <p><pre><code># Example ClassifierBlock configuration\n{\n    \"input_dim\": 96,  # Combined dimension from backbone and graph features\n    \"output_dims\": [2, 3],  # Two tasks: binary classification and 3-class classification\n    \"hidden_dims\": [128, 64],\n    \"activation\": \"ReLU\", \n    \"backbone_kwargs\": \n        - {\n            key1: kwarg1, \n            key2: kwarg2,\n            key3: ...\n        }\n        - {\n            key1: kwarg1, \n            key2: kwarg2,\n            key3: ...\n        }\n       - {\n            key1: kwarg1, \n            key2: kwarg2,\n            key3: ...\n        }\n    output_kwargs: \n        - {\n            key1: kwarg1, \n            key2: kwarg2,\n            key3: ...\n        }, \n        - {\n            key1: kwarg1, \n            key2: kwarg2,\n            key3: ...\n        }\n    \"activation_args\": [arg1, arg2, ...]\n    \"activation_kwargs\": {\n        key1: kwarg1, \n        key2: kwarg2,\n        key3: ...\n    }\n}\n</code></pre> By customizing the <code>hidden_dims</code> you can make the network shallower or deeper or wider or narrower.  In this way, transfer learning is supported - we can change the frontend depending on the task, while keeping the model that produces embeddings. The output layers are implemneted as one linear layer per task, and each of them can be given a list of kwargs. The same holds for the input- and hidden layers: for each of them, the <code>backbone_kwargs</code> config node can contain a set of kwargs, too. you can leave out some of them if desired, but for skipping one you need to put in an empty dictionary because the code maps kwargs to layers by index in the list. </p>"},{"location":"models/#configuration-driven-model-creation","title":"Configuration-driven model creation","text":"<p>All model components can be created from configuration dictionaries, making it easy to experiment with different architectures without changing the code. The <code>from_config</code> class methods in each component allow for declarative model definition through YAML configuration files.</p> <pre><code># Example full model configuration\n{\n    \"encoder\": [\n        {\n            \"in_dim\": 8,\n            \"out_dim\": 64,\n            \"gnn_layer_type\": \"GCNConv\",\n            \"normalizer\": \"BatchNorm\",\n            \"activation\": \"ReLU\"\n        },\n        {\n            \"in_dim\": 64,\n            \"out_dim\": 64,\n            \"gnn_layer_type\": \"GCNConv\",\n            \"normalizer\": \"BatchNorm\",\n            \"activation\": \"ReLU\"\n        }\n    ],\n    \"pooling_layer\": \"mean\",\n    \"graph_features_net\": {\n        \"input_dim\": 10,\n        \"output_dim\": 32,\n        \"hidden_dims\": [64, 64],\n        \"activation\": \"ReLU\"\n    },\n    \"classifier\": {\n        \"input_dim\": 96,  # 64 from backbone + 32 from graph features\n        \"output_dims\": [2],\n        \"hidden_dims\": [128, 64],\n        \"activation\": \"ReLU\"\n    }\n}\n</code></pre> <p>This configuration-based approach aligns with the overall philosophy of QuantumGravPy, which emphasizes a separation between configuration and code to facilitate rapid experimentation and reproducibility. Note how the number of blocks in the <code>encoder</code> part of the config determines the architecture of the backbone model - each block will create a <code>GNNBlock</code> instance, and the data is processed through them sequentially. In the same way, the number of <code>hidden_dims</code> will make the linear models deeper or shallower, and the supplied numbers determine the model width.  The output dim in <code>classifier</code> determines the number of classification tasks. </p> <p>Note that right now, only the <code>ClassifierBlock</code> is explicitly supported. </p>"},{"location":"models/#transfer-learning-and-model-reuse","title":"Transfer learning and model reuse","text":"<p>The separation of backbone and frontend enables effective transfer learning strategies. A backbone trained on a large dataset of causal sets can capture general properties of causal set graphs, which can then be reused for multiple downstream tasks by attaching different frontend classifiers. Currently, only the classifier block is supproted, but later on, we plan to generalize this such that the system can accomodate generative models as well. </p> <p>To extract the embeddings from a trained model for reuse, you can use the <code>get_embeddings</code> method of the <code>GNNModel</code> class, which returns the output of the backbone after pooling, ready to be used for other tasks.</p>"},{"location":"models/#full-config-example","title":"Full config example:","text":"<p>A full configuration that defines a model as it would appear in a YAML config file  could read: <pre><code>  encoder:\n    - in_dim: 12\n      out_dim: 128\n      dropout: 0.3\n      gnn_layer_type: \"sage\"\n      normalizer: \"batch_norm\"\n      activation: \"relu\"\n      norm_args: \n        - 128\n      gnn_layer_kwargs:\n        normalize: False\n        bias: True\n        project: False\n        root_weight: False\n        aggr: \"mean\"\n    - in_dim: 128\n      out_dim: 256\n      dropout: 0.3\n      gnn_layer_type: \"sage\"\n      normalizer: \"batch_norm\"\n      activation: \"relu\"\n      norm_args: \n        - 256\n      gnn_layer_kwargs:\n        normalize: False\n        bias: True\n        project: False\n        root_weight: False\n        aggr: \"mean\"\n    - in_dim: 256\n      out_dim: 128\n      dropout: 0.3\n      gnn_layer_type: \"sage\"\n      normalizer: \"batch_norm\"\n      activation: \"relu\"\n      norm_args: \n        - 128\n      gnn_layer_kwargs:\n        normalize: False\n        bias: True\n        project: False\n        root_weight: False\n        aggr: \"mean\"\n  pooling_layer: mean\n  classifier:\n    input_dim: 128\n    output_dims: \n      - 2\n    hidden_dims: \n      - 48\n      - 18\n    activation: \"relu\"\n    backbone_kwargs: [{}, {}]\n    output_kwargs: [{}]\n    activation_kwargs: [{ \"inplace\": False }]\n</code></pre></p> <p>Here, we use a chain of three <code>GraphSage</code>-based <code>GNNBlocks</code>, using batch normalization, a dropout probability of 0.3 and a <code>ReLU</code> activation function, followed by a global <code>mean</code> pooling layer and a classifer that has an input layer of size 128, two hidden layers of sizes 48 and 18, and an output layer of size 2, i.e., 2 classification tasks. We also make add some arguments for the constructor of the activation (<code>activation_kwargs</code>). The first <code>GNNBlock</code> must have an input dimension that corresponds to the dimensionality of the node features per node. We have no <code>GraphfeaturesBlock</code> in this case, but if we had it would look similar to the <code>classifier</code> block, the config node would just be called <code>graph_features_net</code> instead of <code>classifier</code>. </p> <p>Armed with this knowledge, we now can proceed to model training. </p>"},{"location":"training_a_model/","title":"Training a Model","text":"<p>After the data processing, we can set up the training process. This is done using the <code>Trainer</code> class.</p> <p>The <code>Trainer</code> class follows a pattern in which code and training parameters are separated: It expects a dictionary containing all the parameters, and a set of objects that take care of evaluation of the training process.</p> <p>The config <code>dict</code> allows us to store the parameters in an external file (YAML would be the preferable option) and read it in from there, such that we can have different configs for different runs that can be stored alongside the experiments. This is helpful for reproducibility of experiments.</p>"},{"location":"training_a_model/#the-trainer-class","title":"The Trainer class","text":"<p>Let's have a look at how the trainer class works first. It's constructor reads:</p> <p><pre><code>class Trainer:\n    \"\"\"Trainer class for training and evaluating GNN models.\"\"\"\n\n    def __init__(\n        self,\n        config: dict[str, Any],\n        # training and evaluation functions\n        criterion: Callable,\n        apply_model: Callable | None = None,\n        # training evaluation and reporting\n        early_stopping: Callable[[Collection[Any] | torch.Tensor], bool]\n        | None = None,\n        validator: DefaultValidator | None = None,\n        tester: DefaultTester | None = None,\n    )\n</code></pre> The <code>config</code> argument has its own section below. The <code>criterion</code> is a loss function that must have the following signature: <pre><code>criterion[[model_output, torch_geometric.data.Data], torch.Tensor]\n</code></pre> i.e., it allows for passing in arbitrary model outputs and the original data object (which contains the target for supervised learning for instance) and returns a <code>torch.Tensor</code> which contains a scalar. The second argument <code>data</code> can be ignored if it's not needed.</p> <p>The <code>apply_model</code> function is optional and defines how to call the model with the data. If you choose to use this class with your own model class, this might come in handy, especially if you change the signature of the model's <code>forward</code> method.</p>"},{"location":"training_a_model/#the-evaluators","title":"The evaluators","text":"<p>The next three arguments are needed to evaluate and test the model and to implement a stopping criterion, so they deserve their own little section.</p>"},{"location":"training_a_model/#tester-and-validator","title":"<code>tester</code> and <code>validator</code>","text":"<p>We will start from the end, beginning with <code>tester</code>. This is an object of type <code>DefaultTester</code> (an instance thereof or an object derived from it). This class is build like this:</p> <pre><code>class DefaultTester(DefaultEvaluator):\n    def __init__(\n        self, device, criterion: Callable, apply_model: Callable | None = None\n    )\n</code></pre> <p>i.e., it takes a loss function and an optional function to apply the model to data as well as the device to run on as constructor input. It then has a <code>test</code> function:</p> <pre><code>    def test(\n        self,\n        model: torch.nn.Module,\n        data_loader: torch_geometric.loader.DataLoader,\n    ):\n</code></pre> <p>which applies the model to the data in the passed <code>DataLoader</code> object using the <code>criterion</code> function. This will then be passed to a <code>report</code> function:</p> <pre><code>def report(self, data: list | pd.Series | torch.Tensor | np.ndarray)\n</code></pre> <p>This is a function or callable object that decides when to stop the training process based on a metric it gets. It eats a list or other iterable that contains the output fo applying the model to the data using <code>criterion</code>, and, by default, computes the mean and standard deviation thereof and reports them to standard out.</p> <p>The <code>validator</code> object works exactly the same, (<code>DefaultTester</code> and <code>DefaultValidator</code> have the same base class: <code>DefaultEvaluator</code>), so it's not documented separately here. The only difference is that what's called <code>test</code> in <code>DefaultTester</code> is called <code>validate</code> in <code>DefaultEvaluator</code>.</p> <p>Under the hood, they both use the <code>evalute</code> function of their parent class <code>DefaultEvaluator</code>. So in your derived classes, you can also overwrite that and then build your own type hierarcy on top of it.</p> <p>The idea behind these two classes is that the user derives their own class from them and adjusts the <code>test</code>, <code>validate</code> and <code>report</code> functions to their own needs, e.g., for reporting the F1 score in a classification task. Note that you can also break the type annotation if you think you need and for instance use your own evaluator classes - just make sure the call signatures are correct.</p>"},{"location":"training_a_model/#early_stopping","title":"<code>early_stopping</code>","text":"<p>This function is there to stop training once a certain condition has been reached. It eats the output of <code>Validator</code> and then computes a boolean that tells the <code>Trainer</code> object whether it should stop or continue training.</p> <p>For example, this can look like the following code block, where we use a moving average over the loss and a tolerance to determine if we should stop or not: Each epoch, the mean validation loss over the window is evaluated and compared to the current best one, and if it's above that we subtract 1 from the <code>patience</code> variable. if <code>patience</code> runs out, we stop training. When we find a better best mean loss, we reset patience and continue.</p> <p>Note that we are not using a function here, but a callable object. This is useful for cases where your <code>early stopping</code> logic has parameters that need to be held somewhere.</p> <p><pre><code>class EarlyStopping(DefaultEarlyStopping):\n\n    def __init__(\n        self, patience: int, delta: float = 1e-4, window=7, metric: str = \"loss\"\n    ):\n        super().__init__(patience, delta, window)\n\n        self.metric = metric\n        self.logger = logging.getLogger(\"QuantumGrav.EarlyStopping\")\n\n    def __call__(self, data: pd.DataFrame | list[dict[Any]]) -&gt; bool: # make it a callable object\n        if isinstance(data, pd.DataFrame) is False:\n            data = pd.DataFrame(data)\n\n        window = min(self.window, len(data))\n        smoothed = data[self.metric].rolling(window=window, min_periods=1).mean()\n        if smoothed.iloc[-1] &lt; self.best_score - self.delta:\n            self.best_score = smoothed.iloc[-1]\n            self.current_patience = self.patience\n        else:\n            self.current_patience -= 1\n        self.logger.info(\n            f\"EarlyStopping: current patience: {self.current_patience}, best score: {self.best_score}, smoothed metric: {smoothed.iloc[-1]}\"\n        )\n\n        return self.current_patience &lt;= 0\n</code></pre> This follows a similar principle to the other Evaluators: Derive from a common baseclass and overwrite the relevant methods with your own logic. In writing this, you need to make sure that the input of the <code>__call__</code> method must be compatible with the ouptut of the <code>validate</code> method of the <code>Validator</code> class.</p>"},{"location":"training_a_model/#the-configuration-dict","title":"The configuration <code>dict</code>","text":"<p>This provides all the necessary parameters for a training run, and as such has a fixed structure. This is best shown with an example: <pre><code>model:\n  name: \"QuantumGravTest\"\n  encoder:\n    - in_dim: 12\n      out_dim: 128\n      dropout: 0.3\n      gnn_layer_type: \"sage\"\n      normalizer: \"batch_norm\"\n      activation: \"relu\"\n      norm_args:\n        - 128\n      gnn_layer_kwargs:\n        normalize: False\n        bias: True\n        project: False\n        root_weight: False\n        aggr: \"mean\"\n    - in_dim: 128\n      out_dim: 256\n      dropout: 0.3\n      gnn_layer_type: \"sage\"\n      normalizer: \"batch_norm\"\n      activation: \"relu\"\n      norm_args:\n        - 256\n      gnn_layer_kwargs:\n        normalize: False\n        bias: True\n        project: False\n        root_weight: False\n        aggr: \"mean\"\n    - in_dim: 256\n      out_dim: 128\n      dropout: 0.3\n      gnn_layer_type: \"sage\"\n      normalizer: \"batch_norm\"\n      activation: \"relu\"\n      norm_args:\n        - 128\n      gnn_layer_kwargs:\n        normalize: False\n        bias: True\n        project: False\n        root_weight: False\n        aggr: \"mean\"\n  pooling_layer: mean\n  classifier:\n    input_dim: 128\n    output_dims:\n      - 2\n    hidden_dims:\n      - 48\n      - 18\n    activation: \"relu\"\n    backbone_kwargs: [{}, {}]\n    output_kwargs: [{}]\n    activation_kwargs: [{ \"inplace\": False }]\n\ntraining:\n  seed: 42\n  # training loop\n  device: \"cuda\"\n  early_stopping_patience: 5\n  early_stopping_window: 7\n  early_stopping_tol: 0.001\n  early_stopping_metric: \"f1_weighted\"\n  checkpoint_at: 2\n  checkpoint_path: /path/to/where/the/intermediate/models/should/go\n  # optimizer\n  learning_rate: 0.001\n  weight_decay: 0.0001\n  # training loader\n  batch_size: 32\n  num_workers: 12\n  pin_memory: False\n  drop_last: True\n  num_epochs: 200\n  split: 0.8\nvalidation: &amp;valtest\n  batch_size: 32\n  num_workers: 12\n  pin_memory: False\n  drop_last: True\n  shuffle: True\n  persistent_workers: True\n  split: 0.1\ntesting: *valtest\n</code></pre> A config must have the high-level nodes 'model', 'training', 'validation', and 'testing'. Data processing is currently not governed by a config because it's too specialized for the task at hand. All the nodes in <code>training</code> above are necessary and cannot be left out, because they are needed for the DataLoaders and the evaluation and training semantics.</p> <p>The <code>model</code> part defines the architecture of the model that's used, so please check the <code>Graph Neural Network models</code> section again for how that works.</p>"},{"location":"training_a_model/#train-a-model","title":"Train a model","text":"<p>The following is a complete end-to-end example for model training for a classification task. We are putting toghether the content from 'Using Datasets for data processing and batching' and from 'Training a model' and are overwriting the Evaluators to report F1 scores. Then, we set up the trainer class, prepare everything and run training. For completeness, we put everything into on file here, but it may be advisable to split your script into multiple files if you write as much code as here. Also, we might add several variants of evaluators as default in the future. To get a good idea of how the system works, please work through this example carefully and make sure you understand each step. For bugs or issues, please report them here.</p>"},{"location":"training_a_model/#full-code-example","title":"Full code example","text":"<pre><code>from pathlib import Path\nimport QuantumGrav as QG\n\nimport torch\nfrom torch_geometric.data import Data\nfrom torch_geometric.utils import dense_to_sparse\n\nimport zarr\nimport numpy as np\nimport shutil\nimport yaml\nfrom sklearn.metrics import f1_score\nimport pandas as pd\nimport pickle\nimport logging\n\n# set up logger\nlogging.basicConfig(\n    level=logging.INFO,  # or logging.INFO if you want less verbosity\n    format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\",\n)\n\n\n################################################################################\n# data processing\n# Find all .h5 files in the given directory and its subdirectories\ndef find_files(directory: Path, file_list: list[Path] = []):\n    for path in directory.iterdir():\n        if path.is_dir():\n            find_files(path, file_list)\n        else:\n            if path.suffix == \".h5\" and \"backup\" not in path.name:\n                file_list.append(path)\n\n\n# load data from zarr file and put it into a dictionary for further processing\n# This is a custom reader function for the QGDataset\ndef load_data(\n    file: zarr.Group,\n    idx: int,\n    float_dtype: torch.dtype,\n    int_dtype: torch.dtype,\n    validate_data: bool,\n) -&gt; dict:\n\n    # get the hdf5 group to load data from. We assume there's a group 'adjacency_matrix' because all data is derived from the adjacency matrix of the causal set\n    group = file[\"adjacency_matrix\"]\n\n    data = dict()\n\n    # Load edge index and edge weight\n    cset_size = torch.tensor(group[\"cset_size\"][idx]).to(int_dtype)\n    adj_raw = group[\"adjacency_matrix\"][idx, :, :]\n\n    data[\"adjacency_matrix\"] = adj_raw\n    data[\"cset_size\"] = cset_size\n\n    # target\n    manifold_like = group[\"manifold_like\"][idx]\n\n    # features\n    data[\"manifold_like\"] = manifold_like\n    feature_names = [\n        \"max_pathlens_future\",\n        \"max_pathlens_past\",\n        \"in_degree\",\n        \"out_degree\",\n    ]\n    data[\"feature_names\"] = feature_names\n\n    for feature_name in feature_names:\n        # all features are vectors of size cset_size\n        data[feature_name] = group[feature_name][idx, :][0:cset_size]\n\n        # make features into correct shape and replace NaNs\n        values = torch.tensor(\n            group[feature_name][idx, :][0:cset_size], dtype=float_dtype\n        ).unsqueeze(1)\n        nan_found = torch.isnan(values)\n\n        # replace NaNs with 0.0\n        if nan_found.any():\n            values = torch.nan_to_num(values, nan=0.0)\n        data[feature_name] = values\n\n    # return dictionary of raw data instead of full data object --&gt; better separation of concerns here\n    return data\n\n# function for turning the raw data into a Data object that will be saved on disk\ndef pre_transform(data: dict) -&gt; Data:\n    \"\"\"Pre-transform the data dictionary into a PyG Data object.\"\"\"\n    adjacency_matrix = data[\"adjacency_matrix\"]\n    cset_size = data[\"cset_size\"]\n\n    # this is a workaround for the fact that the adjacency matrix is stored in a transposed form when going from julia to hdf5\n    adjacency_matrix = np.transpose(adjacency_matrix)\n    adjacency_matrix = adjacency_matrix[0:cset_size, 0:cset_size]\n    edge_index, edge_weight = dense_to_sparse(\n        torch.tensor(adjacency_matrix, dtype=torch.float32)\n    )\n\n    # make node features\n    node_features = []\n    for feature_name in data[\"feature_names\"]:\n        node_features.append(data[feature_name])\n    x = torch.cat(node_features, dim=1).to(torch.float32)\n\n    # make targets\n    y = torch.tensor(data[\"manifold_like\"]).to(torch.long)\n\n    # make data object\n    tgdata = Data(\n        x=x,\n        edge_index=edge_index,\n        edge_attr=edge_weight,\n        y=y,\n    )\n\n    if not tgdata.validate():\n        raise ValueError(f\"Data validation failed for index {idx}.\")\n    return tgdata\n\n\n################################################################################\n# Testing and Validation helper classes.\nclass Validator(QG.DefaultValidator):\n    def __init__(\n        self,\n        device,\n        criterion,\n        apply_model=None,\n        prefix: str = \"\",\n    ):\n        super().__init__(device, criterion, apply_model)\n        self.prefix = prefix\n        self.data = pd.DataFrame(\n            columns=[\n                \"avg_loss\",\n                \"std_loss\",\n                \"f1_per_class\",\n                \"f1_unweighted\",\n                \"f1_weighted\",\n            ],\n        )\n\n    # overwrite the default 'evaluate' function\n    def evaluate(self, model, data_loader):\n        model.eval()\n        current_data = pd.DataFrame(\n            np.nan,\n            columns=[\"loss\", \"output\", \"target\"],\n            index=pd.RangeIndex(\n                start=0, stop=len(data_loader) * data_loader.batch_size, step=1\n            ),\n        )\n        start = 0\n        stop = 0\n\n        # evaluate the model and produce data we need\n        with torch.no_grad():\n            for i, batch in enumerate(data_loader):\n                data = batch.to(self.device)\n                if self.apply_model:\n                    outputs = self.apply_model(model, data)\n                else:\n                    outputs = model(data.x, data.edge_index, data.batch)\n                loss = self.criterion(outputs, data)\n                manifold_like = outputs[0].argmax(dim=1).cpu().numpy()\n\n                if loss.isnan().any():\n                    print(f\"NaN loss encountered in batch {i}.\")\n                    continue\n\n                if np.isnan(manifold_like).any():\n                    print(f\"NaN manifold_like encountered in batch {i}.\")\n                    continue\n\n                if torch.isnan(data.y).any():\n                    print(f\"NaN target encountered in batch {i}.\")\n                    continue\n                stop = start + data.num_graphs\n\n                current_data.iloc[start:stop, current_data.columns.get_loc(\"loss\")] = (\n                    loss.item()\n                )\n\n                current_data.iloc[\n                    start:stop, current_data.columns.get_loc(\"output\")\n                ] = manifold_like\n\n                current_data.iloc[\n                    start:stop, current_data.columns.get_loc(\"target\")\n                ] = data.y.cpu().numpy()\n                start = stop\n\n        return current_data\n\n    def report(self, data):\n        # compute avg loss and F1 score and report via print\n        per_class = f1_score(data[\"output\"], data[\"target\"], average=None)\n        unweighted = f1_score(data[\"output\"], data[\"target\"], average=\"macro\")\n        weighted = f1_score(data[\"output\"], data[\"target\"], average=\"weighted\")\n        avg_loss = data[\"loss\"].mean()\n        std_loss = data[\"loss\"].std()\n        self.logger.info(f\"{self.prefix} avg loss: {avg_loss:.4f} +/- {std_loss:.4f}\")\n        self.logger.info(f\"{self.prefix} f1 score per class: {per_class}\")\n        self.logger.info(f\"{self.prefix} f1 score unweighted: {unweighted}\")\n        self.logger.info(f\"{self.prefix} f1 score weighted: {weighted}\")\n\n        self.data.loc[len(self.data)] = [\n            avg_loss,\n            std_loss,\n            per_class,\n            unweighted,\n            weighted,\n        ]\n\n    def validate(self, model, data_loader):\n        return self.evaluate(model, data_loader)\n\nclass Tester(Validator): # this still works, even though it breaks the type annotation for `tester`.\n    def __init__(\n        self,\n        device,\n        criterion,\n        apply_model=None,\n    ):\n        super().__init__(device, criterion, apply_model, prefix=\"Testing\")\n\n    def test(self, model, data_loader):\n        return super().evaluate(model, data_loader)\n\n\n################################################################################\n# functions for the training loop\n# loss function --&gt; 'criterion'\ndef compute_loss(x: torch.Tensor, data: Data) -&gt; torch.Tensor:\n    loss = torch.nn.CrossEntropyLoss()(\n        x[0], data.y\n    )  # one task -&gt; use x[0] for the output\n\n    if loss.isnan().any():\n        raise ValueError(f\"Loss contains NaN values. {x[0]} {data.y}\")\n\n    return loss\n\n################################################################################\n# early stopping class. this checks a validation metric and stops training if it doesn\u00b4t improve anymore over a set window. You can set this up for looking at the F1 score too for instance by changing the value for 'metric'. In this case, we are using a dataframe to collect the validation output.\nclass EarlyStopping(DefaultEarlyStopping):\n\n    def __init__(\n        self, patience: int, delta: float = 1e-4, window=7, metric: str = \"loss\"\n    ):\n        super().__init__(patience, delta, window)\n\n        self.metric = metric\n        self.logger = logging.getLogger(\"QuantumGrav.EarlyStopping\")\n\n    # we are using a dataframe. this works because the Validator class creates one.\n    def __call__(self, data: pd.DataFrame | list[dict[Any, Any]]) -&gt; bool: # make it a callable object\n        window = min(self.window, len(data))\n        smoothed = data[self.metric].rolling(window=window, min_periods=1).mean()\n\n        if isinstance(data, pd.DataFrame) is False:\n            data = pd.DataFrame(data)\n\n        if smoothed.iloc[-1] &lt; self.best_score - self.delta:\n            self.best_score = smoothed.iloc[-1]\n            self.current_patience = self.patience\n        else:\n            self.current_patience -= 1\n        self.logger.info(\n            f\"EarlyStopping: current patience: {self.current_patience}, best score: {self.best_score}, smoothed metric: {smoothed.iloc[-1]}\"\n        )\n\n        return self.current_patience &lt;= 0\n\n# apply model function if necessary. Here, this is not needed, because we have no edge features as such\n\n\n################################################################################\n# main function putting everything together\ndef main(path_to_data: str | Path | None, path_to_config: str | Path | None):\n    logger = logging.getLogger(\"QuantumGrav\")\n\n    if path_to_data is None:\n        raise ValueError(\"Path to data must be provided.\")\n\n    if path_to_config is not None:\n        run_training = True\n        with open(path_to_config, \"r\") as f:\n            config = yaml.safe_load(f)\n    else:\n        config = None\n        run_training = False\n        logger.info(\"No config provided, only processing data\")\n\n    h5files = []\n    find_files(Path(path_to_data), h5files)\n    logger.info(f\"Found {len(h5files)} files.\")\n\n    # augment files if necessary with 'num_causal_sets' dataset\n    for file in h5files:\n        with h5py.File(file, \"r+\") as f:\n            if \"num_causal_sets\" not in f:\n                logger.info(\"Adding num_causal_sets to file.\")\n                if (Path(file).parent / \"backup.h5\").exists() is False:\n                    shutil.copy(file, Path(file).parent / \"backup.h5\")\n                f[\"num_causal_sets\"] = f[\"adjacency_matrix\"][\"adjacency_matrix\"].shape[\n                    0\n                ]\n\n    # create a dataset\n    dataset = QG.QGDataset(\n        input=h5files,\n        output=path_to_data,\n        reader=load_data,\n        float_type=torch.float32,\n        int_type=torch.int64,\n        validate_data=True,\n        n_processes=6,\n        chunksize=200,\n        transform=None,\n        pre_transform=pre_transform,\n        pre_filter=None,\n    )\n\n\n    if run_training and config is not None:\n        logger.info(f\"Running training with config: {path_to_config}\")\n\n        # instantiate all the evaluators\n        validator = Validator(\n            device=torch.device(config[\"training\"][\"device\"]),\n            criterion=compute_loss,\n            apply_model=None,  # No need for apply_model in this case\n        )\n        tester = Tester(\n            device=torch.device(config[\"training\"][\"device\"]),\n            criterion=compute_loss,\n            apply_model=None,  # No need for apply_model in this case\n        )\n\n        early_stopping = EarlyStopping(\n            patience=config[\"training\"][\"early_stopping_patience\"],\n            delta=config[\"training\"].get(\"early_stopping_tol\", 1e-4),\n            window=config[\"training\"].get(\"early_stopping_window\", 7),\n            metric=config[\"training\"].get(\"early_stopping_metric\", \"f1_weighted\"),\n        )\n\n        # set up trainer class\n        logger.info(f\"Using device: {config['training']['device']}\")\n        logger.info(\"Building trainer\")\n        trainer = QG.Trainer(\n            config,\n            compute_loss,\n            validator=validator,\n            tester=tester,\n            early_stopping=early_stopping,\n            apply_model=None,  # No need for apply_model in this case\n        )\n\n        # prepare data loaders\n        train_split = config[\"training\"].get(\"split\", 0.8)\n        test_split = config[\"testing\"].get(\"split\", 0.1)\n        val_split = config[\"validation\"].get(\"split\", 0.1)\n\n        train_loader, val_loader, test_loader = trainer.prepare_dataloaders(\n            dataset, split=[train_split, val_split, test_split]\n        )\n\n        # initialize the model and optimizer\n        trainer.initialize_model()\n\n        trainer.initialize_optimizer()\n\n        # run training\n        training_result, validation_result = trainer.run_training(\n            train_loader=train_loader,\n            val_loader=val_loader,\n        )\n\n        # test the model and return the result\n        test_result = trainer.run_test(test_loader=test_loader)\n        return training_result, validation_result, test_result\n    else:\n        return dataset\n\n\nif __name__ == \"__main__\":\n\n    path_to_data = Path(\n        \"path/to/hdf5_datafiles\n    )\n    # load config from file\n    path_to_config = Path(\n        \"path/to/training_config.yaml\"\n    )\n    # execute training and save result data\n    train_data, valid_data, test_data = main(path_to_data, path_to_config)\n\n    # save result data\n    with open(path_to_data / \"train_data.pkl\", \"wb\") as f:\n        pickle.dump(train_data, f)\n\n    with open(path_to_data / \"valid_data.pkl\", \"wb\") as f:\n        pickle.dump(valid_data, f)\n\n    with open(path_to_data / \"test_data.pkl\", \"wb\") as f:\n        pickle.dump(test_data, f)\n</code></pre>"}]}