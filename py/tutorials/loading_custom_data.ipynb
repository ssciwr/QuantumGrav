{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Creating graph datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## creating In Memory datasets\n",
    "\n",
    "we need 4 fundamental methods: \n",
    "- raw_file_names() -> list of file names in `raw_dir` for raw data used to skip the download \n",
    "- processed_file_names -> list of file name sin `processed_dir` to skip the processing \n",
    "- download() downloads raw data into raw_dir() --> donÂ´t implement if no download necessary \n",
    "- process() process raw data and save it into processed dir "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "the 'process' method is the most important one. this creates a list of 'Data' objects that are saved into 'processed_dir' then. Data objects will be collated into one giant `Data` object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch_geometric.data import InMemoryDataset, download_url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyOwnDataset(InMemoryDataset): \n",
    "    def __init__(self, root, transform=None, pre_transform= None, pre_filter = None): \n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self): \n",
    "        return ['data1.pt', 'data2.pt']\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self): \n",
    "        return ['data.pt']\n",
    "    \n",
    "    def download(self):\n",
    "        url = 'https://example.com/data.zip'\n",
    "        download_url(url, self.raw_dir)\n",
    "\n",
    "    def process(self): \n",
    "        data_list = [...]\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [d for d in data_list if self.pre_filter(d)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(d) for d in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "in my case, I would need to use hdf5 or similar. Zarr could work, but Arrow does not, it's ill suited for this purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Creating 'larger' Datasets\n",
    "\n",
    "if stuff does not fit into memory, we can use the `Dataset` class. This follows closely the concept of the torchvision datasets. It expects the methods len() and get() to be implemented. get() implements the logic to get a single graph, len() gets the number of examples in the dataset. Works in much the same way as the Julia datasets we already have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp \n",
    "from torch_geometric.data import Dataset \n",
    "\n",
    "class MyOwnOnDiskDataset(Dataset): \n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None): \n",
    "        pass \n",
    "    \n",
    "    @property \n",
    "    def raw_file_names(self): \n",
    "        return ['data1.pt', 'data2.pt']\n",
    "    \n",
    "    @property \n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "    \n",
    "    def download(self):\n",
    "        url = 'https://example.com/data.zip'\n",
    "        download_url(url, self.raw_dir)\n",
    "\n",
    "    def process(self): \n",
    "        idx = 0\n",
    "        for raw_path in self.raw_paths:\n",
    "            # Read data from `raw_path\n",
    "            data = Data(...) # this is where the data loading happens and where the performance bottlenecks will be\n",
    "\n",
    "            if self.pre_filter is not None and not self.pre_filter(data): \n",
    "                continue \n",
    "\n",
    "            if self.pre_transform is not None:\n",
    "                data = self.pre_transform(data)\n",
    "\n",
    "            torch.save(data, osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "            idx += 1\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        data =torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Here, each graph data object gets saved individually in process(), and is manually loaded in get(). We might want to cache some for ease of use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Use HDF5 or Zarr if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Loading Graphs from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import download_url, extract_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_zip(download_url(url, '.'), './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_path = './ml-latest-small/movies.csv'\n",
    "rating_path = './ml-latest-small/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "display(pd.read_csv(movie_path).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.read_csv(rating_path).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_node_csv(path, index_col=None, encoders=None, **kwargs): \n",
    "    df = pd.read_csv(path, index_col = index_col, **kwargs)\n",
    "    mapping = {index: i for i ,index in enumerate(df.index.unique())}\n",
    "\n",
    "    x = None \n",
    "\n",
    "    if encoders is None:\n",
    "        encoders = {col: lambda x: x for col in df.columns if col != index_col}\n",
    "    else: \n",
    "        xs = [encoder(df[col]) for col, encoder in encoders.items()]\n",
    "        x = torch.cat(xs, dim=1)\n",
    "    \n",
    "    return x, mapping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "build a bunch of encoders here. this is useful in general, but might become complex code wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceEncoder: \n",
    "    def __init__(self, model_name = 'all-MiniLM-L6-v2', device=None): \n",
    "        self.device = device \n",
    "        self.model = st.SentenceTransformer(model_name, device=device)\n",
    "        \n",
    "\n",
    "    @torch.no_grad() \n",
    "    def __call__(self, df): \n",
    "        x = self.model.encode(df.values, show_progress_bar=True, convert_to_tensor=True, device=self.device)\n",
    "        return x.cpu() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreEncoder: \n",
    "    def __init__(self, sep='|'): \n",
    "        self.sep = sep \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, df): \n",
    "        genres = set(g for col in df.values for g in col.split(self.sep))\n",
    "        mapping = {genre: i for i, genre in enumerate(genres)}\n",
    "\n",
    "        x = torch.zeros(len(df), len(mapping))\n",
    "\n",
    "        for i, col in enumerate(df.values): \n",
    "            for genre in col.split(self.sep): \n",
    "                x[i, mapping[genre]] = 1\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_x, movie_mapping = load_node_csv(\n",
    "    movie_path, index_col='movieId', encoders={\n",
    "        'title': SequenceEncoder(),\n",
    "        'genres': GenreEncoder()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, user_mapping = load_node_csv(rating_path, index_col='userId')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "build the heterodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData() \n",
    "\n",
    "data['user'].num_nodes = len(user_mapping)\n",
    "data['movie'].x = movie_x \n",
    "\n",
    "\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "connect the users to movies according to their ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_edge_csv(path, src_index_col, src_mapping, dst_index_col, dst_mapping,\n",
    "                  encoders=None, **kwargs):\n",
    "    df = pd.read_csv(path, **kwargs)\n",
    "\n",
    "    src = [src_mapping[index] for index in df[src_index_col]]\n",
    "    dst = [dst_mapping[index] for index in df[dst_index_col]]\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "\n",
    "    edge_attrs = None\n",
    "    if encoders is None:\n",
    "        encoders = {col: lambda x: x for col in df.columns if col not in [src_index_col, dst_index_col]}\n",
    "    else:\n",
    "        edge_attrs = [encoder(df[col]) for col, encoder in encoders.items()]\n",
    "        edge_attr = torch.cat(edge_attrs, dim=1)\n",
    "    \n",
    "    return edge_index, edge_attr\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "load additional edge leve features via encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityEncoder: \n",
    "\n",
    "    def __init__(self, dtype=torch.float): \n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __call__(self, df): \n",
    "        return torch.from_numpy(df.values).view(-1, 1).to(dtype=self.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, edge_label = load_edge_csv(\n",
    "    rating_path,\n",
    "    src_index_col='userId',\n",
    "    src_mapping=user_mapping,\n",
    "    dst_index_col='movieId',\n",
    "    dst_mapping=movie_mapping,\n",
    "    encoders={'rating': IdentityEncoder(dtype=torch.long)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['user', 'rates', 'movie'].edge_index = edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['user', 'rates', 'movie'].edge_label = edge_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
