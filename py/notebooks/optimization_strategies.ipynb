{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing Dataset Processing\n",
    "\n",
    "Let's discuss strategies to optimize and parallelize the dataset processing:\n",
    "\n",
    "1. **Multiprocessing** to parallelize file processing\n",
    "2. **Memory-mapped files** to reduce memory usage\n",
    "3. **Efficient serialization** to speed up I/O operations\n",
    "4. **Chunking** to process data in parallel batches\n",
    "5. **Alternative storage formats** for PyTorch Geometric Data objects\n",
    "import concurrent.futures\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "```python\n",
    "def process_single_graph(args):\n",
    "    \"\"\"Process a single graph from an HDF5 file\n",
    "    \n",
    "    Args:\n",
    "        args: Tuple containing (file_path, idx, output_dir, validate_data, pre_filter, pre_transform)\n",
    "        \n",
    "    Returns:\n",
    "        int: The index of the processed graph\n",
    "    \"\"\"\n",
    "    file_path, idx, output_dir, validate_data, pre_filter, pre_transform = args\n",
    "    \n",
    "    with h5py.File(file_path, \"r\") as f:\n",
    "        data = load_graph(\n",
    "            f, \n",
    "            idx,\n",
    "            float_dtype=torch.float32,\n",
    "            int_dtype=torch.int64,\n",
    "            validate=validate_data\n",
    "        )\n",
    "    \n",
    "    if pre_filter is not None and not pre_filter(data):\n",
    "        return idx, False  # Skip this data point\n",
    "        \n",
    "    if pre_transform is not None:\n",
    "        data = pre_transform(data)\n",
    "        \n",
    "    # Save to disk\n",
    "    torch.save(data, os.path.join(output_dir, f\"data_{idx}.pt\"))\n",
    "    return idx, True\n",
    "\n",
    "class OptimizedCsDataset(CsDataset):\n",
    "    def process_parallel(self, max_workers=None, chunk_size=100):\n",
    "        \"\"\"Process the dataset in parallel using multiprocessing.\n",
    "        \n",
    "        Args:\n",
    "            max_workers: Maximum number of worker processes\n",
    "            chunk_size: Number of graphs to process in each worker\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Convert NumPy arrays to Python lists for JSON serialization\n",
    "        d = {\n",
    "            \"manifold_codes\": [v.item() for v in self.manifold_codes],\n",
    "            \"manifolds\": [str(m) for m in self.manifold_names],\n",
    "            \"boundaries\": [str(m) for m in self.boundaries],\n",
    "            \"boundary_codes\": [v.item() for v in self.boundary_codes],\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(self.processed_dir, \"metadata.json\"), \"w\") as f:\n",
    "            json.dump(d, f)\n",
    "            \n",
    "        # Calculate optimal number of workers if not specified\n",
    "        if max_workers is None:\n",
    "            max_workers = min(os.cpu_count(), 16)  # Use at most 16 workers or CPU count\n",
    "            \n",
    "        print(f\"Processing dataset using {max_workers} workers\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        total_processed = 0\n",
    "        \n",
    "        # Process each file\n",
    "        for file_idx, file in enumerate(self.raw_paths):\n",
    "            if not os.path.exists(file):\n",
    "                raise FileNotFoundError(f\"Input file {file} does not exist.\")\n",
    "                \n",
    "            with h5py.File(file, \"r\") as f:\n",
    "                num_graphs = f[\"num_causal_sets\"][()]\n",
    "                print(f\"Processing file {file_idx+1}/{len(self.raw_paths)}: {file} with {num_graphs} graphs\")\n",
    "                \n",
    "                # Create argument list for all graphs in this file\n",
    "                args_list = [\n",
    "                    (file, idx, self.processed_dir, self.validate_data, self.pre_filter, self.pre_transform)\n",
    "                    for idx in range(num_graphs)\n",
    "                ]\n",
    "                \n",
    "                # Process in parallel using chunks\n",
    "                with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "                    # Submit tasks in chunks to reduce overhead\n",
    "                    future_to_idx = {}\n",
    "                    for i in range(0, len(args_list), chunk_size):\n",
    "                        chunk_args = args_list[i:i + chunk_size]\n",
    "                        future_batch = executor.map(process_single_graph, chunk_args)\n",
    "                        future_to_idx[i] = future_batch\n",
    "                    \n",
    "                    # Process results with progress bar\n",
    "                    with tqdm(total=num_graphs) as pbar:\n",
    "                        for start_idx, future_batch in future_to_idx.items():\n",
    "                            for idx, success in future_batch:\n",
    "                                if success:\n",
    "                                    total_processed += 1\n",
    "                                pbar.update(1)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Processed {total_processed} graphs in {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"Average processing time: {(end_time - start_time) / total_processed:.4f} seconds per graph\")\n",
    "    ```\n",
    "\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "import torch.utils.data\n",
    "import tempfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "class MemoryEfficientStorage:\n",
    "    \"\"\"A class to handle memory-efficient storage of PyTorch Geometric Data objects\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_data_efficient(data, file_path):\n",
    "        \"\"\"Save a PyTorch Geometric Data object efficiently.\n",
    "        \n",
    "        This method:\n",
    "        1. Converts sparse tensors to COO format\n",
    "        2. Uses memory-mapped files for large tensors\n",
    "        3. Compresses the file\n",
    "        \n",
    "        Args:\n",
    "            data: PyTorch Geometric Data object\n",
    "            file_path: Path to save the data\n",
    "        \"\"\"\n",
    "        # Create a dictionary to store serialized attributes\n",
    "        serialized = {}\n",
    "        \n",
    "        # Process each attribute of the data object\n",
    "        for key, value in data:\n",
    "            if torch.is_tensor(value):\n",
    "                # For large tensors, use memory mapping\n",
    "                if value.numel() > 1000000:  # Threshold for large tensors\n",
    "                    # Use .npy format for large tensors\n",
    "                    np_path = f\"{file_path}.{key}.npy\"\n",
    "                    np_array = value.detach().cpu().numpy()\n",
    "                    np.save(np_path, np_array)\n",
    "                    serialized[key] = {\n",
    "                        'type': 'numpy_memmap',\n",
    "                        'path': np_path,\n",
    "                        'shape': value.shape,\n",
    "                        'dtype': str(value.dtype)\n",
    "                    }\n",
    "                else:\n",
    "                    # Use standard serialization for smaller tensors\n",
    "                    serialized[key] = {\n",
    "                        'type': 'tensor',\n",
    "                        'data': value.detach().cpu()\n",
    "                    }\n",
    "            elif isinstance(value, SparseTensor):\n",
    "                # Handle sparse tensors\n",
    "                row, col, values = value.coo()\n",
    "                serialized[key] = {\n",
    "                    'type': 'sparse_tensor',\n",
    "                    'row': row.detach().cpu(),\n",
    "                    'col': col.detach().cpu(),\n",
    "                    'values': values.detach().cpu() if values is not None else None,\n",
    "                    'size': value.size()\n",
    "                }\n",
    "            else:\n",
    "                # For other data types, use pickle directly\n",
    "                serialized[key] = {\n",
    "                    'type': 'pickle',\n",
    "                    'data': value\n",
    "                }\n",
    "                \n",
    "        # Save the serialized data\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(serialized, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_data_efficient(file_path):\n",
    "        \"\"\"Load a PyTorch Geometric Data object efficiently.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the saved data\n",
    "            \n",
    "        Returns:\n",
    "            PyTorch Geometric Data object\n",
    "        \"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            serialized = pickle.load(f)\n",
    "            \n",
    "        # Create a new Data object\n",
    "        data = Data()\n",
    "        \n",
    "        # Process each serialized attribute\n",
    "        for key, value in serialized.items():\n",
    "            if value['type'] == 'numpy_memmap':\n",
    "                # Load memory-mapped tensor\n",
    "                np_array = np.load(value['path'], mmap_mode='r')\n",
    "                tensor = torch.from_numpy(np_array).clone()  # Clone to avoid issues with mmap\n",
    "                setattr(data, key, tensor)\n",
    "            elif value['type'] == 'tensor':\n",
    "                # Load standard tensor\n",
    "                setattr(data, key, value['data'])\n",
    "            elif value['type'] == 'sparse_tensor':\n",
    "                # Recreate sparse tensor\n",
    "                sparse = SparseTensor(\n",
    "                    row=value['row'],\n",
    "                    col=value['col'],\n",
    "                    value=value['values'],\n",
    "                    sparse_sizes=value['size']\n",
    "                )\n",
    "                setattr(data, key, sparse)\n",
    "            elif value['type'] == 'pickle':\n",
    "                # Load pickled data\n",
    "                setattr(data, key, value['data'])\n",
    "                \n",
    "        return data\n",
    "\n",
    "class EfficientCsDataset(OptimizedCsDataset):\n",
    "    \"\"\"A memory-efficient version of the CsDataset class\"\"\"\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"Process the dataset using memory-efficient storage\"\"\"\n",
    "        # Convert NumPy arrays to Python lists for JSON serialization\n",
    "        d = {\n",
    "            \"manifold_codes\": [v.item() for v in self.manifold_codes],\n",
    "            \"manifolds\": [str(m) for m in self.manifold_names],\n",
    "            \"boundaries\": [str(m) for m in self.boundaries],\n",
    "            \"boundary_codes\": [v.item() for v in self.boundary_codes],\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(self.processed_dir, \"metadata.json\"), \"w\") as f:\n",
    "            json.dump(d, f)\n",
    "\n",
    "        for file in self.raw_paths:\n",
    "            if not os.path.exists(file):\n",
    "                raise FileNotFoundError(f\"Input file {file} does not exist.\")\n",
    "            with h5py.File(file, \"r\") as f:\n",
    "                print(f\"Processing file: {file}\")\n",
    "                print(f\"Number of causal sets: {f['num_causal_sets'][()]}\")\n",
    "                for idx in tqdm(range(f[\"num_causal_sets\"][()])):\n",
    "                    data = load_graph(\n",
    "                        f,\n",
    "                        idx,\n",
    "                        float_dtype=torch.float32,\n",
    "                        int_dtype=torch.int64,\n",
    "                        validate=self.validate_data,\n",
    "                    )\n",
    "                    if self.pre_filter is not None:\n",
    "                        if not self.pre_filter(data):\n",
    "                            continue\n",
    "                    if self.pre_transform is not None:\n",
    "                        data = self.pre_transform(data)\n",
    "                    \n",
    "                    # Use efficient storage\n",
    "                    output_path = os.path.join(self.processed_dir, f\"data_{idx}.pt\")\n",
    "                    MemoryEfficientStorage.save_data_efficient(data, output_path)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\"Get a data object using memory-efficient loading\"\"\"\n",
    "        data_path = os.path.join(self.processed_dir, f\"data_{idx}.pt\")\n",
    "        data = MemoryEfficientStorage.load_data_efficient(data_path)\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        return data\n",
    "```\n",
    "# Storage Formats for PyTorch Geometric Data Objects\n",
    "\n",
    "When handling large graph datasets, choosing the right storage format is crucial. Here are some options:\n",
    "\n",
    "## 1. PyTorch's Native Format (.pt)\n",
    "- **Pros**: Direct compatibility with PyTorch, fast loading in PyTorch\n",
    "- **Cons**: Files can be large, not human-readable, versioning issues between PyTorch versions\n",
    "\n",
    "## 2. Memory-Mapped Files (with .npy for large tensors)\n",
    "- **Pros**: Reduced memory usage, allows working with datasets that don't fit in RAM\n",
    "- **Cons**: Slightly slower access, more complex implementation\n",
    "\n",
    "## 3. Compressed Formats\n",
    "- **Pros**: Smaller file size, good for storage and transfer\n",
    "- **Cons**: Additional compression/decompression overhead\n",
    "\n",
    "## 4. HDF5 Format (.h5)\n",
    "- **Pros**: Good for hierarchical data, partial loading, good compression\n",
    "- **Cons**: More complex API, potential compatibility issues\n",
    "\n",
    "## 5. LMDB (Lightning Memory-Mapped Database)\n",
    "- **Pros**: Great for large datasets, fast random access, transactional\n",
    "- **Cons**: More setup required, less intuitive API\n",
    "\n",
    "Our `MemoryEfficientStorage` class implements a hybrid approach:\n",
    "- Small tensors: Stored directly in PyTorch format\n",
    "- Large tensors: Stored as memory-mapped NumPy arrays\n",
    "- Metadata: Handled via Python's pickle format\n",
    "\n",
    "This balances memory efficiency and performance for large graph datasets.\n",
    "```python\n",
    "# Example of using the optimized dataset classes\n",
    "\n",
    "# Set up paths\n",
    "datapath = os.path.join(\"/mnt\", \"dataLinux\", \"machinelearning_data\", \"QuantumGrav/causal_sets\")\n",
    "files = [\n",
    "    os.path.join(datapath, \"cset_data_min=300_max=650_N=25000_d=2.h5\"),\n",
    "    os.path.join(datapath, \"cset_data_min=300_max=650_N=25000_d=3.h5\"),\n",
    "    os.path.join(datapath, \"cset_data_min=300_max=650_N=25000_d=4.h5\"),\n",
    "]\n",
    "\n",
    "# Define transforms\n",
    "onehot_transform = OneHotEncodeTargets(manifold_classes=6, boundary_classes=3, dimension_classes=3)\n",
    "\n",
    "# Create dataset with parallel processing\n",
    "print(\"Creating dataset with parallel processing...\")\n",
    "parallel_dset = OptimizedCsDataset(\n",
    "    input=files,\n",
    "    output=os.path.join(datapath, \"processed_parallel\"),\n",
    "    transform=T.ToSparseTensor,\n",
    "    pre_transform=onehot_transform,\n",
    "    pre_filter=None,\n",
    "    validate_data=True,\n",
    ")\n",
    "\n",
    "# This would process the dataset in parallel\n",
    "# Uncomment to run:\n",
    "# parallel_dset.process_parallel(max_workers=8, chunk_size=100)\n",
    "\n",
    "# Create dataset with memory-efficient storage\n",
    "print(\"Creating dataset with memory-efficient storage...\")\n",
    "efficient_dset = EfficientCsDataset(\n",
    "    input=files,\n",
    "    output=os.path.join(datapath, \"processed_efficient\"),\n",
    "    transform=T.ToSparseTensor,\n",
    "    pre_transform=onehot_transform,\n",
    "    pre_filter=None,\n",
    "    validate_data=True,\n",
    ")\n",
    "\n",
    "# This would process the dataset with memory-efficient storage\n",
    "# Uncomment to run:\n",
    "# efficient_dset.process()\n",
    "\n",
    "# Benchmark loading times\n",
    "def benchmark_loading(dataset, num_samples=100):\n",
    "    start_time = time.time()\n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        _ = dataset[i]\n",
    "    end_time = time.time()\n",
    "    print(f\"Average loading time: {(end_time - start_time) / num_samples:.4f} seconds per graph\")\n",
    "    ```\n",
    "# PyTorch Geometric-Specific Optimizations\n",
    "\n",
    "Beyond general parallelization and efficient storage, there are some PyTorch Geometric-specific optimizations:\n",
    "\n",
    "## 1. Pre-Computing Sparse Formats\n",
    "\n",
    "Converting from dense to sparse formats is computationally expensive. Pre-computing and storing these representations can save time:\n",
    "\n",
    "```python\n",
    "# Pre-compute sparse formats during processing\n",
    "data.edge_index, data.edge_attr = dense_to_sparse(adjacency_matrix)\n",
    "```\n",
    "\n",
    "## 2. Using Pre-Normalized Adjacency Matrices\n",
    "\n",
    "For GCN-based models, pre-computing the normalized adjacency matrix can speed up training:\n",
    "\n",
    "```python\n",
    "# During preprocessing:\n",
    "edge_index, edge_weight = dense_to_sparse(adj_matrix)\n",
    "edge_index, edge_weight = GCNConv.norm(edge_index, edge_weight, num_nodes)\n",
    "```\n",
    "\n",
    "## 3. Batching Strategy\n",
    "\n",
    "For large graphs, consider using smaller batch sizes or specialized batching strategies.\n",
    "\n",
    "## 4. In-Memory Dataset for Training\n",
    "\n",
    "After preprocessing, consider using an in-memory dataset variant for training if your data fits in RAM:\n",
    "\n",
    "```python\n",
    "class InMemoryGraphDataset(InMemoryDataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data_list = [dataset[i] for i in range(len(dataset))]\n",
    "        self.data, self.slices = self.collate(self.data_list)\n",
    "```\n",
    "\n",
    "## 5. Using `torch_sparse` and `torch_scatter`\n",
    "\n",
    "These libraries provide optimized operations for sparse data structures and are used by PyTorch Geometric internally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Optimizing Dataset Processing\n",
    "\n",
    "Let's discuss strategies to optimize and parallelize the dataset processing:\n",
    "\n",
    "1. **Multiprocessing** to parallelize file processing\n",
    "2. **Memory-mapped files** to reduce memory usage\n",
    "3. **Efficient serialization** to speed up I/O operations\n",
    "4. **Chunking** to process data in parallel batches\n",
    "5. **Alternative storage formats** for PyTorch Geometric Data objects\n",
    "import concurrent.futures\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "```python\n",
    "def process_single_graph(args):\n",
    "    \"\"\"Process a single graph from an HDF5 file\n",
    "    \n",
    "    Args:\n",
    "        args: Tuple containing (file_path, idx, output_dir, validate_data, pre_filter, pre_transform)\n",
    "        \n",
    "    Returns:\n",
    "        int: The index of the processed graph\n",
    "    \"\"\"\n",
    "    file_path, idx, output_dir, validate_data, pre_filter, pre_transform = args\n",
    "    \n",
    "    with h5py.File(file_path, \"r\") as f:\n",
    "        data = load_graph(\n",
    "            f, \n",
    "            idx,\n",
    "            float_dtype=torch.float32,\n",
    "            int_dtype=torch.int64,\n",
    "            validate=validate_data\n",
    "        )\n",
    "    \n",
    "    if pre_filter is not None and not pre_filter(data):\n",
    "        return idx, False  # Skip this data point\n",
    "        \n",
    "    if pre_transform is not None:\n",
    "        data = pre_transform(data)\n",
    "        \n",
    "    # Save to disk\n",
    "    torch.save(data, os.path.join(output_dir, f\"data_{idx}.pt\"))\n",
    "    return idx, True\n",
    "\n",
    "class OptimizedCsDataset(CsDataset):\n",
    "    def process_parallel(self, max_workers=None, chunk_size=100):\n",
    "        \"\"\"Process the dataset in parallel using multiprocessing.\n",
    "        \n",
    "        Args:\n",
    "            max_workers: Maximum number of worker processes\n",
    "            chunk_size: Number of graphs to process in each worker\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Convert NumPy arrays to Python lists for JSON serialization\n",
    "        d = {\n",
    "            \"manifold_codes\": [v.item() for v in self.manifold_codes],\n",
    "            \"manifolds\": [str(m) for m in self.manifold_names],\n",
    "            \"boundaries\": [str(m) for m in self.boundaries],\n",
    "            \"boundary_codes\": [v.item() for v in self.boundary_codes],\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(self.processed_dir, \"metadata.json\"), \"w\") as f:\n",
    "            json.dump(d, f)\n",
    "            \n",
    "        # Calculate optimal number of workers if not specified\n",
    "        if max_workers is None:\n",
    "            max_workers = min(os.cpu_count(), 16)  # Use at most 16 workers or CPU count\n",
    "            \n",
    "        print(f\"Processing dataset using {max_workers} workers\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        total_processed = 0\n",
    "        \n",
    "        # Process each file\n",
    "        for file_idx, file in enumerate(self.raw_paths):\n",
    "            if not os.path.exists(file):\n",
    "                raise FileNotFoundError(f\"Input file {file} does not exist.\")\n",
    "                \n",
    "            with h5py.File(file, \"r\") as f:\n",
    "                num_graphs = f[\"num_causal_sets\"][()]\n",
    "                print(f\"Processing file {file_idx+1}/{len(self.raw_paths)}: {file} with {num_graphs} graphs\")\n",
    "                \n",
    "                # Create argument list for all graphs in this file\n",
    "                args_list = [\n",
    "                    (file, idx, self.processed_dir, self.validate_data, self.pre_filter, self.pre_transform)\n",
    "                    for idx in range(num_graphs)\n",
    "                ]\n",
    "                \n",
    "                # Process in parallel using chunks\n",
    "                with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "                    # Submit tasks in chunks to reduce overhead\n",
    "                    future_to_idx = {}\n",
    "                    for i in range(0, len(args_list), chunk_size):\n",
    "                        chunk_args = args_list[i:i + chunk_size]\n",
    "                        future_batch = executor.map(process_single_graph, chunk_args)\n",
    "                        future_to_idx[i] = future_batch\n",
    "                    \n",
    "                    # Process results with progress bar\n",
    "                    with tqdm(total=num_graphs) as pbar:\n",
    "                        for start_idx, future_batch in future_to_idx.items():\n",
    "                            for idx, success in future_batch:\n",
    "                                if success:\n",
    "                                    total_processed += 1\n",
    "                                pbar.update(1)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Processed {total_processed} graphs in {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"Average processing time: {(end_time - start_time) / total_processed:.4f} seconds per graph\")\n",
    "    ```\n",
    "\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "import torch.utils.data\n",
    "import tempfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "class MemoryEfficientStorage:\n",
    "    \"\"\"A class to handle memory-efficient storage of PyTorch Geometric Data objects\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_data_efficient(data, file_path):\n",
    "        \"\"\"Save a PyTorch Geometric Data object efficiently.\n",
    "        \n",
    "        This method:\n",
    "        1. Converts sparse tensors to COO format\n",
    "        2. Uses memory-mapped files for large tensors\n",
    "        3. Compresses the file\n",
    "        \n",
    "        Args:\n",
    "            data: PyTorch Geometric Data object\n",
    "            file_path: Path to save the data\n",
    "        \"\"\"\n",
    "        # Create a dictionary to store serialized attributes\n",
    "        serialized = {}\n",
    "        \n",
    "        # Process each attribute of the data object\n",
    "        for key, value in data:\n",
    "            if torch.is_tensor(value):\n",
    "                # For large tensors, use memory mapping\n",
    "                if value.numel() > 1000000:  # Threshold for large tensors\n",
    "                    # Use .npy format for large tensors\n",
    "                    np_path = f\"{file_path}.{key}.npy\"\n",
    "                    np_array = value.detach().cpu().numpy()\n",
    "                    np.save(np_path, np_array)\n",
    "                    serialized[key] = {\n",
    "                        'type': 'numpy_memmap',\n",
    "                        'path': np_path,\n",
    "                        'shape': value.shape,\n",
    "                        'dtype': str(value.dtype)\n",
    "                    }\n",
    "                else:\n",
    "                    # Use standard serialization for smaller tensors\n",
    "                    serialized[key] = {\n",
    "                        'type': 'tensor',\n",
    "                        'data': value.detach().cpu()\n",
    "                    }\n",
    "            elif isinstance(value, SparseTensor):\n",
    "                # Handle sparse tensors\n",
    "                row, col, values = value.coo()\n",
    "                serialized[key] = {\n",
    "                    'type': 'sparse_tensor',\n",
    "                    'row': row.detach().cpu(),\n",
    "                    'col': col.detach().cpu(),\n",
    "                    'values': values.detach().cpu() if values is not None else None,\n",
    "                    'size': value.size()\n",
    "                }\n",
    "            else:\n",
    "                # For other data types, use pickle directly\n",
    "                serialized[key] = {\n",
    "                    'type': 'pickle',\n",
    "                    'data': value\n",
    "                }\n",
    "                \n",
    "        # Save the serialized data\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(serialized, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_data_efficient(file_path):\n",
    "        \"\"\"Load a PyTorch Geometric Data object efficiently.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the saved data\n",
    "            \n",
    "        Returns:\n",
    "            PyTorch Geometric Data object\n",
    "        \"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            serialized = pickle.load(f)\n",
    "            \n",
    "        # Create a new Data object\n",
    "        data = Data()\n",
    "        \n",
    "        # Process each serialized attribute\n",
    "        for key, value in serialized.items():\n",
    "            if value['type'] == 'numpy_memmap':\n",
    "                # Load memory-mapped tensor\n",
    "                np_array = np.load(value['path'], mmap_mode='r')\n",
    "                tensor = torch.from_numpy(np_array).clone()  # Clone to avoid issues with mmap\n",
    "                setattr(data, key, tensor)\n",
    "            elif value['type'] == 'tensor':\n",
    "                # Load standard tensor\n",
    "                setattr(data, key, value['data'])\n",
    "            elif value['type'] == 'sparse_tensor':\n",
    "                # Recreate sparse tensor\n",
    "                sparse = SparseTensor(\n",
    "                    row=value['row'],\n",
    "                    col=value['col'],\n",
    "                    value=value['values'],\n",
    "                    sparse_sizes=value['size']\n",
    "                )\n",
    "                setattr(data, key, sparse)\n",
    "            elif value['type'] == 'pickle':\n",
    "                # Load pickled data\n",
    "                setattr(data, key, value['data'])\n",
    "                \n",
    "        return data\n",
    "\n",
    "class EfficientCsDataset(OptimizedCsDataset):\n",
    "    \"\"\"A memory-efficient version of the CsDataset class\"\"\"\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"Process the dataset using memory-efficient storage\"\"\"\n",
    "        # Convert NumPy arrays to Python lists for JSON serialization\n",
    "        d = {\n",
    "            \"manifold_codes\": [v.item() for v in self.manifold_codes],\n",
    "            \"manifolds\": [str(m) for m in self.manifold_names],\n",
    "            \"boundaries\": [str(m) for m in self.boundaries],\n",
    "            \"boundary_codes\": [v.item() for v in self.boundary_codes],\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(self.processed_dir, \"metadata.json\"), \"w\") as f:\n",
    "            json.dump(d, f)\n",
    "\n",
    "        for file in self.raw_paths:\n",
    "            if not os.path.exists(file):\n",
    "                raise FileNotFoundError(f\"Input file {file} does not exist.\")\n",
    "            with h5py.File(file, \"r\") as f:\n",
    "                print(f\"Processing file: {file}\")\n",
    "                print(f\"Number of causal sets: {f['num_causal_sets'][()]}\")\n",
    "                for idx in tqdm(range(f[\"num_causal_sets\"][()])):\n",
    "                    data = load_graph(\n",
    "                        f,\n",
    "                        idx,\n",
    "                        float_dtype=torch.float32,\n",
    "                        int_dtype=torch.int64,\n",
    "                        validate=self.validate_data,\n",
    "                    )\n",
    "                    if self.pre_filter is not None:\n",
    "                        if not self.pre_filter(data):\n",
    "                            continue\n",
    "                    if self.pre_transform is not None:\n",
    "                        data = self.pre_transform(data)\n",
    "                    \n",
    "                    # Use efficient storage\n",
    "                    output_path = os.path.join(self.processed_dir, f\"data_{idx}.pt\")\n",
    "                    MemoryEfficientStorage.save_data_efficient(data, output_path)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\"Get a data object using memory-efficient loading\"\"\"\n",
    "        data_path = os.path.join(self.processed_dir, f\"data_{idx}.pt\")\n",
    "        data = MemoryEfficientStorage.load_data_efficient(data_path)\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        return data\n",
    "```\n",
    "# Storage Formats for PyTorch Geometric Data Objects\n",
    "\n",
    "When handling large graph datasets, choosing the right storage format is crucial. Here are some options:\n",
    "\n",
    "## 1. PyTorch's Native Format (.pt)\n",
    "- **Pros**: Direct compatibility with PyTorch, fast loading in PyTorch\n",
    "- **Cons**: Files can be large, not human-readable, versioning issues between PyTorch versions\n",
    "\n",
    "## 2. Memory-Mapped Files (with .npy for large tensors)\n",
    "- **Pros**: Reduced memory usage, allows working with datasets that don't fit in RAM\n",
    "- **Cons**: Slightly slower access, more complex implementation\n",
    "\n",
    "## 3. Compressed Formats\n",
    "- **Pros**: Smaller file size, good for storage and transfer\n",
    "- **Cons**: Additional compression/decompression overhead\n",
    "\n",
    "## 4. HDF5 Format (.h5)\n",
    "- **Pros**: Good for hierarchical data, partial loading, good compression\n",
    "- **Cons**: More complex API, potential compatibility issues\n",
    "\n",
    "## 5. LMDB (Lightning Memory-Mapped Database)\n",
    "- **Pros**: Great for large datasets, fast random access, transactional\n",
    "- **Cons**: More setup required, less intuitive API\n",
    "\n",
    "Our `MemoryEfficientStorage` class implements a hybrid approach:\n",
    "- Small tensors: Stored directly in PyTorch format\n",
    "- Large tensors: Stored as memory-mapped NumPy arrays\n",
    "- Metadata: Handled via Python's pickle format\n",
    "\n",
    "This balances memory efficiency and performance for large graph datasets.\n",
    "```python\n",
    "# Example of using the optimized dataset classes\n",
    "\n",
    "# Set up paths\n",
    "datapath = os.path.join(\"/mnt\", \"dataLinux\", \"machinelearning_data\", \"QuantumGrav/causal_sets\")\n",
    "files = [\n",
    "    os.path.join(datapath, \"cset_data_min=300_max=650_N=25000_d=2.h5\"),\n",
    "    os.path.join(datapath, \"cset_data_min=300_max=650_N=25000_d=3.h5\"),\n",
    "    os.path.join(datapath, \"cset_data_min=300_max=650_N=25000_d=4.h5\"),\n",
    "]\n",
    "\n",
    "# Define transforms\n",
    "onehot_transform = OneHotEncodeTargets(manifold_classes=6, boundary_classes=3, dimension_classes=3)\n",
    "\n",
    "# Create dataset with parallel processing\n",
    "print(\"Creating dataset with parallel processing...\")\n",
    "parallel_dset = OptimizedCsDataset(\n",
    "    input=files,\n",
    "    output=os.path.join(datapath, \"processed_parallel\"),\n",
    "    transform=T.ToSparseTensor,\n",
    "    pre_transform=onehot_transform,\n",
    "    pre_filter=None,\n",
    "    validate_data=True,\n",
    ")\n",
    "\n",
    "# This would process the dataset in parallel\n",
    "# Uncomment to run:\n",
    "# parallel_dset.process_parallel(max_workers=8, chunk_size=100)\n",
    "\n",
    "# Create dataset with memory-efficient storage\n",
    "print(\"Creating dataset with memory-efficient storage...\")\n",
    "efficient_dset = EfficientCsDataset(\n",
    "    input=files,\n",
    "    output=os.path.join(datapath, \"processed_efficient\"),\n",
    "    transform=T.ToSparseTensor,\n",
    "    pre_transform=onehot_transform,\n",
    "    pre_filter=None,\n",
    "    validate_data=True,\n",
    ")\n",
    "\n",
    "# This would process the dataset with memory-efficient storage\n",
    "# Uncomment to run:\n",
    "# efficient_dset.process()\n",
    "\n",
    "# Benchmark loading times\n",
    "def benchmark_loading(dataset, num_samples=100):\n",
    "    start_time = time.time()\n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        _ = dataset[i]\n",
    "    end_time = time.time()\n",
    "    print(f\"Average loading time: {(end_time - start_time) / num_samples:.4f} seconds per graph\")\n",
    "    ```\n",
    "# PyTorch Geometric-Specific Optimizations\n",
    "\n",
    "Beyond general parallelization and efficient storage, there are some PyTorch Geometric-specific optimizations:\n",
    "\n",
    "## 1. Pre-Computing Sparse Formats\n",
    "\n",
    "Converting from dense to sparse formats is computationally expensive. Pre-computing and storing these representations can save time:\n",
    "\n",
    "```python\n",
    "# Pre-compute sparse formats during processing\n",
    "data.edge_index, data.edge_attr = dense_to_sparse(adjacency_matrix)\n",
    "```\n",
    "\n",
    "## 2. Using Pre-Normalized Adjacency Matrices\n",
    "\n",
    "For GCN-based models, pre-computing the normalized adjacency matrix can speed up training:\n",
    "\n",
    "```python\n",
    "# During preprocessing:\n",
    "edge_index, edge_weight = dense_to_sparse(adj_matrix)\n",
    "edge_index, edge_weight = GCNConv.norm(edge_index, edge_weight, num_nodes)\n",
    "```\n",
    "\n",
    "## 3. Batching Strategy\n",
    "\n",
    "For large graphs, consider using smaller batch sizes or specialized batching strategies.\n",
    "\n",
    "## 4. In-Memory Dataset for Training\n",
    "\n",
    "After preprocessing, consider using an in-memory dataset variant for training if your data fits in RAM:\n",
    "\n",
    "```python\n",
    "class InMemoryGraphDataset(InMemoryDataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data_list = [dataset[i] for i in range(len(dataset))]\n",
    "        self.data, self.slices = self.collate(self.data_list)\n",
    "```\n",
    "\n",
    "## 5. Using `torch_sparse` and `torch_scatter`\n",
    "\n",
    "These libraries provide optimized operations for sparse data structures and are used by PyTorch Geometric internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
