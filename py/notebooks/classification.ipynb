{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "# torch.multiprocessing.set_start_method('spawn', force=True)  # For multiprocessing support\n",
    "from torch_geometric.loader import DataLoader \n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from functools import lru_cache\n",
    "from collections.abc import Callable\n",
    "import concurrent.futures \n",
    "import torch.nn.functional as F \n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "optimizations: \n",
    "- load a batch of data for all datasets\n",
    "- process them in parallel \n",
    "- then load the next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: this function is too long, break it up into smaller ones\n",
    "\n",
    "\n",
    "def load_graph(\n",
    "    f: h5py.File,\n",
    "    idx: int,\n",
    "    float_dtype: torch.dtype,\n",
    "    int_dtype: torch.dtype,\n",
    "    validate: bool = False,\n",
    ") -> Data:\n",
    "    \n",
    "    # print(f\"  Loading graph at index {idx}...\")\n",
    "    # Load adjacency matrix and convert to edge indices\n",
    "    # print(\"  Data shapes: \")\n",
    "    # print(f\"   Adjacency matrix data shape: {f['adjacency_matrix'].shape}\")\n",
    "    # print(f\"   link matrix data shape: {f['link_matrix'].shape}\")\n",
    "    # print(f\"   past_relations data shape: {f['past_relations'].shape}\")\n",
    "    # print(f\"   future_relations data shape: {f['future_relations'].shape}\")\n",
    "    # print(f\"   in_degrees data shape: {f['in_degrees'].shape}\")\n",
    "    # print(f\"   out_degrees data shape: {f['out_degrees'].shape}\")\n",
    "    # print(f\"   max_path_lengths_future data shape: {f['max_path_lengths_future'].shape}\")\n",
    "    # print(f\"   max_path_lengths_past data shape: {f['max_path_lengths_past'].shape}\")\n",
    "    # print(f\"   Adjacency matrix data shape: {f['adjacency_matrix'].shape}\")\n",
    "\n",
    "    adj_raw = f[\"adjacency_matrix\"][idx, :, :]\n",
    "    # print(' raw adjacency matrix shape:', adj_raw.shape)\n",
    "    adj_matrix = torch.tensor(adj_raw, dtype=float_dtype)\n",
    "    # print('  Converting adjacency matrix to edge indices...')\n",
    "    edge_index, edge_weight = dense_to_sparse(adj_matrix)\n",
    "    # print('  Converting adjacency matrix to sparse format...')\n",
    "    adj_matrix = adj_matrix.to_sparse()\n",
    "    \n",
    "    # print('  Loading node features...')\n",
    "    # Load node features\n",
    "    node_features = []\n",
    "\n",
    "    # Sprinkling coordinates\n",
    "    # sprinkling = torch.tensor(f[\"sprinkling\"][idx, :, :], dtype=float_dtype)\n",
    "\n",
    "    # node_features.append(sprinkling)\n",
    "\n",
    "    # Degree information\n",
    "    in_degrees = torch.tensor(f[\"in_degrees\"][idx,:], dtype=float_dtype).unsqueeze(1)\n",
    "    out_degrees = torch.tensor(f[\"out_degrees\"][idx,:], dtype=float_dtype).unsqueeze(1)\n",
    "    node_features.extend([in_degrees, out_degrees])\n",
    "\n",
    "    # Path lengths\n",
    "    max_path_future = torch.tensor(\n",
    "        f[\"max_path_lengths_future\"][idx,:], dtype=float_dtype\n",
    "    ).unsqueeze(1)\n",
    "    max_path_past = torch.tensor(\n",
    "        f[\"max_path_lengths_past\"][idx,:], dtype=float_dtype\n",
    "    ).unsqueeze(1)\n",
    "    node_features.extend([max_path_future, max_path_past])\n",
    "\n",
    "    # I need more topological information here - angles, etc.\n",
    "    # Link-based path lengths\n",
    "    # max_path_future_links = torch.tensor(\n",
    "    #     f[\"max_path_lengths_future_links\"][idx,:], dtype=float_dtype\n",
    "    # ).unsqueeze(1)\n",
    "    # max_path_past_links = torch.tensor(\n",
    "    #     f[\"max_path_lengths_past_links\"][idx,:], dtype=float_dtype\n",
    "    # ).unsqueeze(1)\n",
    "    # node_features.extend([max_path_future_links, max_path_past_links])\n",
    "\n",
    "    # # Topological ordering --> TODO: check again what this does\n",
    "    # topo_future = torch.tensor(\n",
    "    #     f[\"topological_order_future\"][idx,:], dtype=float_dtype\n",
    "    # ).unsqueeze(1)\n",
    "    # topo_past = torch.tensor(\n",
    "    #     f[\"topological_order_past\"][idx,:], dtype=float_dtype\n",
    "    # ).unsqueeze(1)\n",
    "    # node_features.extend([topo_future, topo_past])\n",
    "\n",
    "    # Concatenate all node features\n",
    "    x = torch.cat(node_features, dim=1)\n",
    "\n",
    "    # Load graph-level features (targets for regression)\n",
    "    # print('  Loading graph-level features...')\n",
    "    manifold_id = int(f[\"manifold_ids\"][idx])\n",
    "    boundary_id = int(f[\"boundary_ids\"][idx])\n",
    "    relation_dim = torch.tensor(f[\"relation_dim\"][idx], dtype=float_dtype)\n",
    "    atom_count = torch.tensor(f[\"atom_count\"][idx], dtype=int_dtype)\n",
    "    num_sources = torch.tensor(f[\"num_sources\"][idx], dtype=int_dtype)\n",
    "    num_sinks = torch.tensor(f[\"num_sinks\"][idx], dtype=int_dtype)\n",
    "    dimension = int(f[\"dimension\"][()])\n",
    "\n",
    "    # print('  Loading additional matrices...')\n",
    "    # matrices\n",
    "    link_matrix = torch.tensor(\n",
    "        f[\"link_matrix\"][idx, :, :], dtype=float_dtype\n",
    "    ).to_sparse()\n",
    "    past_relations = torch.tensor(\n",
    "        f[\"past_relations\"][idx, :, :], dtype=float_dtype\n",
    "    ).to_sparse()\n",
    "    future_relations = torch.tensor(\n",
    "        f[\"future_relations\"][idx, :, :], dtype=float_dtype\n",
    "    ).to_sparse()\n",
    "\n",
    "    # print('  Creating Data object...')\n",
    "    # Create Data object\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index.to('cpu'),\n",
    "        edge_attr=edge_weight.unsqueeze(1).to('cpu')\n",
    "        if edge_weight.numel() > 0\n",
    "        else None,  # Not sure if this is a good idea need to add edge attributes if possible\n",
    "        # node positions as positional attributes as well\n",
    "        # pos=sprinkling,\n",
    "        y=torch.tensor([manifold_id, boundary_id, dimension], dtype=torch.long).to('cpu'),\n",
    "        # Graph-level attributes\n",
    "        manifold_id=manifold_id,\n",
    "        boundary_id=boundary_id,\n",
    "        relation_dim=relation_dim,\n",
    "        dimension=dimension,\n",
    "        atom_count=atom_count,\n",
    "        num_sources=num_sources.to('cpu'),\n",
    "        num_sinks=num_sinks.to('cpu'),\n",
    "        # sprinkling=sprinkling, # don't use this for now.\n",
    "        # Additional matrices as graph attributes. make the shitty past and future relations and all that into node attributes!\n",
    "        adjacency_matrix=adj_matrix.to('cpu'),\n",
    "        link_matrix=link_matrix.to('cpu'),\n",
    "        past_relations=past_relations.to('cpu'),\n",
    "        future_relations=future_relations.to('cpu'),\n",
    "    )\n",
    "\n",
    "    if validate:\n",
    "        data.validate()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OneHotEncodeTargets:\n",
    "    def __init__(self, manifold_classes=6, boundary_classes=3, dimension_classes=3):\n",
    "        self.manifold_classes = manifold_classes\n",
    "        self.boundary_classes = boundary_classes  \n",
    "        self.dimension_classes = dimension_classes\n",
    "        \n",
    "        # Dimension mapping: {2: 0, 3: 1, 4: 2}\n",
    "        self.dim_to_idx = {2: 0, 3: 1, 4: 2}\n",
    "        \n",
    "    def __call__(self, data: Data) -> Data:\n",
    "        # Extract the original targets\n",
    "        manifold_id = data.y[0].long() - 1  # Convert to 0-based indexing\n",
    "        boundary_id = data.y[1].long() - 1  # Convert to 0-based indexing  \n",
    "        dimension = data.y[2].long()        # Keep as is for mapping\n",
    "        \n",
    "        # Map dimension to 0-based index\n",
    "        dim_idx = self.dim_to_idx[dimension.item()]\n",
    "        \n",
    "        # Create one-hot encodings\n",
    "        manifold_onehot = F.one_hot(manifold_id, num_classes=self.manifold_classes).float()\n",
    "        boundary_onehot = F.one_hot(boundary_id, num_classes=self.boundary_classes).float()\n",
    "        dimension_onehot = F.one_hot(torch.tensor(dim_idx), num_classes=self.dimension_classes).float()\n",
    "        y_onehot = torch.cat([manifold_onehot, boundary_onehot, dimension_onehot], dim=0)\n",
    "\n",
    "        data.y_original = data.y  # Keep original for reference\n",
    "        data.y = y_onehot\n",
    "\n",
    "        data.target_info={\n",
    "            'manifold_classes': self.manifold_classes,\n",
    "            'boundary_classes': self.boundary_classes,\n",
    "            'dimension_classes': self.dimension_classes,\n",
    "            'manifold_offset': 0, \n",
    "            'boundary_offset': self.manifold_classes,\n",
    "            'dimension_offset': self.manifold_classes + self.boundary_classes,   \n",
    "            'total_classes': self.manifold_classes + self.boundary_classes + self.dimension_classes,\n",
    "        }\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}(manifold_classes={self.manifold_classes}, boundary_classes={self.boundary_classes}, dimension_classes={self.dimension_classes})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_shift(data, manifold_classes=6, boundary_classes=3, dimension_classes=3) -> Data: \n",
    "    manifold_id = data.y[0].long() - 1  # conver to 0 based\n",
    "    boundary_id = data.y[1].long() - 1  # conver to 0 based\n",
    "    dimension = data.y[2].long() - 2  # conver to 0 based -> 2D is the lowest we can have\n",
    "    \n",
    "    data.y_original = data.y  # Keep original for reference\n",
    "    data.y = torch.tensor([[dimension, boundary_id, manifold_id],], dtype=torch.long)\n",
    "\n",
    "    data.target_info={\n",
    "        'manifold_classes': manifold_classes,\n",
    "        'boundary_classes': boundary_classes,\n",
    "        'dimension_classes': dimension_classes,\n",
    "        'dimension_offset': 0,   \n",
    "        'boundary_offset': dimension_classes,\n",
    "        'manifold_offset': dimension_classes + boundary_classes, \n",
    "        'total_classes': manifold_classes + boundary_classes + dimension_classes,\n",
    "    }\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input: list[str],\n",
    "        output: str,  # = root directory for processed data\n",
    "        transform: Callable[[Data], Data] | None = None,\n",
    "        pre_transform: Callable[[Data], Data] | None = None,\n",
    "        pre_filter: Callable[[Data], Data] | None = None,\n",
    "        validate_data: bool = False,\n",
    "        loader: Callable[[h5py.File, torch.dtype, torch.dtype, bool], Data] = load_graph,\n",
    "    ):\n",
    "        self.input = input\n",
    "        self._num_samples = None\n",
    "        self.validate_data = validate_data\n",
    "        self.root = output\n",
    "        self.loader = loader\n",
    "        self.root = output\n",
    "\n",
    "        if len(self.processed_file_names) > 0:\n",
    "            self.num_samples = len(self.processed_file_names)\n",
    "\n",
    "            if len(self.processed_file_names) == 0:\n",
    "                raise ValueError(\"No processed data found in the output directory.\")\n",
    "            \n",
    "            self._load_metadata()\n",
    "\n",
    "            self._num_samples = len(self.processed_file_names)\n",
    "        else:\n",
    "            if input is None or len(input) == 0:\n",
    "                raise ValueError(\"Input files must be provided for processing.\")\n",
    "            \n",
    "            with h5py.File(input[0], \"r\") as f:\n",
    "                self.manifold_codes = f[\"manifold_codes\"][()]\n",
    "                self.manifold_names = f[\"manifolds\"][()]\n",
    "                self.boundaries = f[\"boundaries\"][()]\n",
    "                self.boundary_codes = f[\"boundary_codes\"][()]\n",
    "\n",
    "            self._num_samples = 0\n",
    "            for file in self.input:\n",
    "                if not os.path.exists(file):\n",
    "                    raise FileNotFoundError(f\"Input file {file} does not exist.\")\n",
    "                with h5py.File(file, \"r\") as f:\n",
    "                    self._num_samples += f[\"num_causal_sets\"][()]\n",
    "                    print(f\"Processing file: {file}, current number of samples: {self._num_samples}\")\n",
    "\n",
    "        super().__init__(output, transform, pre_transform, pre_filter)\n",
    "\n",
    "\n",
    "    def _load_metadata(self): \n",
    "        metadata_path = os.path.join(self.processed_dir, \"metadata.json\")\n",
    "        if not os.path.exists(metadata_path):\n",
    "            raise FileNotFoundError(f\"Metadata file {metadata_path} does not exist.\")\n",
    "        \n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "            self.manifold_codes = metadata[\"manifold_codes\"]\n",
    "            self.manifold_names = metadata[\"manifolds\"]\n",
    "            self.boundaries = metadata[\"boundaries\"]\n",
    "            self.boundary_codes = metadata[\"boundary_codes\"]\n",
    "\n",
    "    @property\n",
    "    def raw_paths(self):\n",
    "        return self.input\n",
    "\n",
    "    @property\n",
    "    def output(self):\n",
    "        return self.root\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [os.path.basename(f) for f in self.input]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        if os.path.isdir(self.processed_dir) is False: \n",
    "            return []\n",
    "\n",
    "        all_files = os.listdir(self.processed_dir)\n",
    "        return [\n",
    "            f\n",
    "            for f in all_files\n",
    "            if f.startswith(\"data_\") and f.endswith(\".pt\")\n",
    "        ]\n",
    "\n",
    "    def process(self):\n",
    "        print(\"processed dir: \", self.processed_dir, len(self.processed_file_names))\n",
    "        # Convert NumPy arrays to Python lists for JSON serialization\n",
    "        if not os.path.exists(self.processed_dir) or len(self.processed_file_names) == 0: \n",
    "            d = {\n",
    "                \"manifold_codes\": [v.item() for v in self.manifold_codes],\n",
    "                \"manifolds\": [str(m) for m in self.manifold_names],\n",
    "                \"boundaries\": [str(m) for m in self.boundaries],\n",
    "                \"boundary_codes\": [v.item() for v in self.boundary_codes],\n",
    "            }\n",
    "\n",
    "            with open(os.path.join(self.processed_dir, \"metadata.json\"), \"w\") as f:\n",
    "                json.dump(d, f)\n",
    "\n",
    "            file_index = 0\n",
    "            for file in self.raw_paths:\n",
    "                if not os.path.exists(file):\n",
    "                    raise FileNotFoundError(f\"Input file {file} does not exist.\")\n",
    "                with h5py.File(file, \"r\") as f:\n",
    "                    # FIXME: this loop should be parallelized for large datasets\n",
    "                    print(f\"Processing file: {file}\")\n",
    "                    print(f\"Number of causal sets: {f['num_causal_sets'][()]} of total {self._num_samples} with current index {file_index}\")\n",
    "                    for idx in tqdm(range(f[\"num_causal_sets\"][()])):\n",
    "                        data = self.loader(\n",
    "                            f,\n",
    "                            idx,\n",
    "                            float_dtype=torch.float32,\n",
    "                            int_dtype=torch.int64,\n",
    "                            validate=self.validate_data,\n",
    "                        )\n",
    "                        if self.pre_filter is not None:\n",
    "                            if not self.pre_filter(data):\n",
    "                                continue\n",
    "                        if self.pre_transform is not None:\n",
    "                            data = self.pre_transform(data)\n",
    "                        torch.save(data.to('cpu'), os.path.join(self.processed_dir, f\"data_{file_index}.pt\"))\n",
    "                        file_index += 1\n",
    "\n",
    "    def len(self):\n",
    "        return self._num_samples\n",
    "\n",
    "    def get(self, idx):\n",
    "        # TODO: check again about the weights_only=False part\n",
    "        data = torch.load(os.path.join(self.processed_dir, f\"data_{idx}.pt\"), weights_only=False)\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch_geometric\n",
    "import os\n",
    "from torch.nn import Linear \n",
    "from torch_geometric.nn.conv import GCNConv, GATConv, SAGEConv, GraphConv, GATv2Conv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool, SAGPooling, Set2Set\n",
    "import torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBlock(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.5, gcn_type=GCNConv, batchnorm=torch.nn.Identity, activation = F.relu, gcn_kwargs=None):\n",
    "        super(GCNBlock, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.gcn_type = gcn_type\n",
    "        self.conv = gcn_type(input_dim, output_dim, **(gcn_kwargs if gcn_kwargs else {}))\n",
    "        self.activation = activation\n",
    "        self.batch_norm = batchnorm\n",
    "\n",
    "        if input_dim != output_dim:\n",
    "            # Use 1x1 convolution for projection\n",
    "            self.projection = Linear(input_dim, output_dim, bias=False)\n",
    "        else: \n",
    "            self.projection = torch.nn.Identity()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None, kwargs=None):\n",
    "        x_res = x\n",
    "        x = self.conv(x, edge_index, edge_weight=edge_weight, **(kwargs if kwargs else {}))  # Apply the GCN layer\n",
    "        x = self.batch_norm(x, )  # this is a no-op if batch normalization is not used\n",
    "        x = self.activation(x)\n",
    "        x = x + self.projection(x_res)  # skip connection\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)  # this is only applied during training\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBackbone(torch.nn.Module): \n",
    "    def __init__(self, gcn_net: list[GCNBlock]): \n",
    "        super(GCNBackbone, self).__init__()\n",
    "        self.gcn_net = torch.nn.ModuleList(gcn_net)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None, gcn_kwargs=None):\n",
    "        out = x\n",
    "        for layer in self.gcn_net:\n",
    "            out = layer(out, edge_index, edge_weight=edge_weight, kwargs=gcn_kwargs)\n",
    "            # Note: also changed gcn_kwargs to kwargs to match GCNBlock signature\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        output_dim,\n",
    "        hidden_dims,\n",
    "        manifold_classes=6,\n",
    "        boundary_classes=3,\n",
    "        dimension_classes=3,\n",
    "        activation=F.relu,\n",
    "        linear_kwargs=None,\n",
    "        dim_kwargs=None,\n",
    "        boundary_kwargs=None,\n",
    "        manifold_kwargs=None,\n",
    "    ):\n",
    "        super(ClassifierBlock, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.total_classes = manifold_classes + boundary_classes + dimension_classes\n",
    "        self.manifold_classes = manifold_classes\n",
    "        self.boundary_classes = boundary_classes\n",
    "        self.dimension_classes = dimension_classes\n",
    "\n",
    "        if len(hidden_dims) == 0:\n",
    "            self.backbone = Linear(input_dim, output_dim, **(linear_kwargs[0] if linear_kwargs else {}))\n",
    "        else:\n",
    "            layers = []\n",
    "            in_dim = input_dim\n",
    "            for (i, hidden_dim) in enumerate(hidden_dims):\n",
    "                layers.append(\n",
    "                    Linear(in_dim, hidden_dim, **(linear_kwargs[i] if linear_kwargs and linear_kwargs[i] else {}))\n",
    "                )  # check again if we need the bias there, I don't think so actually...\n",
    "                layers.append(activation)\n",
    "                in_dim = hidden_dim\n",
    "\n",
    "            self.backbone = torch.nn.Sequential(*layers)\n",
    "\n",
    "            self.dim_layer = torch.nn.Linear(hidden_dim, self.dimension_classes, **(dim_kwargs if dim_kwargs else {}))\n",
    "\n",
    "            self.boundary_layer = torch.nn.Linear(hidden_dim, self.boundary_classes, **(boundary_kwargs if boundary_kwargs else {}))\n",
    "\n",
    "            self.manifold_layer = torch.nn.Linear(hidden_dim, self.manifold_classes, **(manifold_kwargs if manifold_kwargs else {}))\n",
    "\n",
    "    def forward(self, x, backbone_kwargs=None, dim_layer_kwargs=None, boundary_layer_kwargs=None, manifold_layer_kwargs=None):\n",
    "\n",
    "        x = self.backbone(\n",
    "            x, \n",
    "            **(backbone_kwargs if backbone_kwargs is not None else {})\n",
    "        )  \n",
    "\n",
    "        dim_logit = self.dim_layer(x, **(dim_layer_kwargs if dim_layer_kwargs is not None else {}))\n",
    "        boundary_logit = self.boundary_layer(x, **(boundary_layer_kwargs if boundary_layer_kwargs is not None else {}))\n",
    "        manifold_logit = self.manifold_layer(x, **(manifold_layer_kwargs if manifold_layer_kwargs is not None else {}))\n",
    "\n",
    "        return dim_logit, boundary_logit, manifold_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphFeaturesBlock(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims, dropout=0.5, activation = F.relu, linear_kwargs=None, final_linear_kwargs=None):\n",
    "\n",
    "        super(GraphFeaturesBlock, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        if len(hidden_dims) == 0: \n",
    "            self.linear = Linear(input_dim, output_dim)\n",
    "        else: \n",
    "            layers = []\n",
    "            in_dim = input_dim\n",
    "            for (i, hidden_dim) in enumerate(hidden_dims): \n",
    "                layers.append(Linear(in_dim, hidden_dim, **(linear_kwargs[i] if linear_kwargs and linear_kwargs[i] else {})))\n",
    "                layers.append(activation)\n",
    "                in_dim = hidden_dim\n",
    "            layers.append(Linear(in_dim, output_dim, **(final_linear_kwargs if final_linear_kwargs else {})))\n",
    "            self.linear = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "basic model class to organize the other things. does the following: \n",
    "- passes input through gcn network. This is a succession of GCN blocks\n",
    "- applies graph feature network to graph level features and concatenates them with pooled node features **if** `use_graph_features = true`. \n",
    "\n",
    "- passes the result through the regression net to get out (dimension, boundary_id, manifold_id): \n",
    "```bash\n",
    "x -> gcn -> pool -> concat(_, g) -> regression -> output\n",
    "g ----------------> MLP_g(g) _|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gcn_net,\n",
    "        classifier,\n",
    "        pooling_layer,\n",
    "        use_graph_features=False,\n",
    "        graph_features_net=torch.nn.Identity,\n",
    "    ):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.gcn_net = gcn_net\n",
    "        self.classifier = classifier\n",
    "        self.graph_features_net = graph_features_net\n",
    "        self.use_graph_features = use_graph_features\n",
    "        self.pooling_layer = pooling_layer\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        edge_index,\n",
    "        batch,\n",
    "        edge_weight=None,\n",
    "        graph_features=None,\n",
    "        gcn_kwargs=None,\n",
    "    ):\n",
    "        x = self.gcn_net(\n",
    "            x, edge_index, edge_weight=edge_weight, **(gcn_kwargs if gcn_kwargs else {})\n",
    "        )\n",
    "        \n",
    "        if batch is None:\n",
    "            batch = torch.zeros(x.shape[0], dtype=torch.long, device=x.device)\n",
    "        \n",
    "        x = self.pooling_layer(x, batch)\n",
    "        if self.use_graph_features:\n",
    "            graph_features = self.graph_features_net(graph_features)\n",
    "            x = torch.cat((x, graph_features), dim=-1)  # last dim\n",
    "        manifold, boundary, dim = self.classifier(x)\n",
    "\n",
    "        return manifold, boundary, dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(x_pred, y, dim_kwargs = None, boundary_kwargs=None, manifold_kwargs=None, dim_weight = 1.0, boundary_weight=1.0, manifold_weight=1.0):\n",
    "    dim_logits= x_pred[0]\n",
    "    boundary_logits = x_pred[1]\n",
    "    manifold_logits = x_pred[2]\n",
    "\n",
    "    dim_truth = y[:, 0]\n",
    "    boundary_truth = y[:, 1]\n",
    "    manifold_truth = y[:, 2]\n",
    "    assert dim_logits.shape[0] == y.shape[0], f\"Mismatch: {x_pred.shape} vs {y.shape}\"\n",
    "    assert boundary_logits.shape[0] == y.shape[0], f\"Mismatch: {x_pred.shape} vs {y.shape}\"\n",
    "    assert manifold_logits.shape[0] == y.shape[0], f\"Mismatch: {x_pred.shape} vs {y.shape}\"\n",
    "\n",
    "    dim_cel = torch.nn.CrossEntropyLoss(**(dim_kwargs if dim_kwargs else {}))\n",
    "    boundary_cel = torch.nn.CrossEntropyLoss(**(boundary_kwargs if boundary_kwargs else {}))\n",
    "    manifold_cel = torch.nn.CrossEntropyLoss(**(manifold_kwargs if manifold_kwargs else {}))\n",
    "\n",
    "    loss_dim = dim_cel(dim_logits, dim_truth)\n",
    "    loss_boundary = boundary_cel(boundary_logits, boundary_truth)\n",
    "    loss_manifold = manifold_cel(manifold_logits, manifold_truth)\n",
    "\n",
    "    total_loss = loss_dim * dim_weight + loss_boundary * boundary_weight + loss_manifold * manifold_weight\n",
    "\n",
    "    return total_loss, loss_dim, loss_boundary, loss_manifold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## function for training the datamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GCNBlockHyperparams:\n",
    "    input_dim: int\n",
    "    output_dim: int\n",
    "    dropout: float\n",
    "    gcn_type: torch.nn.Module\n",
    "    activation: torch.nn.Module\n",
    "    kwargs: dict\n",
    "\n",
    "@dataclass\n",
    "class ClassifierHyperparams:\n",
    "    input_dim: int\n",
    "    output_dim: int\n",
    "    hidden_dims: list[int]\n",
    "    manifold_classes: list[int]\n",
    "    boundary_classes: list[int]\n",
    "    dimension_classes: list[int]\n",
    "    activation: torch.nn.Module\n",
    "    linear_kwargs: dict\n",
    "    dim_kwargs: dict\n",
    "    boundary_kwargs: dict\n",
    "    manifold_kwargs: dict\n",
    "\n",
    "@dataclass\n",
    "class GraphnetHyperparams:\n",
    "    input_dim: int\n",
    "    output_dim: int\n",
    "    hidden_dims: list[int]\n",
    "    dropout: float\n",
    "    activation: torch.nn.Module\n",
    "    linear_kwargs: dict\n",
    "    final_linear_kwargs: dict\n",
    "\n",
    "@dataclass\n",
    "class GCNmodelHyperparams:\n",
    "    gcn_net: torch.nn.Module\n",
    "    classifier: torch.nn.Module\n",
    "    pooling_layer:torch.nn.Module\n",
    "    graph_features_net: torch.nn.Module\n",
    "    use_graph_features: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class Hyperparameters: \n",
    "    # training\n",
    "    epochs: int \n",
    "    device: str \n",
    "    early_stopping_patience: int \n",
    "    early_stopping_delta: float \n",
    "    criterion: Callable[[torch.tensor, torch.tensor], tuple[float, float, float, float]]\n",
    "    target_weights: tuple[float, float, float]\n",
    "\n",
    "    # optimizer \n",
    "    init_learning_rate: float \n",
    "    weight_decay: float\n",
    "\n",
    "    # model \n",
    "    gcn_block_hyperparams: list[GCNBlockHyperparams]\n",
    "    classifier_hyperparams: ClassifierHyperparams\n",
    "    graphnet_hyperparams: GraphnetHyperparams | None\n",
    "    gcn_model_hyperparams: GCNmodelHyperparams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### training function \n",
    "this must be made smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:GCNModel, \n",
    "          train_loader:DataLoader, \n",
    "          val_loader:DataLoader, \n",
    "          optimizer:torch.optim.Optimizer, \n",
    "          epochs:int=10, \n",
    "          device:str='cpu', \n",
    "          criterion=criterion, \n",
    "          early_stopping_patience:int=10,\n",
    "          early_stopping_delta:float=0.01,\n",
    "          ):\n",
    "\n",
    "    total_training_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "    dim_training_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "    boundary_training_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "    manifold_training_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "\n",
    "    total_validation_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "    dim_validation_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "    boundary_validation_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "    manifold_validation_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    current_patience = early_stopping_patience\n",
    "\n",
    "    lossfunc = criterion  # Use the custom criterion defined above\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_training_loss_bt = np.zeros(len(train_loader), dtype=np.float64)\n",
    "        dim_training_loss_bt = np.zeros(len(train_loader), dtype=np.float64)\n",
    "        boundary_training_loss_bt = np.zeros(len(train_loader), dtype=np.float64)\n",
    "        manifold_training_loss_bt = np.zeros(len(train_loader), dtype=np.float64)\n",
    "        \n",
    "        model.train()\n",
    "        for batchnum, data in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch} Training\")):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            total_loss, loss_dim, loss_boundary, loss_manifold = lossfunc(out, data.y)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_training_loss_bt[batchnum] = total_loss.item()\n",
    "            dim_training_loss_bt[batchnum] = loss_dim.item()\n",
    "            boundary_training_loss_bt[batchnum] = loss_boundary.item()\n",
    "            manifold_training_loss_bt[batchnum] = loss_manifold.item()\n",
    "\n",
    "        mean_total_loss = total_training_loss_bt.mean()\n",
    "        mean_dim_loss = dim_training_loss_bt.mean()\n",
    "        mean_boundary_loss = boundary_training_loss_bt.mean()\n",
    "        mean_manifold_loss = manifold_training_loss_bt.mean()\n",
    "\n",
    "        std_total_loss = total_training_loss_bt.std()\n",
    "        std_dim_loss = dim_training_loss_bt.std()\n",
    "        std_boundary_loss = boundary_training_loss_bt.std()\n",
    "        std_manifold_loss = manifold_training_loss_bt.std()\n",
    "\n",
    "        total_training_loss[epoch, 0] = mean_total_loss\n",
    "        dim_training_loss[epoch, 0] = mean_dim_loss\n",
    "        boundary_training_loss[epoch, 0] = mean_boundary_loss\n",
    "        manifold_training_loss[epoch, 0] = mean_manifold_loss   \n",
    "\n",
    "        total_training_loss[epoch, 1] = std_total_loss\n",
    "        dim_training_loss[epoch, 1] = std_dim_loss\n",
    "        boundary_training_loss[epoch, 1] = std_boundary_loss\n",
    "        manifold_training_loss[epoch, 1] = std_manifold_loss\n",
    "\n",
    "        # Validation step\n",
    "        total_validation_loss_bt = np.zeros(len(val_loader), dtype=np.float64)\n",
    "        dim_validation_loss_bt = np.zeros(len(val_loader), dtype=np.float64)\n",
    "        boundary_validation_loss_bt = np.zeros(len(val_loader), dtype=np.float64)\n",
    "        manifold_validation_loss_bt = np.zeros(len(val_loader), dtype=np.float64)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batchnum, data in enumerate(tqdm(val_loader, desc=f\"Epoch {epoch} Validation\")):\n",
    "                data = data.to(device)\n",
    "                assert torch.all((data.y[:, 0] >= 0) & (data.y[:, 0] < 3)), \"Invalid dimension label\"\n",
    "                assert torch.all((data.y[:, 1] >= 0) & (data.y[:, 1] < 3)), \"Invalid boundary label\"\n",
    "                assert torch.all((data.y[:, 2] >= 0) & (data.y[:, 2] < 6)), \"Invalid manifold label\"\n",
    "                assert data.y.dtype == torch.long\n",
    "                out = model(data.x, data.edge_index, data.batch) # this fails with incomplete batches\n",
    "                total_loss, loss_dim, loss_boundary, loss_manifold = lossfunc(out, data.y)\n",
    "\n",
    "                total_validation_loss_bt[batchnum] = total_loss.item()\n",
    "                dim_validation_loss_bt[batchnum] = loss_dim.item()\n",
    "                boundary_validation_loss_bt[batchnum] = loss_boundary.item()\n",
    "                manifold_validation_loss_bt[batchnum] = loss_manifold.item()\n",
    "\n",
    "            mean_val_total_loss = total_validation_loss_bt.mean()\n",
    "            mean_val_dim_loss = dim_validation_loss_bt.mean()\n",
    "            mean_val_boundary_loss = boundary_validation_loss_bt.mean()\n",
    "            mean_val_manifold_loss = manifold_validation_loss_bt.mean()\n",
    "\n",
    "            std_val_total_loss = total_validation_loss_bt.std()\n",
    "            std_val_dim_loss = dim_validation_loss_bt.std()\n",
    "            std_val_boundary_loss = boundary_validation_loss_bt.std()\n",
    "            std_val_manifold_loss = manifold_validation_loss_bt.std()\n",
    "\n",
    "            total_validation_loss[epoch, 0] = mean_val_total_loss\n",
    "            dim_validation_loss[epoch, 0] = mean_val_dim_loss\n",
    "            boundary_validation_loss[epoch, 0] = mean_val_boundary_loss\n",
    "            manifold_validation_loss[epoch, 0] = mean_val_manifold_loss\n",
    "\n",
    "            total_validation_loss[epoch, 1] = std_val_total_loss\n",
    "            dim_validation_loss[epoch, 1] = std_val_dim_loss\n",
    "            boundary_validation_loss[epoch, 1] = std_val_boundary_loss\n",
    "            manifold_validation_loss[epoch, 1] = std_val_manifold_loss\n",
    "\n",
    "        # Check for early stopping\n",
    "        if mean_val_total_loss < best_val_loss - early_stopping_delta:\n",
    "            best_val_loss = mean_val_total_loss\n",
    "            current_patience = early_stopping_patience  # Reset patience \n",
    "        else:\n",
    "            print(f\"No improvement in validation loss: {mean_val_total_loss:.4f} at epoch {epoch}, current patience: {current_patience}\")\n",
    "            current_patience -= 1\n",
    "\n",
    "        if current_patience <= 0:\n",
    "            print(f\"Early stopping at epoch {epoch} with best validation loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "        print(\n",
    "            f\"  training loss: {mean_total_loss:.4f} ± {std_total_loss:.4f}, Dim : {mean_dim_loss:.4f} ± {std_dim_loss:.4f}, Boundary : {mean_boundary_loss:.4f} ± {std_boundary_loss:.4f}, Manifold : {mean_manifold_loss:.4f} ± {std_manifold_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"  validation loss: {mean_val_total_loss:.4f} ± {std_val_total_loss:.4f}, Dim : {mean_val_dim_loss:.4f} ± {std_val_dim_loss:.4f}, Boundary : {mean_val_boundary_loss:.4f} ± {std_val_boundary_loss:.4f}, Manifold : {mean_val_manifold_loss:.4f} ± {std_val_manifold_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        return total_training_loss, dim_training_loss, boundary_training_loss, manifold_training_loss, total_validation_loss, dim_validation_loss, boundary_validation_loss, manifold_validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### test run\n",
    "we might be able to do this with graphgym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    model: GCNModel, test_loader: torch_geometric.loader.DataLoader, device: str\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "\n",
    "            dim, boundary, manifold = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "            dim = torch.argmax(\n",
    "                dim, dim=1\n",
    "            )  # this is enough here, because the encodings are concruential to the indices\n",
    "            boundary = torch.argmax(\n",
    "                boundary, dim=1\n",
    "            )  # this is enough here, because the encodings are concruential to the indices\n",
    "            manifold = torch.argmax(\n",
    "                manifold, dim=1\n",
    "            )  # this is enough here, because the encodings are concruential to the indices\n",
    "            total += 3 * data.y.size(0)\n",
    "            correct += (dim == data.y[:, 0]).sum().item()\n",
    "            correct += (boundary == data.y[:, 1]).sum().item()\n",
    "            correct += (manifold == data.y[:, 2]).sum().item()\n",
    "        accuracy = correct / total\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Actual training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(532432)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### get cuda device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### define the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapath  = os.path.join(os.path.expanduser(\"~\"), \"data\", \"causal_sets\")\n",
    "datapath = os.path.join(\"/mnt\", \"dataLinux\", \"machinelearning_data\", \"QuantumGrav\", \"causal_sets\")\n",
    "files = [\n",
    "    os.path.join(datapath, \"cset_data_min=300_max=650_N=25000_d=2.h5\"),\n",
    "    os.path.join(datapath, \"cset_data_min=300_max=650_N=25000_d=3.h5\"),\n",
    "    os.path.join(datapath, \"cset_data_min=300_max=650_N=25000_d=4.h5\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = CsDataset(\n",
    "    input=files,\n",
    "    output=os.path.join(datapath),\n",
    "    pre_transform=target_shift,  # maybe add some augmentation stuff here later\n",
    "    pre_filter=None,  # Filter data before loading, e.g., based on manifold or boundary or something like that\n",
    "    validate_data=True,  # Validate data after loading\n",
    "    loader=load_graph,  # Custom loader function\n",
    ").shuffle()\n",
    "\n",
    "train_size = int(math.ceil(0.8 * len(dset)))\n",
    "test_size = int(math.ceil(0.1 * len(dset)))\n",
    "val_size = len(dset) - train_size - test_size\n",
    "\n",
    "# train_size = 10*64\n",
    "# test_size = 5*64\n",
    "# val_size= 5*64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dset[0:train_size],\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    # num_workers=4,\n",
    "    # persistent_workers=True,\n",
    "    # prefetch_factor=5,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dset[train_size : train_size + test_size],\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    # num_workers=4,\n",
    "    # persistent_workers=True,\n",
    "    # prefetch_factor=5,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dset[train_size + test_size: train_size + test_size + val_size],\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    # drop_last = True\n",
    "    # num_workers=4,\n",
    "    # persistent_workers=True,\n",
    "    # prefetch_factor=5,\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_loader)}, Test size: {len(test_loader)}, Validation size: {len(val_loader)}\")\n",
    "print(f\"Total dataset size: {len(train_loader.dataset)}, {len(test_loader.dataset)}, {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### define a new model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_node_features = dset[0].x.shape[1]  # Number of node features\n",
    "n_edge_features = dset[0].edge_attr.shape[1] if dset[0].edge_attr is not None else 0  # Number of edge features\n",
    "\n",
    "# normalizer = torch.nn.Identity \n",
    "normalizer = torch.nn.BatchNorm1d \n",
    "conv_layer = GCNConv  # You can change this to GATConv, SAGEConv, etc. as needed\n",
    "\n",
    "conv1 = GCNBlock(\n",
    "    input_dim=n_node_features,\n",
    "    output_dim=128,\n",
    "    dropout=0.3,\n",
    "    batchnorm=normalizer(128),  # Use BatchNorm1d for batch normalization\n",
    "    gcn_type=conv_layer,  # You can change this to GATConv, SAGEConv, etc.\n",
    "    activation=torch.nn.ReLU(),\n",
    "    gcn_kwargs={\"cached\": False, \"bias\": True, \"add_self_loops\": True}  # Example of passing additional arguments to the GCN layer\n",
    ")\n",
    "\n",
    "conv2 = GCNBlock(\n",
    "    input_dim=128,\n",
    "    output_dim=256,\n",
    "    dropout=0.3,\n",
    "    batchnorm=normalizer(256),  # Use BatchNorm1d for batch normalization\n",
    "    gcn_type=conv_layer,  # You can change this to GATConv, SAGEConv, etc.\n",
    "    activation=torch.nn.ReLU(),\n",
    "    gcn_kwargs={\"cached\": False, \"bias\": True, \"add_self_loops\": True}  # Example of passing additional arguments to the GCN layer\n",
    ")\n",
    "\n",
    "conv3 = GCNBlock(\n",
    "    input_dim=256,\n",
    "    output_dim=128,\n",
    "    dropout=0.3,\n",
    "    batchnorm=normalizer(128),  # Use BatchNorm1d for batch normalization\n",
    "    gcn_type=conv_layer,  # You can change this to GATConv, SAGEConv, etc.\n",
    "    activation=torch.nn.ReLU(),\n",
    "    gcn_kwargs={\"cached\": False, \"bias\": True, \"add_self_loops\": True}  # Example of passing additional arguments to the GCN layer\n",
    ")\n",
    "\n",
    "\n",
    "gcn_backbone = GCNBackbone([conv1, conv2, conv3])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ClassifierBlock(\n",
    "    input_dim=128,  # Output dimension of the last GCN layer\n",
    "    output_dim=3,  # Assuming you want to predict manifold_id, boundary_id, and dimension\n",
    "    hidden_dims=[64, 32],  # Example hidden dimensions\n",
    "    manifold_classes=6,\n",
    "    boundary_classes=3,\n",
    "    dimension_classes=3,\n",
    "    activation=torch.nn.ReLU(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_layer = global_mean_pool  # You can change this to global_max_pool, global_add_pool, etc.\n",
    "\n",
    "model = GCNModel(\n",
    "    gcn_net=gcn_backbone,\n",
    "    classifier=classifier,\n",
    "    pooling_layer=pooling_layer,\n",
    "    use_graph_features=False,  # Set to True if you want to use graph features\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn((first.x.shape[0], n_node_features), dtype=torch.float32)  \n",
    "# out = model(x, first.edge_index, first.batch)\n",
    "\n",
    "# dot = torchviz.make_dot(\n",
    "#     out,\n",
    "#     params=dict(model.named_parameters()),\n",
    "#     show_attrs=False,  # Hide detailed attributes\n",
    "#     show_saved=False,  # Hide saved tensors\n",
    "# )\n",
    "\n",
    "# dot.render(\"model_visualization\", format=\"pdf\", cleanup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 120\n",
    "patience = 10\n",
    "current_patience = patience\n",
    "best_val_loss = float(\"inf\")\n",
    "tolerance = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_training_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "dim_training_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "boundary_training_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "manifold_training_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "\n",
    "total_validation_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "dim_validation_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "boundary_validation_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "manifold_validation_loss = np.zeros((epochs, 2), dtype=np.float32)\n",
    "\n",
    "lossfunc = criterion  # Use the custom criterion defined above\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_training_loss_bt = np.zeros(len(train_loader), dtype=np.float64)\n",
    "    dim_training_loss_bt = np.zeros(len(train_loader), dtype=np.float64)\n",
    "    boundary_training_loss_bt = np.zeros(len(train_loader), dtype=np.float64)\n",
    "    manifold_training_loss_bt = np.zeros(len(train_loader), dtype=np.float64)\n",
    "    \n",
    "    model.train()\n",
    "    for batchnum, data in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch} Training\")):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        total_loss, loss_dim, loss_boundary, loss_manifold = lossfunc(out, data.y)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_training_loss_bt[batchnum] = total_loss.item()\n",
    "        dim_training_loss_bt[batchnum] = loss_dim.item()\n",
    "        boundary_training_loss_bt[batchnum] = loss_boundary.item()\n",
    "        manifold_training_loss_bt[batchnum] = loss_manifold.item()\n",
    "\n",
    "    mean_total_loss = total_training_loss_bt.mean()\n",
    "    mean_dim_loss = dim_training_loss_bt.mean()\n",
    "    mean_boundary_loss = boundary_training_loss_bt.mean()\n",
    "    mean_manifold_loss = manifold_training_loss_bt.mean()\n",
    "\n",
    "    std_total_loss = total_training_loss_bt.std()\n",
    "    std_dim_loss = dim_training_loss_bt.std()\n",
    "    std_boundary_loss = boundary_training_loss_bt.std()\n",
    "    std_manifold_loss = manifold_training_loss_bt.std()\n",
    "\n",
    "    total_training_loss[epoch, 0] = mean_total_loss\n",
    "    dim_training_loss[epoch, 0] = mean_dim_loss\n",
    "    boundary_training_loss[epoch, 0] = mean_boundary_loss\n",
    "    manifold_training_loss[epoch, 0] = mean_manifold_loss   \n",
    "\n",
    "    total_training_loss[epoch, 1] = std_total_loss\n",
    "    dim_training_loss[epoch, 1] = std_dim_loss\n",
    "    boundary_training_loss[epoch, 1] = std_boundary_loss\n",
    "    manifold_training_loss[epoch, 1] = std_manifold_loss\n",
    "\n",
    "    # Validation step\n",
    "    total_validation_loss_bt = np.zeros(len(val_loader), dtype=np.float64)\n",
    "    dim_validation_loss_bt = np.zeros(len(val_loader), dtype=np.float64)\n",
    "    boundary_validation_loss_bt = np.zeros(len(val_loader), dtype=np.float64)\n",
    "    manifold_validation_loss_bt = np.zeros(len(val_loader), dtype=np.float64)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batchnum, data in enumerate(tqdm(val_loader, desc=f\"Epoch {epoch} Validation\")):\n",
    "            data = data.to(device)\n",
    "            assert torch.all((data.y[:, 0] >= 0) & (data.y[:, 0] < 3)), \"Invalid dimension label\"\n",
    "            assert torch.all((data.y[:, 1] >= 0) & (data.y[:, 1] < 3)), \"Invalid boundary label\"\n",
    "            assert torch.all((data.y[:, 2] >= 0) & (data.y[:, 2] < 6)), \"Invalid manifold label\"\n",
    "            assert data.y.dtype == torch.long\n",
    "            out = model(data.x, data.edge_index, data.batch) # this fails with incomplete batches\n",
    "            total_loss, loss_dim, loss_boundary, loss_manifold = lossfunc(out, data.y)\n",
    "\n",
    "            total_validation_loss_bt[batchnum] = total_loss.item()\n",
    "            dim_validation_loss_bt[batchnum] = loss_dim.item()\n",
    "            boundary_validation_loss_bt[batchnum] = loss_boundary.item()\n",
    "            manifold_validation_loss_bt[batchnum] = loss_manifold.item()\n",
    "\n",
    "        mean_val_total_loss = total_validation_loss_bt.mean()\n",
    "        mean_val_dim_loss = dim_validation_loss_bt.mean()\n",
    "        mean_val_boundary_loss = boundary_validation_loss_bt.mean()\n",
    "        mean_val_manifold_loss = manifold_validation_loss_bt.mean()\n",
    "\n",
    "        std_val_total_loss = total_validation_loss_bt.std()\n",
    "        std_val_dim_loss = dim_validation_loss_bt.std()\n",
    "        std_val_boundary_loss = boundary_validation_loss_bt.std()\n",
    "        std_val_manifold_loss = manifold_validation_loss_bt.std()\n",
    "\n",
    "        total_validation_loss[epoch, 0] = mean_val_total_loss\n",
    "        dim_validation_loss[epoch, 0] = mean_val_dim_loss\n",
    "        boundary_validation_loss[epoch, 0] = mean_val_boundary_loss\n",
    "        manifold_validation_loss[epoch, 0] = mean_val_manifold_loss\n",
    "\n",
    "        total_validation_loss[epoch, 1] = std_val_total_loss\n",
    "        dim_validation_loss[epoch, 1] = std_val_dim_loss\n",
    "        boundary_validation_loss[epoch, 1] = std_val_boundary_loss\n",
    "        manifold_validation_loss[epoch, 1] = std_val_manifold_loss \n",
    "\n",
    "    # Check for early stopping\n",
    "    if best_val_loss - tolerance <= mean_val_total_loss <= best_val_loss + tolerance\n",
    "        print(f\"No improvement in validation loss: {mean_val_total_loss:.4f} vs {best_val_loss:.4f} at epoch {epoch}, current patience: {current_patience}\")\n",
    "        current_patience -= 1 # model has currently stopped learning - reduce patience\n",
    "    else\n",
    "        current_patience = 10 \n",
    "        \n",
    "    if current_patience <= 0:\n",
    "        print(f\"Early stopping at epoch {epoch} with best validation loss: {best_val_loss:.4f}\") # model has stopped learning for a while --> stop training\n",
    "        break\n",
    "\n",
    "    print(\n",
    "        f\"  training loss: {mean_total_loss:.4f} ± {std_total_loss:.4f}, Dim : {mean_dim_loss:.4f} ± {std_dim_loss:.4f}, Boundary : {mean_boundary_loss:.4f} ± {std_boundary_loss:.4f}, Manifold : {mean_manifold_loss:.4f} ± {std_manifold_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"  validation loss: {mean_val_total_loss:.4f} ± {std_val_total_loss:.4f}, Dim : {mean_val_dim_loss:.4f} ± {std_val_dim_loss:.4f}, Boundary : {mean_val_boundary_loss:.4f} ± {std_val_boundary_loss:.4f}, Manifold : {mean_val_manifold_loss:.4f} ± {std_val_manifold_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "# Loss visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(epochs), total_training_loss[:, 0], label='Mean', color='blue')\n",
    "plt.fill_between(range(epochs), total_training_loss[:, 0] - total_training_loss[:, 1], total_training_loss[:, 0] + total_training_loss[:, 1], color='blue', alpha=0.3, label='±1 Std Dev')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss with Std Dev Shading')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "run model on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dim = []\n",
    "pred_dim = []\n",
    "true_boundary = []\n",
    "pred_boundary = []\n",
    "true_manifold = []\n",
    "pred_manifold = []\n",
    "\n",
    "for data in test_loader: \n",
    "    data = data.to(device)\n",
    "    with torch.no_grad(): \n",
    "        dim_out, boundary_out, manifold_out = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "    pred_dim.extend(dim_out.argmax(dim=1).cpu().numpy())\n",
    "    pred_boundary.extend(boundary_out.argmax(dim=1).cpu().numpy())\n",
    "    pred_manifold.extend(manifold_out.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "    true_dim.extend(data.y[:, 0].cpu().numpy())\n",
    "    true_boundary.extend(data.y[:, 1].cpu().numpy())\n",
    "    true_manifold.extend(data.y[:, 2].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Per-label Accuracy\n",
    "cummulative accuracy over all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "\n",
    "        dim, boundary, manifold = model(data.x, data.edge_index, data.batch)\n",
    "        \n",
    "        dim = torch.argmax(dim, dim=1) # this is enough here, because the encodings are concruential to the indices\n",
    "        boundary = torch.argmax(boundary, dim=1) # this is enough here, because the encodings are concruential to the indices\n",
    "        manifold = torch.argmax(manifold, dim=1) # this is enough here, because the encodings are concruential to the indices\n",
    "        total += 3*data.y.size(0)\n",
    "        correct += (dim == data.y[:, 0]).sum().item()\n",
    "        correct += (boundary == data.y[:, 1]).sum().item()\n",
    "        correct += (manifold == data.y[:, 2]).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "# Per-task Accuracy \n",
    "which task is correct how often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dim_correct = 0\n",
    "boundary_correct = 0\n",
    "manifold_correct = 0\n",
    "total_samples = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        dim_out, boundary_out, manifold_out = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "        dim_pred = torch.argmax(dim_out, dim=1)\n",
    "        boundary_pred = torch.argmax(boundary_out, dim=1)\n",
    "        manifold_pred = torch.argmax(manifold_out, dim=1)\n",
    "\n",
    "        total_samples += data.y.size(0)\n",
    "\n",
    "        dim_correct += (dim_pred == data.y[:, 0]).sum().item()\n",
    "        boundary_correct += (boundary_pred == data.y[:, 1]).sum().item()\n",
    "        manifold_correct += (manifold_pred == data.y[:, 2]).sum().item()\n",
    "\n",
    "dim_acc = dim_correct / total_samples\n",
    "boundary_acc = boundary_correct / total_samples\n",
    "manifold_acc = manifold_correct / total_samples\n",
    "overall_acc = (dim_correct + boundary_correct + manifold_correct) / (3 * total_samples)\n",
    "\n",
    "print(f\"Dimension Accuracy: {dim_acc:.4f}\")\n",
    "print(f\"Boundary Accuracy: {boundary_acc:.4f}\")\n",
    "print(f\"Manifold Accuracy: {manifold_acc:.4f}\")\n",
    "print(f\"Overall Accuracy: {overall_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "# Per-sample accuracy\n",
    "\n",
    "how often are they all correct for a given sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_all = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        dim_out, boundary_out, manifold_out = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "        dim_pred = torch.argmax(dim_out, dim=1)\n",
    "        boundary_pred = torch.argmax(boundary_out, dim=1)\n",
    "        manifold_pred = torch.argmax(manifold_out, dim=1)\n",
    "\n",
    "        match_all = (\n",
    "            (dim_pred == data.y[:, 0]) &\n",
    "            (boundary_pred == data.y[:, 1]) &\n",
    "            (manifold_pred == data.y[:, 2])\n",
    "        )\n",
    "        correct_all += match_all.sum().item()\n",
    "        total += data.y.size(0)\n",
    "\n",
    "strict_accuracy = correct_all / total\n",
    "print(f\"Strict (all-correct) Accuracy: {strict_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "we build one per task: dimension, boundary, manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \n",
    "\n",
    "cm_dim = confusion_matrix(true_dim, pred_dim)\n",
    "cm_boundary = confusion_matrix(true_boundary, pred_boundary)\n",
    "cm_manifold = confusion_matrix(true_manifold, pred_manifold)\n",
    "\n",
    "ConfusionMatrixDisplay(cm_dim, display_labels=[0,1,2]).plot() \n",
    "ConfusionMatrixDisplay(cm_boundary, display_labels=[0,1,2]).plot() \n",
    "ConfusionMatrixDisplay(cm_manifold, display_labels=[0,1,2,3,4,5]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "# Precision/Recall per class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "| Term |\tMeaning | \n",
    "| -------|---------- | \n",
    "| TP | True Positives: correctly predicted samples of the class | \n",
    "| FP | False Positives: predicted as the class, but actually something else | \n",
    "| FN | False Negatives: actual class samples predicted as something else | \n",
    "\n",
    "\n",
    "**Precision** \n",
    "```math\n",
    "Precision = \\frac{TP}{TP + FP} \\; \\in [0, 1]\n",
    "```\n",
    "High precision -> few false positives\n",
    "\n",
    "**Recall** \n",
    "```math \n",
    "Recall = \\frac{TP}{TP + FN}  \\; \\in [0, 1]\n",
    "```\n",
    "High recall -> few false negatives \n",
    "\n",
    "We want both to be high, i.e., as close 1 as possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "precision_dim_per_class = precision_score(true_dim, pred_dim, average = None) \n",
    "recall_dim_per_class = recall_score(true_dim, pred_dim, average = None)\n",
    "\n",
    "\n",
    "precision_boundary_per_class = precision_score(true_boundary, pred_boundary, average=None)\n",
    "recall_boundary_per_class = recall_score(true_boundary, pred_boundary, average=None)\n",
    "\n",
    "precision_manifold_per_class = precision_score(true_manifold, pred_manifold, average = None)\n",
    "recall_manifold_per_class = recall_score(true_manifold, pred_manifold, average = None)\n",
    "\n",
    "\n",
    "# average over all classes\n",
    "precision_dim_avg = precision_score(true_dim, pred_dim, average = 'macro') \n",
    "recall_dim_avg = recall_score(true_dim, pred_dim, average = 'macro')\n",
    "\n",
    "precision_boundary_avg = precision_score(true_boundary, pred_boundary, average='macro')\n",
    "recall_boundary_avg = recall_score(true_boundary, pred_boundary, average='macro')\n",
    "\n",
    "precision_manifold_avg = precision_score(true_manifold, pred_manifold, average = 'macro')\n",
    "recall_manifold_avg = recall_score(true_manifold, pred_manifold, average = 'macro')\n",
    "\n",
    "\n",
    "# weighted average over classes - weight with support = number of samples per class\n",
    "precision_dim_weighted = precision_score(true_dim, pred_dim, average = 'weighted') \n",
    "recall_dim_weighted = recall_score(true_dim, pred_dim, average = 'weighted')\n",
    "\n",
    "precision_boundary_weighted = precision_score(true_boundary, pred_boundary, average='weighted')\n",
    "recall_boundary_weighted = recall_score(true_boundary, pred_boundary, average='weighted')\n",
    "\n",
    "precision_manifold_weighted = precision_score(true_manifold, pred_manifold, average = 'weighted')\n",
    "recall_manifold_weighted = recall_score(true_manifold, pred_manifold, average = 'weighted')\n",
    "\n",
    "print(\"Precision and Recall per class for Dimension:\")\n",
    "for i in range(3):\n",
    "    print(f\"Class {i}: Precision = {precision_dim_per_class[i]:.4f}, Recall = {recall_dim_per_class[i]:.4f}\")\n",
    "\n",
    "print(\"Precision and Recall per class for Boundary: \")\n",
    "for i in range(3): \n",
    "    print(f\"Call {i}: Precision = {precision_boundary_per_class[i]: 4f}, Recall = {recall_boundary_per_class[i]:.4f}\")\n",
    "\n",
    "print(\"Precision and Recall per class for Manifold: \")\n",
    "for i in range(3): \n",
    "    print(f\"Call {i}: Precision = {precision_manifold_per_class[i]: 4f}, Recall = {recall_manifold_per_class[i]:.4f}\")\n",
    "\n",
    "print(f\"Average Precision (Macro): {precision_dim_avg:.4f}, Average Recall (Macro): {recall_dim_avg:.4f}\")\n",
    "print(f\"Average Precision (Weighted): {precision_dim_weighted:.4f}, Average Recall (Weighted): {recall_dim_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "# F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    " The F1 score is the harmonic mean of precision and recall. It combines them into a single number that balances both, especially when you want a trade-off between the two:\n",
    "\n",
    " ```math\n",
    " F1 = 2 * \\frac{\n",
    "    Precision * Recall\n",
    " }{Precision + Recall} \n",
    " ```\n",
    "\n",
    " The harmonic mean penalizes extreme imbalance more than the arithmetic mean.\n",
    "E.g., if precision = 1.0 and recall = 0.0, the F1 score is 0.0 — you can't ignore one metric entirely.\n",
    "\n",
    "macro F1: simple mean across all classes (treats each class equally)\n",
    "\n",
    "weighted F1: mean weighted by number of samples per class (favours common classes)\n",
    "\n",
    "The F1 score is especially valuable when you're optimizing for model robustness across all classes, particularly if one class is harder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_dim = f1_score(true_dim, pred_dim, average=None)  # Per-class F1\n",
    "f1_dim_macro = f1_score(true_dim, pred_dim, average='macro')  # Unweighted average\n",
    "f1_dim_weighted = f1_score(true_dim, pred_dim, average='weighted')  # Weighted by class support\n",
    "\n",
    "f1_boundary = f1_score(true_boundary, pred_boundary, average=None)  # Per-class F1\n",
    "f1_boundary_macro = f1_score(true_boundary, pred_boundary, average='macro')  # Unweighted average\n",
    "f1_boundary_weighted = f1_score(true_boundary, pred_boundary, average='weighted')  # Weighted by class support\n",
    "\n",
    "f1_manifold = f1_score(true_manifold, pred_manifold, average = None)\n",
    "f1_manifold_macro = f1_score(true_manifold, pred_manifold, average = 'macro')\n",
    "f1_manifold_weighted = f1_score(true_manifold, pred_manifold, average = 'weighted')\n",
    "\n",
    "print(\"F1 score per class for dimension:\")\n",
    "for i in range(3):\n",
    "    print(f\"  class {i}: {f1_dim[i]}\")\n",
    "print(\"base average of F1 score for dimension: \", f1_dim_macro)\n",
    "print(\"weighted average of F1 score for dimension: \", f1_dim_weighted)\n",
    "\n",
    "print(\"F1 score per class for boundary:\")\n",
    "for i in range(3):\n",
    "    print(f\"  class {i}: {f1_boundary[i]}\")\n",
    "print(\"base average of F1 score for boundary: \", f1_boundary_macro)\n",
    "print(\"weighted average of F1 score for boundary: \", f1_boundary_weighted)\n",
    "\n",
    "print(\"F1 score per class for manifold:\")\n",
    "for i in range(3):\n",
    "    print(f\"  class {i}: {f1_manifold[i]}\")\n",
    "print(\"base average of F1 score for manifold: \", f1_manifold_macro)\n",
    "print(\"weighted average of F1 score for manifold: \", f1_manifold_weighted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"Adam\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"weight_decay_rate\": 5e-4,\n",
    "    }, \n",
    "    \"training\": {\n",
    "        \"epochs\": epochs,\n",
    "        \"patience\": patience,\n",
    "        \"tolerance\": tolerance, \n",
    "        \"batchsize\": 128\n",
    "    }, \n",
    "    \"model\": {\n",
    "        \"Backbone\": {\n",
    "            {\n",
    "                \"input\": 4,\n",
    "                \"output\":128,\n",
    "                \"type\": \"GCNConv\",\n",
    "                \"activation\": \"relu\",\n",
    "                \"dropout\": 0.3,\n",
    "                \"normalizer\": \"batchnorm\",\n",
    "                \"kwargs\": {\"cached\": False, \"bias\": True, \"add_self_loops\": True}\n",
    "            },\n",
    "            {\n",
    "                \"input\": 128,\n",
    "                \"output\":256,\n",
    "                \"type\": \"GCNConv\",\n",
    "                \"activation\": \"relu\",\n",
    "                \"dropout\": 0.3,\n",
    "                \"normalizer\": \"batchnorm\",\n",
    "                \"kwargs\": {\"cached\": False, \"bias\": True, \"add_self_loops\": True}\n",
    "            },\n",
    "            {\n",
    "                \"input\": 256,\n",
    "                \"output\":128,\n",
    "                \"type\": \"GCNConv\",\n",
    "                \"activation\": \"relu\",\n",
    "                \"dropout\": 0.3,\n",
    "                \"normalizer\": \"batchnorm\",\n",
    "                \"kwargs\": {\"cached\": False, \"bias\": True, \"add_self_loops\": True}\n",
    "            },\n",
    "\n",
    "        },\n",
    "        \"Classifier\": {\n",
    "            \"input_dim\": 128,  # Output dimension of the last GCN layer\n",
    "            \"output_dim\": 3,  # Assuming you want to predict manifold_id, boundary_id, and dimension\n",
    "            \"hidden_dims\": [64, 32],  # Example hidden dimensions\n",
    "            \"manifold_classes\": 6,\n",
    "            \"boundary_classes\": 3,\n",
    "            \"dimension_classes\": 3,\n",
    "            \"activation\": \"relu\",\n",
    "        },\n",
    "        \"GraphFeatures\": None,\n",
    "    }, \n",
    "    \"data\": {\n",
    "        \"node_features\": [\"in_degree\", \"out_degree\", \"max_path_future\", \"max_path_past\"], \n",
    "        \"graph_features\": None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(datapath, \"models\", \"gcn_model_simple_unoptimized.pt\")\n",
    "model_dir = os.path.abspath(os.path.dirname(model_path))\n",
    "os.makedirs(model_dir, exist_ok = False)\n",
    "\n",
    "with open(os.path.join(model_dir, \"hyperparameters.json\"), \"w\") as file: \n",
    "    json.dump(hyperparams, \"hyperparameters.json\")\n",
    "\n",
    "torch.save(model.to('cpu'), os.path.abspath(model_path, \"model.pth\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "- [ ] gpu utilization low \n",
    "- [ ] parallel data processing and -loading (related to dataprocessing)\n",
    "- [x] implement early stopping \n",
    "- [ ] performance improvements \n",
    "- [x] make sure the model trains\n",
    "- [ ] experiments with different models \n",
    "- [ ] add support for initialization of parameters\n",
    "- [ ] add support for single graph eval in a better way\n",
    "- [x] fix indexing problem\n",
    "- [ ] parallelization of training on multiple GPUs\n",
    "- [ ] add hyperparameter output to file together with model for documentation\n",
    "- [ ] add quality metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
