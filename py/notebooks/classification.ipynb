{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "We are leveraging pytorch's data loading capabilities. This is currently only done on a single process because this is a prototype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.multiprocessing.set_start_method('spawn', force=True)  # For multiprocessing support\n",
    "from torch_geometric.loader import DataLoader \n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from collections.abc import Callable\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "The function `load_graph` constructs a `pytorch_geometric.Data` object from the data in a hdf5 file and selects some generated data in it to include as node features. This is still very basic. We will use this function to create build a custom Dataset for our graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(\n",
    "    f: h5py.File,\n",
    "    idx: int,\n",
    "    float_dtype: torch.dtype,\n",
    "    int_dtype: torch.dtype,\n",
    "    validate: bool = False,\n",
    ") -> Data:\n",
    "    \"\"\"Load a single graph from an hdf5 file into a pytorch_geometric Data object.\n",
    "\n",
    "    Args:\n",
    "        f (h5py.File): The hdf5 file containing the graph data.\n",
    "        idx (int): The index of the graph to load from the hdf5 file. The index is shared across all datasets in the hdf5 file, i.e., the same index corresponds to the same graph property across all datasets.\n",
    "        float_dtype (torch.dtype): The data type to use for floating point tensors.\n",
    "        int_dtype (torch.dtype): The data type to use for integer tensors.\n",
    "        validate (bool, optional): Validate the created `Data` object, i.e., making sure that it represents a valid graph. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Data: create a single pytorch_geometric Data object from the hdf5 file `f` at index `idx`.\n",
    "    \"\"\"\n",
    "    # In general: these data are julia generated, which is column major and has the \n",
    "    # data element index at the last dimension, while python is row major and has the\n",
    "    # data element index at the first dimension, consequently.\n",
    "\n",
    "    # pull out the adjacency matrix for graph index idx,\n",
    "    # make a torch tensor from it, and convert it to edge indices and edge weights which\n",
    "    # pytorch_geometric uses to define the graph structure.\n",
    "    adj_raw = f[\"adjacency_matrix\"][idx, :, :]\n",
    "    adj_matrix = torch.tensor(adj_raw, dtype=float_dtype)\n",
    "    edge_index, edge_weight = dense_to_sparse(adj_matrix)\n",
    "    adj_matrix = adj_matrix.to_sparse()\n",
    "\n",
    "    # Load node features. We are only using degree information and path lengths for now.\n",
    "    node_features = []\n",
    "\n",
    "    # Degree information\n",
    "    in_degrees = torch.tensor(f[\"in_degrees\"][idx, :], dtype=float_dtype).unsqueeze(1)\n",
    "    out_degrees = torch.tensor(f[\"out_degrees\"][idx, :], dtype=float_dtype).unsqueeze(1)\n",
    "    node_features.extend([in_degrees, out_degrees])\n",
    "\n",
    "    # Path lengths\n",
    "    max_path_future = torch.tensor(\n",
    "        f[\"max_path_lengths_future\"][idx, :], dtype=float_dtype\n",
    "    ).unsqueeze(1) # make this a (num_nodes, 1) tensor\n",
    "\n",
    "    max_path_past = torch.tensor(\n",
    "        f[\"max_path_lengths_past\"][idx, :], dtype=float_dtype\n",
    "    ).unsqueeze(1) # make this a (num_nodes, 1) tensor\n",
    "    node_features.extend([max_path_future, max_path_past])\n",
    "\n",
    "    # Concatenate all node features\n",
    "    x = torch.cat(node_features, dim=1)\n",
    "\n",
    "    # Load graph-level features (targets for classification)\n",
    "    manifold_id = int(f[\"manifold_ids\"][idx])\n",
    "    boundary_id = int(f[\"boundary_ids\"][idx])\n",
    "    relation_dim = torch.tensor(f[\"relation_dim\"][idx], dtype=float_dtype)\n",
    "    atom_count = torch.tensor(f[\"atom_count\"][idx], dtype=int_dtype)\n",
    "    num_sources = torch.tensor(f[\"num_sources\"][idx], dtype=int_dtype)\n",
    "    num_sinks = torch.tensor(f[\"num_sinks\"][idx], dtype=int_dtype)\n",
    "    dimension = int(f[\"dimension\"][()])\n",
    "\n",
    "    # matrices as graph attributes\n",
    "    link_matrix = torch.tensor(\n",
    "        f[\"link_matrix\"][idx, :, :], dtype=float_dtype\n",
    "    ).to_sparse()\n",
    "\n",
    "    past_relations = torch.tensor(\n",
    "        f[\"past_relations\"][idx, :, :], dtype=float_dtype\n",
    "    ).to_sparse()\n",
    "\n",
    "    future_relations = torch.tensor(\n",
    "        f[\"future_relations\"][idx, :, :], dtype=float_dtype\n",
    "    ).to_sparse()\n",
    "\n",
    "    # Create Data object\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_weight.unsqueeze(1)\n",
    "        if edge_weight.numel() > 0\n",
    "        else None,  # Not sure if this is a good idea need to add edge attributes if possible\n",
    "        \n",
    "        # the classification target. This concatenates everything into a single 1D tensor. \n",
    "        y=torch.tensor([manifold_id, boundary_id, dimension], dtype=torch.long),\n",
    "        # Graph-level attributes which might be useful later to the model\n",
    "        manifold_id=manifold_id,\n",
    "        boundary_id=boundary_id,\n",
    "        relation_dim=relation_dim,\n",
    "        dimension=dimension,\n",
    "        atom_count=atom_count,\n",
    "        num_sources=num_sources,\n",
    "        num_sinks=num_sinks,\n",
    "        # Additional matrices as graph attributes\n",
    "        adjacency_matrix=adj_matrix,\n",
    "        link_matrix=link_matrix,\n",
    "        past_relations=past_relations,\n",
    "        future_relations=future_relations,\n",
    "    )\n",
    "\n",
    "    if validate:  # validate the data object if desired.\n",
    "        data.validate()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "The function `target_shift` will be used as a simple `pre_transform` for the pytorch dataset. This will trigger the processing of the entire dataset,i.e., each graph therein with this function. We can do a lot more complex things here in the future, if we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_shift(data, manifold_classes=6, boundary_classes=3, dimension_classes=3) -> Data:\n",
    "    \"\"\"Shift the target values in the data object to a 0-based index and combine them into a single tensor.\n",
    "\n",
    "    Args:\n",
    "        data (Data): The input data object.\n",
    "        manifold_classes (int, optional): The number of manifold classes. Defaults to 6.\n",
    "        boundary_classes (int, optional): The number of boundary classes. Defaults to 3.\n",
    "        dimension_classes (int, optional): The number of dimension classes. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        Data: The modified data object with shifted targets.\n",
    "    \"\"\"\n",
    "    manifold_id = data.y[0].long() - 1  # convert to 0 based from 1 based: 1,2,3,4,5,6 -> 0,1,2,3,4,5\n",
    "    boundary_id = data.y[1].long() - 1  # convert to 0 based from 1 based: 1,2,3 -> 0,1,2\n",
    "    dimension = data.y[2].long() - 2  # convert to 0 based -> 2D is the lowest we can have, so 2, 3, 4 -> 0, 1, 2\n",
    "    \n",
    "    # put the manifold_id, boundary_id, and dimension into a single tensor used as a target\n",
    "    data.y_original = data.y  # Keep original for reference\n",
    "    data.y = torch.tensor([[dimension, boundary_id, manifold_id],], dtype=torch.long)\n",
    "\n",
    "    # add some more metadata\n",
    "    data.target_info={\n",
    "        'manifold_classes': manifold_classes,\n",
    "        'boundary_classes': boundary_classes,\n",
    "        'dimension_classes': dimension_classes,\n",
    "        'dimension_offset': 0,   \n",
    "        'boundary_offset': dimension_classes,\n",
    "        'manifold_offset': dimension_classes + boundary_classes, \n",
    "        'total_classes': manifold_classes + boundary_classes + dimension_classes,\n",
    "    }\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Implement a custom Dataset class \n",
    "\n",
    "This implements a custom Dataset as described [here](https://pytorch-geometric.readthedocs.io/en/2.5.3/tutorial/create_dataset.html). We are creating this as a dataset that lives on disk and loads examples dynamically instead of building an `InMemoryDataset` that lives only in RAM.  A Dataset becomes a dataset by implementing the `get` and `len` functions. Here, we overwrite also the `process ` function because this is particular to our data storage system. Here, because this is a prototype, we are also processing every data example into a single pytorch `.pt` file, which essentially python-pickles the `Data` objects created by `load_graph`. This is not optimal performance-wise, but works in a very simple way without having to handle perpetually open files accessed from multiple places, and we don´t have to immediatelly take care of caching and everything. For the current prototype, this is good enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsDataset(Dataset):\n",
    "    \"\"\"Custom dataset implementation for loading and processing causal set data from hdf5 files.\n",
    "\n",
    "    Args:\n",
    "        Dataset (pytorch_geometric.data.Dataset): Base class for PyTorch Geometric datasets.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input: list[str],\n",
    "        output: str,  # = root directory for processed data\n",
    "        transform: Callable[[Data], Data] | None = None,\n",
    "        pre_transform: Callable[[Data], Data] | None = None,\n",
    "        pre_filter: Callable[[Data], Data] | None = None,\n",
    "        validate_data: bool = False,\n",
    "        loader: Callable[[h5py.File, torch.dtype, torch.dtype, bool], Data] = load_graph,\n",
    "    ):\n",
    "        \"\"\"Create a new CsDataset instance for loading and processing causal set data from hdf5 files.\n",
    "\n",
    "        Args:\n",
    "            input (list[str]): List of input file paths. We can have an arbitrary number of input files, so this is a list of file paths.\n",
    "            output (str): Root directory for processed data.\n",
    "            pre_transform (Callable[[Data], Data] | None, optional): Optional transform to be applied to the data before loading. Defaults to None. These are only applied once to the whole dataset. Afterwards the data is stored in processed form and is only loaded from disk.\n",
    "            pre_filter (Callable[[Data], Data] | None, optional): Optional filter to be applied to the data before loading. Defaults to None. These are only applied once to the whole dataset.\n",
    "            validate_data (bool, optional): Whether to validate the data after loading. Defaults to False.\n",
    "            loader (Callable[[h5py.File, torch.dtype, torch.dtype, bool], Data], optional): Function to load the data from the hdf5 file. Defaults to load_graph.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input files are not valid.\n",
    "            ValueError: If the output directory is not valid.\n",
    "            FileNotFoundError: If the input files do not exist.\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        self._num_samples = None\n",
    "        self.validate_data = validate_data\n",
    "        self.root = output\n",
    "        self.loader = loader\n",
    "        self.root = output\n",
    "\n",
    "        if len(self.processed_file_names) > 0:\n",
    "            self.num_samples = len(self.processed_file_names)\n",
    "\n",
    "            if len(self.processed_file_names) == 0:\n",
    "                raise ValueError(\"No processed data found in the output directory.\")\n",
    "            \n",
    "            self._load_metadata()\n",
    "\n",
    "            self._num_samples = len(self.processed_file_names)\n",
    "        else:\n",
    "            if input is None or len(input) == 0:\n",
    "                raise ValueError(\"Input files must be provided for processing.\")\n",
    "            \n",
    "            with h5py.File(input[0], \"r\") as f:\n",
    "                self.manifold_codes = f[\"manifold_codes\"][()]\n",
    "                self.manifold_names = f[\"manifolds\"][()]\n",
    "                self.boundaries = f[\"boundaries\"][()]\n",
    "                self.boundary_codes = f[\"boundary_codes\"][()]\n",
    "\n",
    "            self._num_samples = 0\n",
    "            for file in self.input:\n",
    "                if not os.path.exists(file):\n",
    "                    raise FileNotFoundError(f\"Input file {file} does not exist.\")\n",
    "                with h5py.File(file, \"r\") as f:\n",
    "                    self._num_samples += f[\"num_causal_sets\"][()]\n",
    "                    print(f\"Processing file: {file}, current number of samples: {self._num_samples}\")\n",
    "\n",
    "        super().__init__(output, transform, pre_transform, pre_filter)\n",
    "\n",
    "\n",
    "    def _load_metadata(self): \n",
    "        \"\"\"Get the metadata from the processed directory, which contains manifold codes, manifold names, boundaries, and boundary codes.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the metadata file does not exist.\n",
    "        \"\"\"\n",
    "        metadata_path = os.path.join(self.processed_dir, \"metadata.json\")\n",
    "        if not os.path.exists(metadata_path):\n",
    "            raise FileNotFoundError(f\"Metadata file {metadata_path} does not exist.\")\n",
    "        \n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "            self.manifold_codes = metadata[\"manifold_codes\"]\n",
    "            self.manifold_names = metadata[\"manifolds\"]\n",
    "            self.boundaries = metadata[\"boundaries\"]\n",
    "            self.boundary_codes = metadata[\"boundary_codes\"]\n",
    "\n",
    "    @property\n",
    "    def raw_paths(self) -> list[str]:\n",
    "        \"\"\"Get the raw paths of the input files. We can have an arbitrary number of input files, so this is a list of file paths.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of raw file paths.\n",
    "        \"\"\"\n",
    "        return self.input\n",
    "\n",
    "    @property\n",
    "    def output(self) -> str:\n",
    "        \"\"\"Get the output directory for processed files.\n",
    "\n",
    "        Returns:\n",
    "            str: The output directory path.\n",
    "        \"\"\"\n",
    "        return self.root\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> list[str]:\n",
    "        \"\"\"Get the raw file names of the input files.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of raw file names.\n",
    "        \"\"\"\n",
    "        return [os.path.basename(f) for f in self.input]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> list[str]:\n",
    "        \"\"\"Get the processed file names in the output directory.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of processed file names.\n",
    "        \"\"\"\n",
    "        if os.path.isdir(self.processed_dir) is False: \n",
    "            return []\n",
    "\n",
    "        all_files = os.listdir(self.processed_dir)\n",
    "        return [\n",
    "            f\n",
    "            for f in all_files\n",
    "            if f.startswith(\"data_\") and f.endswith(\".pt\")\n",
    "        ]\n",
    "\n",
    "    def process(self) -> None:\n",
    "        \"\"\"Process the input files and save the processed data to the output directory as PyTorch .pt files, one file per sample.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If an input file does not exist.\n",
    "        \"\"\"\n",
    "        # Convert NumPy arrays to Python lists for JSON serialization\n",
    "        if not os.path.exists(self.processed_dir) or len(self.processed_file_names) == 0: \n",
    "            d = {\n",
    "                \"manifold_codes\": [v.item() for v in self.manifold_codes],\n",
    "                \"manifolds\": [str(m) for m in self.manifold_names],\n",
    "                \"boundaries\": [str(m) for m in self.boundaries],\n",
    "                \"boundary_codes\": [v.item() for v in self.boundary_codes],\n",
    "            }\n",
    "\n",
    "            with open(os.path.join(self.processed_dir, \"metadata.json\"), \"w\") as f:\n",
    "                json.dump(d, f)\n",
    "\n",
    "            file_index = 0\n",
    "            for file in self.raw_paths:\n",
    "                if not os.path.exists(file):\n",
    "                    raise FileNotFoundError(f\"Input file {file} does not exist.\")\n",
    "                with h5py.File(file, \"r\") as f:\n",
    "                    for idx in tqdm(range(f[\"num_causal_sets\"][()])):\n",
    "                        data = self.loader(\n",
    "                            f,\n",
    "                            idx,\n",
    "                            float_dtype=torch.float32,\n",
    "                            int_dtype=torch.int64,\n",
    "                            validate=self.validate_data,\n",
    "                        )\n",
    "                        if self.pre_filter is not None:\n",
    "                            if not self.pre_filter(data):\n",
    "                                continue\n",
    "                        if self.pre_transform is not None:\n",
    "                            data = self.pre_transform(data)\n",
    "                        torch.save(data.to('cpu'), os.path.join(self.processed_dir, f\"data_{file_index}.pt\"))\n",
    "                        file_index += 1\n",
    "\n",
    "    def len(self)-> int:\n",
    "        \"\"\"Get the length of the dataset as the number of samples.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of samples in the dataset. \n",
    "        \"\"\"\n",
    "        return self._num_samples\n",
    "\n",
    "    def get(self, idx: int) -> Data:\n",
    "        \"\"\" Retrieve a single sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            pytorch_geometric.data.Data: A single data object corresponding to the index `idx`, i.e., a single sample from the dataset.\n",
    "        \"\"\"\n",
    "        data = torch.load(os.path.join(self.processed_dir, f\"data_{idx}.pt\"), weights_only=False)\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "With the data code taken care of, we can go on to build the actual model code. \n",
    "The operators beside `GCNConv` are not used in the code, but are imported for potential use in the future with experimentations. The same holds for the pooling operations besides `global_mean_pool` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch_geometric\n",
    "import os\n",
    "from torch.nn import Linear \n",
    "from torch_geometric.nn.conv import GCNConv, GATConv, SAGEConv, GraphConv, GATv2Conv  \n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool, SAGPooling, Set2Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "We are using graph convolutional network operators here, i.e., we are not dealing with anything transformer like or so. For the simple experiments we are doing here at the moment, the architecture is very simple, but might have to become more complex later when we are dealing with larger, more complex graphs. See the diagram below for how it works: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "![gcnblock](img/gcnblock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "This comes a little out of the blue, especially since it's not clear how many layers we should use here. The idea is the following, however: \n",
    "- the graph convolutional layers build new presentations based on local neighborhoods that are successively aggregated until the entire graph is covered, layer by layer. \n",
    "- this creates feature squashing for deep systems, so we cannot make them too deep \n",
    "- deep nets become difficult to train at some point, so it's a good idea to put in some gradient regularization like batch-norm. This comes before the activation to not have strongly fluctuating values amplified further (although for relu that shouldn't be much of a problem). \n",
    "- the skip connection counteracts the feature squashing by reinjecting unprocessed neighborhood information. This helps with avoiding vanishing gradients\n",
    "- dropout helps with generalization and counteracts overfitting for overparameterized models \n",
    "\n",
    "As for how many of these we should have - \n",
    "- one or two is too little probably \n",
    "- 4 or 5 are too many because squashing becomes stronger with growing depth\n",
    "- 3 seems to be a good starting point\n",
    "\n",
    "\n",
    "The skip connection is placed a little oddly perhaps, it might make more sense to put it around the whole chain of blocks? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBlock(torch.nn.Module):\n",
    "    \"\"\"Graph Convolutional Network (GCN) Block.\n",
    "\n",
    "    Args:\n",
    "        torch (Module): PyTorch module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        output_dim,\n",
    "        dropout=0.5,\n",
    "        gcn_type=GCNConv,\n",
    "        normalizer=torch.nn.Identity,\n",
    "        activation=F.relu,\n",
    "        gcn_kwargs=None,\n",
    "    ):\n",
    "        \"\"\"Initialize the GCNBlock.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Input feature dimension.\n",
    "            output_dim (int): Output feature dimension.\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.5.\n",
    "            gcn_type (torch.nn.Module, optional): Type of GCN layer to use. Defaults to GCNConv.\n",
    "            normalizer (torch.nn.Module, optional): Batch normalization layer. Defaults to torch.nn.Identity. with identity given, no normalization is applied.\n",
    "            activation (Callable[[Tensor], Tensor], optional): Activation function. Defaults to F.relu.\n",
    "            gcn_kwargs (dict, optional): Additional arguments for the GCN layer constructor. Defaults to None.\n",
    "        \"\"\"\n",
    "        super(GCNBlock, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.gcn_type = gcn_type\n",
    "        self.conv = gcn_type(\n",
    "            input_dim, output_dim, **(gcn_kwargs if gcn_kwargs else {})\n",
    "        )\n",
    "        self.activation = activation\n",
    "        self.normalizer = normalizer\n",
    "\n",
    "        if input_dim != output_dim:\n",
    "            # Use 1x1 convolution for projection into a different dimensional space to enable skip connections\n",
    "            # we are using a linear layer without bias\n",
    "            self.projection = Linear(input_dim, output_dim, bias=False)\n",
    "        else:\n",
    "            self.projection = torch.nn.Identity()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.tensor,\n",
    "        edge_index: torch.tensor,\n",
    "        edge_weight: torch.tensor = None,\n",
    "        kwargs: dict = None,\n",
    "    ):\n",
    "        \"\"\"Forward pass for the GCNBlock.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features.\n",
    "            edge_index (Tensor): Graph connectivity in COO format.\n",
    "            edge_weight (Tensor, optional): Edge weights. Defaults to None.\n",
    "            kwargs (dict, optional): Additional arguments for the GCN layer call. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output node features.\n",
    "        \"\"\"\n",
    "        x_res = x\n",
    "        x = self.conv(\n",
    "            x, edge_index, edge_weight=edge_weight, **(kwargs if kwargs else {})\n",
    "        )  # Apply the GCN layer\n",
    "        x = self.normalizer(\n",
    "            x,\n",
    "        )  # this is a no-op if batch normalization is not used\n",
    "        x = self.activation(x)\n",
    "        x = x + self.projection(\n",
    "            x_res\n",
    "        )  # skip connection. this is a no-op if input_dim == output_dim\n",
    "        x = F.dropout(\n",
    "            x, p=self.dropout, training=self.training\n",
    "        )  # this is only applied during training and acts as a noise regularization\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "We collect a chain of `GCNBlock`s into a `GCNBackbone` structure. All this does is to apply a sequence of `GCNBlock`s to the data, one after the other. We could think about adding a final skip connection here too if the backbone becomes too deep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBackbone(torch.nn.Module): \n",
    "    \"\"\"Graph Convolutional Network Backbone.\n",
    "\n",
    "    Args:\n",
    "        torch (Module): PyTorch module.\n",
    "    \"\"\"\n",
    "    def __init__(self, gcn_net: list[GCNBlock]):\n",
    "        \"\"\"Initialize the GCNBackbone.\n",
    "\n",
    "        Args:\n",
    "            gcn_net (list[GCNBlock]): List of GCNBlock layers.\n",
    "        \"\"\"\n",
    "        super(GCNBackbone, self).__init__()\n",
    "        self.gcn_net = torch.nn.ModuleList(gcn_net)\n",
    "\n",
    "    def forward(self, x: torch.tensor, edge_index: torch.tensor, edge_weight: torch.tensor = None, gcn_kwargs: dict = None):\n",
    "        \"\"\"Forward pass for the GCNBackbone.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features.\n",
    "            edge_index (Tensor): Graph connectivity in COO format.\n",
    "            edge_weight (Tensor, optional): Edge weights. Defaults to None.\n",
    "            gcn_kwargs (dict, optional): Additional arguments for the GCN layer call. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output node features.\n",
    "        \"\"\"\n",
    "        out = x\n",
    "        for layer in self.gcn_net:\n",
    "            out = layer(out, edge_index, edge_weight=edge_weight, kwargs=gcn_kwargs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "The classifier block is the part of the model that produces the output classification. It passes the input graph representation of size 'input_dim' (typically the number of node features) through several affine ('linear') layers to create a new embedding of the graph representation, and then passes it through 3 parallel affine layers that create the final classification for dimension, boundary and manifold. This doesn´t add any regularization inbetween, as it should not be a very deep network. If it has to be, that is probably a sign that something with the backbone if off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierBlock(torch.nn.Module):\n",
    "    \"\"\"Classifier Block for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        torch (Module): PyTorch module.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        hidden_dims: list[int],\n",
    "        manifold_classes: int = 6,\n",
    "        boundary_classes: int = 3,\n",
    "        dimension_classes: int = 3,\n",
    "        activation: Callable[[torch.Tensor], torch.tensor] = F.relu,\n",
    "        linear_kwargs: list[dict] = None,\n",
    "        dim_kwargs: dict = None,\n",
    "        boundary_kwargs: dict = None,\n",
    "        manifold_kwargs: dict = None,\n",
    "    ):\n",
    "        \"\"\"Initialize the ClassifierBlock.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Input feature dimension.\n",
    "            output_dim (int): Output feature dimension.\n",
    "            hidden_dims (list[int]): List of hidden layer dimensions.\n",
    "            manifold_classes (int, optional): Number of manifold classes. Defaults to 6.\n",
    "            boundary_classes (int, optional): Number of boundary classes. Defaults to 3.\n",
    "            dimension_classes (int, optional): Number of dimension classes. Defaults to 3.\n",
    "            activation (Callable[[Tensor], Tensor], optional): Activation function. Defaults to F.relu.\n",
    "            linear_kwargs (list[dict], optional): Additional arguments for linear layers. Defaults to None.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Input feature dimension.\n",
    "            output_dim (int): Output feature dimension.\n",
    "            hidden_dims (list[int]): List of hidden layer dimensions.\n",
    "            manifold_classes (int, optional): Number of manifold classes. Defaults to 6.\n",
    "            boundary_classes (int, optional): Number of boundary classes. Defaults to 3.\n",
    "            dimension_classes (int, optional): Number of dimension classes. Defaults to 3.\n",
    "            activation (Callable[[Tensor], Tensor], optional): Activation function. Defaults to F.relu.\n",
    "            linear_kwargs (list[dict], optional): Additional arguments for linear layers. Defaults to None.\n",
    "            dim_kwargs (dict, optional): Additional arguments for dimension layer. Defaults to None.\n",
    "            boundary_kwargs (dict, optional): Additional arguments for boundary layer. Defaults to None.\n",
    "            manifold_kwargs (dict, optional): Additional arguments for manifold layer. Defaults to None.\n",
    "        \"\"\"\n",
    "        super(ClassifierBlock, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.total_classes = manifold_classes + boundary_classes + dimension_classes\n",
    "        self.manifold_classes = manifold_classes\n",
    "        self.boundary_classes = boundary_classes\n",
    "        self.dimension_classes = dimension_classes\n",
    "\n",
    "        if len(hidden_dims) == 0:\n",
    "            self.backbone = Linear(input_dim, output_dim, **(linear_kwargs[0] if linear_kwargs else {}))\n",
    "        else:\n",
    "            layers = []\n",
    "            in_dim = input_dim\n",
    "            for (i, hidden_dim) in enumerate(hidden_dims):\n",
    "                layers.append(\n",
    "                    Linear(in_dim, hidden_dim, **(linear_kwargs[i] if linear_kwargs and linear_kwargs[i] else {}))\n",
    "                )  # check again if we need the bias there, I don't think so actually...\n",
    "                layers.append(activation)\n",
    "                in_dim = hidden_dim\n",
    "\n",
    "            self.backbone = torch.nn.Sequential(*layers)\n",
    "\n",
    "            self.dim_layer = torch.nn.Linear(hidden_dim, self.dimension_classes, **(dim_kwargs if dim_kwargs else {}))\n",
    "\n",
    "            self.boundary_layer = torch.nn.Linear(hidden_dim, self.boundary_classes, **(boundary_kwargs if boundary_kwargs else {}))\n",
    "\n",
    "            self.manifold_layer = torch.nn.Linear(hidden_dim, self.manifold_classes, **(manifold_kwargs if manifold_kwargs else {}))\n",
    "\n",
    "    def forward(self, x: torch.tensor, backbone_kwargs: dict=None, dim_layer_kwargs: dict=None, boundary_layer_kwargs: dict=None, manifold_layer_kwargs: dict=None)->tuple[torch.Tensor, torch.tensor, torch.tensor]:\n",
    "        \"\"\"Forward pass for the ClassifierBlock.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            backbone_kwargs (dict, optional): Additional arguments for backbone layer. Defaults to None.\n",
    "            dim_layer_kwargs (dict, optional): Additional arguments for dimension layer. Defaults to None.\n",
    "            boundary_layer_kwargs (dict, optional): Additional arguments for boundary layer. Defaults to None.\n",
    "            manifold_layer_kwargs (dict, optional): Additional arguments for manifold layer. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.tensor, torch.tensor]: Dimension, boundary, and manifold logits.\n",
    "        \"\"\"\n",
    "        x = self.backbone(\n",
    "            x, \n",
    "            **(backbone_kwargs if backbone_kwargs is not None else {})\n",
    "        )  \n",
    "\n",
    "        dim_logit = self.dim_layer(x, **(dim_layer_kwargs if dim_layer_kwargs is not None else {}))\n",
    "        boundary_logit = self.boundary_layer(x, **(boundary_layer_kwargs if boundary_layer_kwargs is not None else {}))\n",
    "        manifold_logit = self.manifold_layer(x, **(manifold_layer_kwargs if manifold_layer_kwargs is not None else {}))\n",
    "\n",
    "        return dim_logit, boundary_logit, manifold_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "We also have a block for including graph features. These would normally be added after the graph representation has been created from the node features and are used to inject additional global information into the network. This would be done via concatenation of the graph representation created by the `GCNBackbone` and the output of a `GraphFeaturesBlock` instance. Alternatively, they can be summed up, too. \n",
    "\n",
    "This is somewhat similar to the classifier in that it passes the input features through a number of affine/linear layers to provide the model with enough capacity to learn useful representations of them to add to the output of the `GCNBackbone`. Currently, this has no regularizers, because it has not been tested in the model yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphFeaturesBlock(torch.nn.Module):\n",
    "    \"\"\"Graph Features Block for processing global graph features.\n",
    "\n",
    "    Args:\n",
    "        torch (Module): PyTorch module.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dims: int, activation: Callable[[torch.Tensor], torch.tensor] = F.relu, linear_kwargs: list[dict] = None, final_linear_kwargs: dict = None):\n",
    "        \"\"\"Initialize the GraphFeaturesBlock.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Input feature dimension.\n",
    "            output_dim (int): Output feature dimension.\n",
    "            hidden_dims (list[int]): List of hidden layer dimensions.\n",
    "            activation (Callable[[torch.Tensor], torch.tensor], optional): Activation function. Defaults to F.relu.\n",
    "            linear_kwargs (list[dict], optional): Additional arguments for linear layers. Defaults to None.\n",
    "            final_linear_kwargs (dict, optional): Additional arguments for final linear layer. Defaults to None.\n",
    "        \"\"\"\n",
    "        super(GraphFeaturesBlock, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        if len(hidden_dims) == 0: \n",
    "            self.layers = [Linear(input_dim, output_dim, **(linear_kwargs[0] if linear_kwargs else {})),]\n",
    "        else: \n",
    "            layers = []\n",
    "            in_dim = input_dim\n",
    "            for (i, hidden_dim) in enumerate(hidden_dims): \n",
    "                layers.append(Linear(in_dim, hidden_dim, **(linear_kwargs[i] if linear_kwargs and linear_kwargs[i] else {})))\n",
    "                layers.append(activation)\n",
    "                in_dim = hidden_dim\n",
    "            layers.append(Linear(in_dim, output_dim, **(final_linear_kwargs if final_linear_kwargs else {})))\n",
    "            self.layers = layers\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Forward pass for the GraphFeaturesBlock.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        for layer in self.layers: \n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Main model class \n",
    "This class binds everything together - the `GCNBackbone`, the `Classifier` and the optional `GraphFeaturesBlock`. Additionally, it adds a pooling layer after the `GCNBackbone` to create the final graph representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModel(torch.nn.Module):\n",
    "    \"\"\"Torch module for the full GCN model, which consists of a GCN backbone, a classifier, and a pooling layer, augmented with optional graph features network.\n",
    "    Args:\n",
    "        torch.nn.Module: base class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        gcn_net: GCNBackbone,\n",
    "        classifier: ClassifierBlock,\n",
    "        pooling_layer: torch.nn.Module,\n",
    "        use_graph_features: bool = False,\n",
    "        graph_features_net: torch.nn.Module = torch.nn.Identity,\n",
    "    ):\n",
    "        \"\"\"Initialize the GCNModel.\n",
    "\n",
    "        Args:\n",
    "            gcn_net (GCNBackbone): GCN backbone network.\n",
    "            classifier (ClassifierBlock): Classifier block.\n",
    "            pooling_layer (torch.nn.Module): Pooling layer.\n",
    "            use_graph_features (bool, optional): Whether to use graph features. Defaults to False.\n",
    "            graph_features_net (torch.nn.Module, optional): Graph features network. Defaults to torch.nn.Identity.\n",
    "        \"\"\"\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.gcn_net = gcn_net\n",
    "        self.classifier = classifier\n",
    "        self.graph_features_net = graph_features_net\n",
    "        self.use_graph_features = use_graph_features\n",
    "        self.pooling_layer = pooling_layer\n",
    "\n",
    "    def get_embeddings(\n",
    "        self,\n",
    "        x: torch.tensor,\n",
    "        edge_index: torch.tensor,\n",
    "        batch: torch.tensor,\n",
    "        edge_weight: torch.tensor = None,\n",
    "        gcn_kwargs: dict = None,\n",
    "    ):\n",
    "        \"\"\"Get embeddings from the GCN model.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input node features.\n",
    "            edge_index (torch.Tensor): Graph connectivity information.\n",
    "            batch (torch.Tensor): Batch vector.\n",
    "            edge_weight (torch.Tensor, optional): Edge weights. Defaults to None.\n",
    "            gcn_kwargs (dict, optional): Additional arguments for GCN. Defaults to None.\n",
    "        Returns:\n",
    "            torch.Tensor: Node embeddings after GCN processing and pooling.\n",
    "        \"\"\"\n",
    "\n",
    "        # apply the GCN backbone to the node features\n",
    "        x = self.gcn_net(\n",
    "            x, edge_index, edge_weight=edge_weight, **(gcn_kwargs if gcn_kwargs else {})\n",
    "        )\n",
    "\n",
    "        # for single graph processing, we can have batch=None, so we need to handle that case\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(x.shape[0], dtype=torch.long, device=x.device)\n",
    "\n",
    "        # pool everything together into a single graph representation\n",
    "        x = self.pooling_layer(x, batch)\n",
    "        return x\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.tensor,\n",
    "        edge_index: torch.tensor,\n",
    "        batch: torch.tensor,\n",
    "        edge_weight: torch.tensor = None,\n",
    "        graph_features: torch.tensor = None,\n",
    "        gcn_kwargs: dict = None,\n",
    "    ) -> tuple[torch.Tensor, torch.tensor, torch.tensor]:\n",
    "        \"\"\"Forward pass for the GCNModel.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input node features.\n",
    "            edge_index (torch.Tensor): Graph connectivity information.\n",
    "            batch (torch.Tensor): Batch vector.\n",
    "            edge_weight (torch.Tensor, optional): Edge weights. Defaults to None.\n",
    "            graph_features (torch.Tensor, optional): Graph features. Defaults to None.\n",
    "            gcn_kwargs (dict, optional): Additional arguments for GCN. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.tensor, torch.tensor]: Manifold, boundary, and dimension logits.\n",
    "        \"\"\"\n",
    "\n",
    "        # apply the GCN backbone to the node features\n",
    "        x = self.gcn_net(\n",
    "            x, edge_index, edge_weight=edge_weight, **(gcn_kwargs if gcn_kwargs else {})\n",
    "        )\n",
    "\n",
    "        # for single graph processing, we can have batch=None, so we need to handle that case\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(x.shape[0], dtype=torch.long, device=x.device)\n",
    "\n",
    "        # pool everything together into a single graph representation\n",
    "        x = self.pooling_layer(x, batch)\n",
    "\n",
    "        # If we have graph features, we need to process them and concatenate them with the node features\n",
    "        if self.use_graph_features:\n",
    "            graph_features = self.graph_features_net(graph_features)\n",
    "            x = torch.cat(\n",
    "                (x, graph_features), dim=-1\n",
    "            )  # -1 -> last dim. This concatenates, but we also could sum them\n",
    "\n",
    "        # Classifier creates raw the logits for manifold, boundary, and dimension classification\n",
    "        # no softmax or sigmoid is applied here, as we want to keep the logits for loss calculation\n",
    "        dim, boundary, manifold = self.classifier(x)\n",
    "\n",
    "        return dim, boundary, manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(x_pred: torch.tensor, y: torch.tensor, dim_kwargs: dict = None, boundary_kwargs: dict=None, manifold_kwargs: dict=None, dim_weight: float = 1.0, boundary_weight: float=1.0, manifold_weight: float=1.0):\n",
    "    \"\"\"Compute the loss for the GCN model.\n",
    "\n",
    "    Args:\n",
    "        x_pred (torch.tensor): Predicted logits.\n",
    "        y (torch.tensor): Ground truth labels.\n",
    "        dim_kwargs (dict, optional): Additional arguments for dimension loss. Defaults to None.\n",
    "        boundary_kwargs (dict, optional): Additional arguments for boundary loss. Defaults to None.\n",
    "        manifold_kwargs (dict, optional): Additional arguments for manifold loss. Defaults to None.\n",
    "        dim_weight (float, optional): Weight for dimension loss. Defaults to 1.0.\n",
    "        boundary_weight (float, optional): Weight for boundary loss. Defaults to 1.0.\n",
    "        manifold_weight (float, optional): Weight for manifold loss. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Total loss, dimension loss, boundary loss, manifold loss\n",
    "    \"\"\"\n",
    "    # split up the logits into the three parts we need for the loss calculation\n",
    "    # the logits are expected to be in the order: [dim_logits, boundary_logits,\n",
    "    # manifold_logits]\n",
    "\n",
    "    # since we have an unequal number of classes, we use inverse probability weighting\n",
    "    # to balance the loss contributions from each class. This is done by weighting the loss\n",
    "    dim_logits= x_pred[0]\n",
    "    boundary_logits = x_pred[1]\n",
    "    manifold_logits = x_pred[2]\n",
    "\n",
    "    dim_truth = y[:, 0]\n",
    "    boundary_truth = y[:, 1]\n",
    "    manifold_truth = y[:, 2]\n",
    "\n",
    "    dim_cel = torch.nn.CrossEntropyLoss(**(dim_kwargs if dim_kwargs else {}))\n",
    "    boundary_cel = torch.nn.CrossEntropyLoss(**(boundary_kwargs if boundary_kwargs else {}))\n",
    "    manifold_cel = torch.nn.CrossEntropyLoss(**(manifold_kwargs if manifold_kwargs else {}))\n",
    "\n",
    "    loss_dim = dim_cel(dim_logits, dim_truth)\n",
    "    loss_boundary = boundary_cel(boundary_logits, boundary_truth)\n",
    "    loss_manifold = manifold_cel(manifold_logits, manifold_truth)\n",
    "\n",
    "    total_loss = loss_dim * dim_weight + loss_boundary * boundary_weight + loss_manifold * manifold_weight\n",
    "\n",
    "    return total_loss, loss_dim, loss_boundary, loss_manifold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## function for training the datamodel\n",
    "\n",
    "\n",
    "We record mean and std-deviation (over batches) of the losses per epoch to investigate later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: GCNModel,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epochs: int = 10,\n",
    "    device: str = \"cpu\",\n",
    "    criterion=criterion,\n",
    "    early_stopping_patience: int = 10,\n",
    "    early_stopping_window: int = 5,\n",
    "    early_stopping_delta: float = 0.01,\n",
    ") -> tuple[pd.DataFrame, GCNModel, int, float]:\n",
    "    \"\"\"Train the model and return the training results.\n",
    "\n",
    "    Args:\n",
    "        model (GCNModel): Model to be trained.\n",
    "        train_loader (DataLoader): DataLoader for the training set.\n",
    "        val_loader (DataLoader): DataLoader for the validation set.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for the training process.\n",
    "        epochs (int, optional): Number of training epochs. Defaults to 10.\n",
    "        device (str, optional): Device to run the model on. Defaults to 'cpu'.\n",
    "        criterion (_type_, optional): Loss function to be used. Defaults to criterion.\n",
    "        early_stopping_patience (int, optional): Patience for early stopping. Defaults to 10.\n",
    "        early_stopping_delta (float, optional): Minimum change to qualify as an improvement. Defaults to 0.01.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, Dict, int, int]: A tuple containing the loss data, trained model, number of epochs, and best validation loss.\n",
    "    \"\"\"\n",
    "    # make the losses into pandas dataframes, so we can easily plot them later and have them more orangized\n",
    "\n",
    "    loss_data = pd.DataFrame(\n",
    "        np.nan,\n",
    "        index=range(epochs),\n",
    "        columns=[\n",
    "            \"total_training_loss_mean\",\n",
    "            \"total_training_loss_std\",\n",
    "            \"dim_training_loss_mean\",\n",
    "            \"dim_training_loss_std\",\n",
    "            \"boundary_training_loss_mean\",\n",
    "            \"boundary_training_loss_std\",\n",
    "            \"manifold_training_loss_mean\",\n",
    "            \"manifold_training_loss_std\",\n",
    "            \"total_validation_loss_mean\",\n",
    "            \"total_validation_loss_std\",\n",
    "            \"dim_validation_loss_mean\",\n",
    "            \"dim_validation_loss_std\",\n",
    "            \"boundary_validation_loss_mean\",\n",
    "            \"boundary_validation_loss_std\",\n",
    "            \"manifold_validation_loss_mean\",\n",
    "            \"manifold_validation_loss_std\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch = 0\n",
    "    best_model = None\n",
    "    current_patience = early_stopping_patience\n",
    "\n",
    "    lossfunc = criterion  # Use the custom criterion defined above\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_training_loss_epoch = np.zeros(len(train_loader), dtype=np.float64)\n",
    "        dim_training_loss_epoch = np.zeros(len(train_loader), dtype=np.float64)\n",
    "        boundary_training_loss_epoch = np.zeros(len(train_loader), dtype=np.float64)\n",
    "        manifold_training_loss_epoch = np.zeros(len(train_loader), dtype=np.float64)\n",
    "\n",
    "        model.train()\n",
    "        for batchnum, data in enumerate(\n",
    "            tqdm(train_loader, desc=f\"Epoch {epoch} Training\")\n",
    "        ):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            total_loss, loss_dim, loss_boundary, loss_manifold = lossfunc(out, data.y)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_training_loss_epoch[batchnum] = total_loss.item()\n",
    "            dim_training_loss_epoch[batchnum] = loss_dim.item()\n",
    "            boundary_training_loss_epoch[batchnum] = loss_boundary.item()\n",
    "            manifold_training_loss_epoch[batchnum] = loss_manifold.item()\n",
    "\n",
    "        # Store the losses in the loss_data DataFrame\n",
    "        loss_data.loc[epoch, \"total_training_loss_mean\"] = (\n",
    "            total_training_loss_epoch.mean()\n",
    "        )\n",
    "        loss_data.loc[epoch, \"total_training_loss_std\"] = (\n",
    "            total_training_loss_epoch.std()\n",
    "        )\n",
    "        loss_data.loc[epoch, \"dim_training_loss_mean\"] = dim_training_loss_epoch.mean()\n",
    "        loss_data.loc[epoch, \"dim_training_loss_std\"] = dim_training_loss_epoch.std()\n",
    "        loss_data.loc[epoch, \"boundary_training_loss_mean\"] = (\n",
    "            boundary_training_loss_epoch.mean()\n",
    "        )\n",
    "        loss_data.loc[epoch, \"boundary_training_loss_std\"] = (\n",
    "            boundary_training_loss_epoch.std()\n",
    "        )\n",
    "        loss_data.loc[epoch, \"manifold_training_loss_mean\"] = (\n",
    "            manifold_training_loss_epoch.mean()\n",
    "        )\n",
    "        loss_data.loc[epoch, \"manifold_training_loss_std\"] = (\n",
    "            manifold_training_loss_epoch.std()\n",
    "        )\n",
    "\n",
    "        # Validation step\n",
    "        total_validation_loss_epoch = np.zeros(len(val_loader), dtype=np.float64)\n",
    "        dim_validation_loss_epoch = np.zeros(len(val_loader), dtype=np.float64)\n",
    "        boundary_validation_loss_epoch = np.zeros(len(val_loader), dtype=np.float64)\n",
    "        manifold_validation_loss_epoch = np.zeros(len(val_loader), dtype=np.float64)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batchnum, data in enumerate(\n",
    "                tqdm(val_loader, desc=f\"Epoch {epoch} Validation\")\n",
    "            ):\n",
    "                data = data.to(device)\n",
    "\n",
    "                out = model(\n",
    "                    data.x, data.edge_index, data.batch\n",
    "                )  # this fails with incomplete batches\n",
    "                total_loss, loss_dim, loss_boundary, loss_manifold = lossfunc(\n",
    "                    out, data.y\n",
    "                )\n",
    "\n",
    "                total_validation_loss_epoch[batchnum] = total_loss.item()\n",
    "                dim_validation_loss_epoch[batchnum] = loss_dim.item()\n",
    "                boundary_validation_loss_epoch[batchnum] = loss_boundary.item()\n",
    "                manifold_validation_loss_epoch[batchnum] = loss_manifold.item()\n",
    "\n",
    "            # Store the validation losses in the loss_data DataFrame\n",
    "            loss_data.loc[epoch, \"total_validation_loss_mean\"] = (\n",
    "                total_validation_loss_epoch.mean()\n",
    "            )\n",
    "            loss_data.loc[epoch, \"total_validation_loss_std\"] = (\n",
    "                total_validation_loss_epoch.std()\n",
    "            )\n",
    "            loss_data.loc[epoch, \"dim_validation_loss_mean\"] = (\n",
    "                dim_validation_loss_epoch.mean()\n",
    "            )\n",
    "            loss_data.loc[epoch, \"dim_validation_loss_std\"] = (\n",
    "                dim_validation_loss_epoch.std()\n",
    "            )\n",
    "            loss_data.loc[epoch, \"boundary_validation_loss_mean\"] = (\n",
    "                boundary_validation_loss_epoch.mean()\n",
    "            )\n",
    "            loss_data.loc[epoch, \"boundary_validation_loss_std\"] = (\n",
    "                boundary_validation_loss_epoch.std()\n",
    "            )\n",
    "            loss_data.loc[epoch, \"manifold_validation_loss_mean\"] = (\n",
    "                manifold_validation_loss_epoch.mean()\n",
    "            )\n",
    "            loss_data.loc[epoch, \"manifold_validation_loss_std\"] = (\n",
    "                manifold_validation_loss_epoch.std()\n",
    "            )\n",
    "\n",
    "        # Check for early stopping. We could also monitor F1 or precision/recall metrics to get a better estimate of the model performance, but for now we use the validation loss\n",
    "\n",
    "        smoothed_val_loss = (\n",
    "            loss_data.loc[:, \"total_validation_loss_mean\"]\n",
    "            .rolling(window=early_stopping_window, min_periods=1)\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        if smoothed_val_loss[epoch] < best_val_loss - early_stopping_delta:\n",
    "            best_val_loss = smoothed_val_loss[epoch]\n",
    "            current_patience = early_stopping_patience  # Reset patience\n",
    "            best_model = model.state_dict()\n",
    "            best_epoch = epoch\n",
    "        else:\n",
    "            print(\n",
    "                f\"  No improvement in validation loss: {loss_data.loc[epoch, 'total_validation_loss_mean']:.4f} at epoch {epoch}, current patience: {current_patience}\"\n",
    "            )\n",
    "            current_patience -= 1\n",
    "\n",
    "        if current_patience <= 0:\n",
    "            print(\n",
    "                f\"  Early stopping at epoch {epoch} with best validation loss: {best_val_loss:.4f}\"\n",
    "            )\n",
    "            if best_model is not None:\n",
    "                model.load_state_dict(best_model)\n",
    "            break\n",
    "\n",
    "        print(\n",
    "            f\"  training loss: {loss_data.loc[epoch, 'total_training_loss_mean']:.4f} ± {loss_data.loc[epoch, 'total_training_loss_std']:.4f}, Dim : {loss_data.loc[epoch, 'dim_training_loss_mean']:.4f} ± {loss_data.loc[epoch, 'dim_training_loss_std']:.4f}, Boundary : {loss_data.loc[epoch, 'boundary_training_loss_mean']:.4f} ± {loss_data.loc[epoch, 'boundary_training_loss_std']:.4f}, Manifold : {loss_data.loc[epoch, 'manifold_training_loss_mean']:.4f} ± {loss_data.loc[epoch, 'manifold_training_loss_std']:.4f}\"\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"  validation loss: {loss_data.loc[epoch, 'total_validation_loss_mean']:.4f} ± {loss_data.loc[epoch, 'total_validation_loss_std']:.4f}, Dim : {loss_data.loc[epoch, 'dim_validation_loss_mean']:.4f} ± {loss_data.loc[epoch, 'dim_validation_loss_std']:.4f}, Boundary : {loss_data.loc[epoch, 'boundary_validation_loss_mean']:.4f} ± {loss_data.loc[epoch, 'boundary_validation_loss_std']:.4f}, Manifold : {loss_data.loc[epoch, 'manifold_validation_loss_mean']:.4f} ± {loss_data.loc[epoch, 'manifold_validation_loss_std']:.4f}\"\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "    loss_data = loss_data.loc[0 : epoch + 1, :]  # Keep only the losses up to the current epoch\n",
    "    return loss_data, best_model, best_epoch, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### test run\n",
    "The `test` function simply applies the model to the test data without gradient computation and in evaluation mode only --> no dropout or other regularizers that are used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    model: GCNModel, test_loader: torch_geometric.loader.DataLoader, device: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"run the model on the test set and return the results as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        model (GCNModel): Model to be evaluated.\n",
    "        test_loader (torch_geometric.loader.DataLoader): DataLoader for the test set.\n",
    "        device (str): Device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the test results.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    loss_data = pd.DataFrame(\n",
    "        np.nan, \n",
    "        index = range(len(test_loader.dataset)),\n",
    "        columns=[\n",
    "            \"dim\",\n",
    "            \"boundary\",\n",
    "            \"manifold\",\n",
    "            \"dim_correct\",\n",
    "            \"boundary_correct\",\n",
    "            \"manifold_correct\",\n",
    "            \"dim_true\",\n",
    "            \"boundary_true\",\n",
    "            \"manifold_true\",\n",
    "        ]\n",
    "    )\n",
    "    start = 0\n",
    "    stop = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            data = data.to(device)\n",
    "\n",
    "            dim, boundary, manifold = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "            dim = torch.argmax(\n",
    "                dim, dim=1\n",
    "            )  # this is enough here, because the encodings are concruential to the indices\n",
    "            boundary = torch.argmax(\n",
    "                boundary, dim=1\n",
    "            )  # this is enough here, because the encodings are concruential to the indices\n",
    "            manifold = torch.argmax(\n",
    "                manifold, dim=1\n",
    "            )  # this is enough here, because the encodings are concruential to the indices\n",
    "\n",
    "            stop = start + data.num_graphs\n",
    "            \n",
    "            loss_data.iloc[start:stop, loss_data.columns.get_loc(\"dim\")] = dim.cpu().numpy()\n",
    "            loss_data.iloc[start:stop, loss_data.columns.get_loc(\"boundary\")] = boundary.cpu().numpy()\n",
    "            loss_data.iloc[start:stop, loss_data.columns.get_loc(\"manifold\")] = manifold.cpu().numpy()\n",
    "            loss_data.iloc[start:stop, loss_data.columns.get_loc(\"dim_true\")] = data.y[:, 0].cpu().numpy()\n",
    "            loss_data.iloc[start:stop, loss_data.columns.get_loc(\"boundary_true\")] = data.y[:, 1].cpu().numpy()\n",
    "            loss_data.iloc[start:stop, loss_data.columns.get_loc(\"manifold_true\")] = data.y[:, 2].cpu().numpy()\n",
    "\n",
    "\n",
    "            loss_data.iloc[start:stop, loss_data.columns.get_loc(\"dim_correct\")] = np.float64(\n",
    "                dim.cpu().numpy() == data.y[:, 0].cpu().numpy()\n",
    "            )\n",
    "            loss_data.iloc[start:stop, loss_data.columns.get_loc(\"boundary_correct\")] = np.float64(\n",
    "                boundary.cpu().numpy() == data.y[:, 1].cpu().numpy()\n",
    "            )\n",
    "            loss_data.iloc[start:stop, loss_data.columns.get_loc(\"manifold_correct\")] = np.float64(\n",
    "                manifold.cpu().numpy() == data.y[:, 2].cpu().numpy()\n",
    "            )\n",
    "            start = stop\n",
    "            \n",
    "\n",
    "    return loss_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Actual training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Set the seed for the internal rng. This is important for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(532432)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### get cuda device\n",
    "If we don´t have one, we need to run this on the cpu. That would not be suitable for any useful work though, and you might want to reduce the data size in such cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Define where the data is.  \n",
    "\n",
    "We can have arbitrarily many hdf5 files to read data from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapath  = os.path.join(os.path.expanduser(\"~\"), \"data\", \"causal_sets\")\n",
    "datapath = os.path.join(\"/mnt\", \"dataLinux\", \"machinelearning_data\", \"QuantumGrav\", \"causal_sets\")\n",
    "files = [\n",
    "    os.path.join(datapath, \"cset_data_min=300_max=650_N=25000_d=2.h5\"),\n",
    "    os.path.join(datapath, \"cset_data_min=300_max=650_N=25000_d=3.h5\"),\n",
    "    os.path.join(datapath, \"cset_data_min=300_max=650_N=25000_d=4.h5\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Define the dataset and the train, validation and test split. Then make `DataLoader` instances from them to use with the model. These take care of batching, shuffling and so on. notice that we use the same transformations as before. the dataset is smart enough to notice if it has to process data again. (the current one only does this by checking files though, so you have to delete the processessed ones again if you want to change the transformations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = CsDataset(\n",
    "    input=files,\n",
    "    output=os.path.join(datapath),\n",
    "    pre_transform=target_shift,  # maybe add some augmentation stuff here later\n",
    "    pre_filter=None,  # Filter data before loading, e.g., based on manifold or boundary or something like that\n",
    "    validate_data=True,  # Validate data after loading\n",
    "    loader=load_graph,  # Custom loader function\n",
    ").shuffle() # shuffle the dataset to ensure a good mix of samples in each batch\n",
    "\n",
    "# roughly 80-10-10 split for train-test-validation\n",
    "train_size = int(math.ceil(0.8 * len(dset)))\n",
    "test_size = int(math.ceil(0.1 * len(dset)))\n",
    "val_size = len(dset) - train_size - test_size\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dset[0:train_size],\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dset[train_size : train_size + test_size],\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dset[train_size + test_size: train_size + test_size + val_size],\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_loader)}, Test size: {len(test_loader)}, Validation size: {len(val_loader)}\")\n",
    "print(f\"Total dataset size: {len(train_loader.dataset)}, {len(test_loader.dataset)}, {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Define a new model \n",
    "\n",
    "This is done by building it element by element --> first the graph convolutional blocks, then the classifier, and finally the whole model is assembled from these blocks together with a global pooling operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_node_features = dset[0].x.shape[1]  # Number of node features\n",
    "n_edge_features = dset[0].edge_attr.shape[1] if dset[0].edge_attr is not None else 0  # Number of edge features\n",
    "\n",
    "# normalizer = torch.nn.Identity \n",
    "normalizer = torch.nn.BatchNorm1d \n",
    "conv_layer = GCNConv  # You can change this to GATConv, SAGEConv, etc. as needed\n",
    "\n",
    "conv1 = GCNBlock(\n",
    "    input_dim=n_node_features,\n",
    "    output_dim=128,\n",
    "    dropout=0.3,\n",
    "    normalizer=normalizer(128),  # Use BatchNorm1d for batch normalization\n",
    "    gcn_type=conv_layer,  # You can change this to GATConv, SAGEConv, etc.\n",
    "    activation=torch.nn.ReLU(),\n",
    "    gcn_kwargs={\"cached\": False, \"bias\": True, \"add_self_loops\": True}  # Example of passing additional arguments to the GCN layer\n",
    ")\n",
    "\n",
    "conv2 = GCNBlock(\n",
    "    input_dim=128,\n",
    "    output_dim=256,\n",
    "    dropout=0.3,\n",
    "    normalizer=normalizer(256),  # Use BatchNorm1d for batch normalization\n",
    "    gcn_type=conv_layer,  # You can change this to GATConv, SAGEConv, etc.\n",
    "    activation=torch.nn.ReLU(),\n",
    "    gcn_kwargs={\"cached\": False, \"bias\": True, \"add_self_loops\": True}  # Example of passing additional arguments to the GCN layer\n",
    ")\n",
    "\n",
    "conv3 = GCNBlock(\n",
    "    input_dim=256,\n",
    "    output_dim=128,\n",
    "    dropout=0.3,\n",
    "    normalizer=normalizer(128),  # Use BatchNorm1d for batch normalization\n",
    "    gcn_type=conv_layer,  # You can change this to GATConv, SAGEConv, etc.\n",
    "    activation=torch.nn.ReLU(),\n",
    "    gcn_kwargs={\"cached\": False, \"bias\": True, \"add_self_loops\": True}  # Example of passing additional arguments to the GCN layer\n",
    ")\n",
    "\n",
    "\n",
    "gcn_backbone = GCNBackbone([conv1, conv2, conv3])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ClassifierBlock(\n",
    "    input_dim=128,  # Output dimension of the last GCN layer\n",
    "    output_dim=3,  # Assuming you want to predict manifold_id, boundary_id, and dimension\n",
    "    hidden_dims=[64, 32],  # Example hidden dimensions\n",
    "    manifold_classes=6,\n",
    "    boundary_classes=3,\n",
    "    dimension_classes=3,\n",
    "    activation=torch.nn.ReLU(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "we use mean pooling here, just because this is the simplest. Be aware however that this can enhance feature squishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_layer = global_mean_pool  # You can change this to global_max_pool, global_add_pool, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCNModel(\n",
    "    gcn_net=gcn_backbone,\n",
    "    classifier=classifier,\n",
    "    pooling_layer=pooling_layer,\n",
    "    use_graph_features=False,  # Set to True if you want to use graph features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Hyperparameters for training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 120\n",
    "patience = 10\n",
    "current_patience = patience\n",
    "best_val_loss = float(\"inf\")\n",
    "tolerance = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data, best_model, best_epoch, stopping_epoch = train(model, train_loader, val_loader, optimizer, epochs=epochs, device=device, criterion=criterion, early_stopping_patience=patience, early_stopping_delta=tolerance);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "# Loss visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.style.use('petroff10')  # Use a clean style for the plots\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = plt.gca()\n",
    "mean_cols = [\n",
    "        \"total_training_loss_mean\",\n",
    "        \"total_validation_loss_mean\",\n",
    "        \"dim_training_loss_mean\",\n",
    "        \"dim_validation_loss_mean\",\n",
    "        \"boundary_training_loss_mean\",\n",
    "        \"boundary_validation_loss_mean\",\n",
    "        \"manifold_training_loss_mean\", \n",
    "        \"manifold_validation_loss_mean\"]\n",
    "std_cols =[\n",
    "        \"total_training_loss_std\",\n",
    "        \"total_validation_loss_std\",\n",
    "        \"dim_training_loss_std\",\n",
    "        \"dim_validation_loss_std\",\n",
    "        \"boundary_training_loss_std\",\n",
    "        \"boundary_validation_loss_std\",\n",
    "        \"manifold_training_loss_std\",\n",
    "        \"manifold_validation_loss_std\"\n",
    "    ]\n",
    "\n",
    "loss_data.plot(\n",
    "    y=mean_cols, \n",
    "    yerr=loss_data[std_cols].set_axis(mean_cols, axis=1),\n",
    "    ax=ax,\n",
    "    title=\"Training and Validation Losses\",\n",
    "    xlabel=\"Epoch\",\n",
    "    ylabel=\"Loss\",\n",
    "    legend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "run model on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model)\n",
    "model.eval() \n",
    "test_df = test(model, test_loader, device)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "## Per-label Accuracy\n",
    "cummulative accuracy over all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_acc = (test_df[\"dim_correct\"].sum() + test_df[\"boundary_correct\"].sum() + test_df[\"manifold_correct\"].sum()) / (3*len(test_df))\n",
    "print(f\"Per-label accuracy: {cum_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "# Per-task Accuracy \n",
    "which task is correct how often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_acc = test_df[\"dim_correct\"].mean()\n",
    "boundary_acc = test_df[\"boundary_correct\"].mean()\n",
    "manifold_acc = test_df[\"manifold_correct\"].mean()\n",
    "print(f\"Dimension Accuracy: {dim_acc:.4f}\")\n",
    "print(f\"Boundary Accuracy: {boundary_acc:.4f}\")\n",
    "print(f\"Manifold Accuracy: {manifold_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "the model is not super bad on individual tasks, but manifold accuracy is very low compared to the other 2 because it is much more difficult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "# Per-sample accuracy\n",
    "\n",
    "how often are they all correct for a given sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_accuracy = (test_df[\"dim_correct\"].astype(bool) & test_df[\"boundary_correct\"].astype(bool) & test_df[\"manifold_correct\"].astype(bool)).mean()\n",
    "print(f\"Strict (all-correct) Accuracy: {strict_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "but it is pretty bad on the full task. this is the metric we need to push, because they are all relevant. Check the loss function again for this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "we build one per task: dimension, boundary, manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \n",
    "\n",
    "cm_dim = confusion_matrix(test_df[\"dim_true\"], test_df[\"dim\"])\n",
    "cm_boundary = confusion_matrix(test_df[\"boundary_true\"], test_df[\"boundary\"])\n",
    "cm_manifold = confusion_matrix(test_df[\"manifold_true\"], test_df[\"manifold\"])\n",
    "\n",
    "dim_cd = ConfusionMatrixDisplay(cm_dim, display_labels=[0,1,2]).plot()\n",
    "boundary_cd = ConfusionMatrixDisplay(cm_boundary, display_labels=[0,1,]).plot() \n",
    "manifold_cd = ConfusionMatrixDisplay(cm_manifold, display_labels=[0,1,2,3,4,5]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.nan, index = range(len(train_loader.dataset)), columns=[\"dim\", \"boundary\", \"manifold\"])\n",
    "\n",
    "i = 0\n",
    "for data in tqdm(train_loader.dataset):\n",
    "\n",
    "    df.iloc[i, 0] = data.y[0][0].item()\n",
    "    df.iloc[i, 1] = data.y[0][1].item()\n",
    "    df.iloc[i, 2] = data.y[0][2].item()\n",
    "    i += 1\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"manifold\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "# Precision/Recall per class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "| Term |\tMeaning | \n",
    "| -------|---------- | \n",
    "| TP | True Positives: correctly predicted samples of the class | \n",
    "| FP | False Positives: predicted as the class, but actually something else | \n",
    "| FN | False Negatives: actual class samples predicted as something else | \n",
    "\n",
    "\n",
    "**Precision** \n",
    "```math\n",
    "Precision = \\frac{TP}{TP + FP} \\; \\in [0, 1]\n",
    "```\n",
    "High precision -> few false positives\n",
    "\n",
    "**Recall** \n",
    "```math \n",
    "Recall = \\frac{TP}{TP + FN}  \\; \\in [0, 1]\n",
    "```\n",
    "High recall -> few false negatives \n",
    "\n",
    "We want both to be high, i.e., as close 1 as possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "precision_dim_per_class = precision_score(test_df['dim_true'], test_df[\"dim\"], average = None) \n",
    "recall_dim_per_class = recall_score(test_df['dim_true'], test_df[\"dim\"], average = None)\n",
    "\n",
    "\n",
    "precision_boundary_per_class = precision_score(test_df['boundary_true'], test_df[\"boundary\"], average=None)\n",
    "recall_boundary_per_class = recall_score(test_df['boundary_true'], test_df[\"boundary\"], average=None)\n",
    "\n",
    "precision_manifold_per_class = precision_score(test_df['manifold_true'], test_df[\"manifold\"], average = None)\n",
    "recall_manifold_per_class = recall_score(test_df['manifold_true'], test_df[\"manifold\"], average = None)\n",
    "\n",
    "\n",
    "# average over all classes\n",
    "precision_dim_avg = precision_score(test_df['dim_true'], test_df[\"dim\"], average = 'macro') \n",
    "recall_dim_avg = recall_score(test_df['dim_true'], test_df[\"dim\"], average = 'macro')\n",
    "\n",
    "precision_boundary_avg = precision_score(test_df['boundary_true'], test_df[\"boundary\"], average='macro')\n",
    "recall_boundary_avg = recall_score(test_df['boundary_true'], test_df[\"boundary\"], average='macro')\n",
    "\n",
    "precision_manifold_avg = precision_score(test_df['manifold_true'], test_df[\"manifold\"], average = 'macro')\n",
    "recall_manifold_avg = recall_score(test_df['manifold_true'], test_df[\"manifold\"], average = 'macro')\n",
    "\n",
    "\n",
    "# weighted average over classes - weight with support = number of samples per class\n",
    "precision_dim_weighted = precision_score(test_df['dim_true'], test_df[\"dim\"], average = 'weighted') \n",
    "recall_dim_weighted = recall_score(test_df['dim_true'], test_df[\"dim\"], average = 'weighted')\n",
    "\n",
    "precision_boundary_weighted = precision_score(test_df['boundary_true'], test_df[\"boundary\"], average='weighted')\n",
    "recall_boundary_weighted = recall_score(test_df['boundary_true'], test_df[\"boundary\"], average='weighted')\n",
    "\n",
    "precision_manifold_weighted = precision_score(test_df['manifold_true'], test_df[\"manifold\"], average = 'weighted')\n",
    "recall_manifold_weighted = recall_score(test_df['manifold_true'], test_df[\"manifold\"], average = 'weighted')\n",
    "\n",
    "print(\"Precision and Recall per class for Dimension:\")\n",
    "for i in range(3):\n",
    "    print(f\"Class {i}: Precision = {precision_dim_per_class[i]:.4f}, Recall = {recall_dim_per_class[i]:.4f}\")\n",
    "\n",
    "print(\"Precision and Recall per class for Boundary: \")\n",
    "for i in range(2): \n",
    "    print(f\"Class {i}: Precision = {precision_boundary_per_class[i]:.4f}, Recall = {recall_boundary_per_class[i]:.4f}\")\n",
    "\n",
    "print(\"Precision and Recall per class for Manifold: \")\n",
    "for i in range(6):\n",
    "    print(f\"Class {i}: Precision = {precision_manifold_per_class[i]:.4f}, Recall = {recall_manifold_per_class[i]:.4f}\")\n",
    "\n",
    "print(f\"Average Precision (Macro): {precision_dim_avg:.4f}, Average Recall (Macro): {recall_dim_avg:.4f}\")\n",
    "print(f\"Average Precision (Weighted): {precision_dim_weighted:.4f}, Average Recall (Weighted): {recall_dim_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "# F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    " The F1 score is the harmonic mean of precision and recall. It combines them into a single number that balances both, especially when you want a trade-off between the two:\n",
    "\n",
    " ```math\n",
    " F1 = 2 * \\frac{\n",
    "    Precision * Recall\n",
    " }{Precision + Recall} \n",
    " ```\n",
    "\n",
    " The harmonic mean penalizes extreme imbalance more than the arithmetic mean.\n",
    "E.g., if precision = 1.0 and recall = 0.0, the F1 score is 0.0 — you can't ignore one metric entirely.\n",
    "\n",
    "macro F1: simple mean across all classes (treats each class equally)\n",
    "\n",
    "weighted F1: mean weighted by number of samples per class (favours common classes)\n",
    "\n",
    "per-class F1: no mean at all\n",
    "\n",
    "The F1 score is especially valuable when you're optimizing for model robustness across all classes, particularly if one class is harder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_dim = f1_score(test_df['dim_true'], test_df['dim'], average=None)  # Per-class F1\n",
    "f1_dim_macro = f1_score(test_df['dim_true'], test_df['dim'], average='macro')  # Unweighted average\n",
    "f1_dim_weighted = f1_score(test_df['dim_true'], test_df['dim'], average='weighted')  # Weighted by class support\n",
    "\n",
    "f1_boundary = f1_score(test_df['boundary_true'], test_df['boundary'], average=None)  # Per-class F1\n",
    "f1_boundary_macro = f1_score(test_df['boundary_true'], test_df['boundary'], average='macro')  # Unweighted average\n",
    "f1_boundary_weighted = f1_score(test_df['boundary_true'], test_df['boundary'], average='weighted')  # Weighted by class support\n",
    "\n",
    "f1_manifold = f1_score(test_df['manifold_true'], test_df['manifold'], average=None)\n",
    "f1_manifold_macro = f1_score(test_df['manifold_true'], test_df['manifold'], average='macro')\n",
    "f1_manifold_weighted = f1_score(test_df['manifold_true'], test_df['manifold'], average='weighted')\n",
    "\n",
    "print(\"F1 score per class for dimension:\")\n",
    "for i in range(3):\n",
    "    print(f\"  class {i}: {f1_dim[i]}\")\n",
    "print(\"base average of F1 score for dimension: \", f1_dim_macro)\n",
    "print(\"weighted average of F1 score for dimension: \", f1_dim_weighted)\n",
    "\n",
    "print(\"F1 score per class for boundary:\")\n",
    "for i in range(2):\n",
    "    print(f\"  class {i}: {f1_boundary[i]}\")\n",
    "print(\"base average of F1 score for boundary: \", f1_boundary_macro)\n",
    "print(\"weighted average of F1 score for boundary: \", f1_boundary_weighted)\n",
    "\n",
    "print(\"F1 score per class for manifold:\")\n",
    "for i in range(6):\n",
    "    print(f\"  class {i}: {f1_manifold[i]}\")\n",
    "print(\"base average of F1 score for manifold: \", f1_manifold_macro)\n",
    "print(\"weighted average of F1 score for manifold: \", f1_manifold_weighted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"Adam\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"weight_decay_rate\": 5e-4,\n",
    "    }, \n",
    "    \"training\": {\n",
    "        \"epochs\": epochs,\n",
    "        \"patience\": patience,\n",
    "        \"tolerance\": tolerance, \n",
    "        \"batchsize\": 128\n",
    "    }, \n",
    "    \"model\": {\n",
    "        \"Backbone\": [\n",
    "            {\n",
    "                \"input\": 4,\n",
    "                \"output\":128,\n",
    "                \"type\": \"GCNConv\",\n",
    "                \"activation\": \"relu\",\n",
    "                \"dropout\": 0.3,\n",
    "                \"normalizer\": \"batchnorm\",\n",
    "                \"kwargs\": {\"cached\": False, \"bias\": True, \"add_self_loops\": True}\n",
    "            },\n",
    "            {\n",
    "                \"input\": 128,\n",
    "                \"output\":256,\n",
    "                \"type\": \"GCNConv\",\n",
    "                \"activation\": \"relu\",\n",
    "                \"dropout\": 0.3,\n",
    "                \"normalizer\": \"batchnorm\",\n",
    "                \"kwargs\": {\"cached\": False, \"bias\": True, \"add_self_loops\": True}\n",
    "            },\n",
    "            {\n",
    "                \"input\": 256,\n",
    "                \"output\":128,\n",
    "                \"type\": \"GCNConv\",\n",
    "                \"activation\": \"relu\",\n",
    "                \"dropout\": 0.3,\n",
    "                \"normalizer\": \"batchnorm\",\n",
    "                \"kwargs\": {\"cached\": False, \"bias\": True, \"add_self_loops\": True}\n",
    "            },\n",
    "\n",
    "        ],\n",
    "        \"Classifier\": {\n",
    "            \"input_dim\": 128,  # Output dimension of the last GCN layer\n",
    "            \"output_dim\": 3,  # Assuming you want to predict manifold_id, boundary_id, and dimension\n",
    "            \"hidden_dims\": [64, 32],  # Example hidden dimensions\n",
    "            \"manifold_classes\": 6,\n",
    "            \"boundary_classes\": 3,\n",
    "            \"dimension_classes\": 3,\n",
    "            \"activation\": \"relu\",\n",
    "        },\n",
    "        \"GraphFeatures\": None,\n",
    "    }, \n",
    "    \"data\": {\n",
    "        \"node_features\": [\"in_degree\", \"out_degree\", \"max_path_future\", \"max_path_past\"], \n",
    "        \"graph_features\": None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(datapath, \"models\", \"gcn_model_simple_unoptimized.pt\")\n",
    "model_dir = os.path.abspath(os.path.dirname(model_path))\n",
    "os.makedirs(model_dir, exist_ok = True)\n",
    "\n",
    "with open(os.path.join(model_dir, \"hyperparameters.json\"), \"w\") as file: \n",
    "    json.dump(hyperparams, file)\n",
    "\n",
    "torch.save(model.to('cpu'), os.path.join(model_dir, \"model.pth\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "## TSNE feature space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    " get the embedding vectors of the model on the test set first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = np.zeros((test_size, 128), dtype=np.float32)  # Preallocate space for embeddings. \n",
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "results = pd.DataFrame(np.nan, index=range(test_size), columns=[\"dim\", \"boundary\", \"manifold\", \"tsne_x\", \"tsne_y\"])\n",
    "start = 0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader):\n",
    "        data = data.to(device)\n",
    "        embeddings = model.get_embeddings(data.x, data.edge_index, data.batch)\n",
    "        e = embeddings.cpu().numpy()\n",
    "        chunk = data.num_graphs\n",
    "        stop = start + chunk\n",
    "        results.iloc[start:stop, results.columns.get_loc(\"dim\")] = data.y[:, 0].cpu().numpy()\n",
    "        results.iloc[start:stop, results.columns.get_loc(\"boundary\")] = data.y[:, 1].cpu().numpy()\n",
    "        results.iloc[start:stop, results.columns.get_loc(\"manifold\")] = data.y[:, 2].cpu().numpy()\n",
    "        embedding_vectors[start:stop] = e\n",
    "        start = stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42, max_iter=1000, early_exaggeration= 4.0, perplexity=45, init='pca', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_result = tsne.fit_transform(embedding_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_result.shape\n",
    "results[\"tsne_x\"] = tsne_result[:, 0]\n",
    "results[\"tsne_y\"] = tsne_result[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=results,\n",
    "    x=\"tsne_x\", \n",
    "    y=\"tsne_y\",\n",
    "    hue=\"dim\",\n",
    "    palette='muted',\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title(\"t-SNE Visualization of Dimension Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=results,\n",
    "    x=\"tsne_x\", \n",
    "    y=\"tsne_y\",\n",
    "    hue=\"boundary\",\n",
    "    palette='muted',\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title(\"t-SNE Visualization of Boundary Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=results,\n",
    "    x=\"tsne_x\", \n",
    "    y=\"tsne_y\",\n",
    "    hue=\"manifold\",\n",
    "    palette='muted',\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title(\"t-SNE Visualization of Manifold Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
