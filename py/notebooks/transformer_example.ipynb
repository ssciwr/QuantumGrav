{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict, Optional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import (BatchNorm1d, Embedding, Linear, ModuleList, ReLU, Sequential)\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torch_geometric.transforms as T \n",
    "from torch_geometric.datasets import ZINC \n",
    "from torch_geometric.loader import DataLoader \n",
    "from torch_geometric.nn import GINEConv, GPSConv, global_add_pool \n",
    "from torch_geometric.nn.attention import PerformerAttention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.path.dirname(os.path.realpath('./')), 'data', 'ZINC-PE')\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.AddRandomWalkPE(walk_length=20, attr_name='pe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ZINC(path, subset=True, split='train', pre_transform=transform)\n",
    "val_dataset = ZINC(path, subset=True, split='val', pre_transform=transform)\n",
    "test_dataset = ZINC(path, subset=True, split='test', pre_transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# GPS graph transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "An architecture that processes the graph in parallel through a global transformer and a local MPNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is specific to the Performer architecture\n",
    "class RedrawProjection: \n",
    "    def __init__(self, model:torch.nn.Module, redraw_interval: Optional[int] = None): \n",
    "        self.model = model \n",
    "        self.redraw_interval = redraw_interval \n",
    "        self.num_last_redraw = 0\n",
    "\n",
    "    def redraw_projections(self): \n",
    "        if not self.model.training or self.redraw_interval is None: \n",
    "            return\n",
    "        \n",
    "        if self.num_last_redraw >= self.redraw_interval: \n",
    "            fast_attentions = [\n",
    "                module for module in self.model.modules()  if isinstance(module, PerformerAttention)\n",
    "            ]\n",
    "\n",
    "            for fast_attention in fast_attentions: \n",
    "                fast_attention.redraw_projection_matrix() \n",
    "            self.num_last_redraw = 0\n",
    "        else: \n",
    "            self.num_last_redraw += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### The actual GPS architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPS(torch.nn.Module): \n",
    "\n",
    "    def __init__(self, channels:int, pe_dim: int, num_layers: int, attn_type: str, attn_kwargs: Dict[str, Any] ): \n",
    "        super().__init__() \n",
    "\n",
    "\n",
    "        self.node_emb = Embedding(28, channels - pe_dim) # this can be sparse \n",
    "        self.pe_lin = Linear(20, pe_dim)\n",
    "        self.pe_norm = BatchNorm1d(20)\n",
    "        self.edge_emb = Embedding(4, channels)\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        for _ in range(num_layers): \n",
    "            nn = Sequential(\n",
    "                Linear(channels, channels), \n",
    "                ReLU(), \n",
    "                Linear(channels, channels)\n",
    "            )\n",
    "            conv = GPSConv(channels, GINEConv(nn), heads=4, attn_type=attn_type, attn_kwargs=attn_kwargs)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.mlp = Sequential(\n",
    "            Linear(channels, channels // 2), \n",
    "            ReLU(), \n",
    "            Linear(channels // 2, channels // 4), \n",
    "            ReLU(), \n",
    "            Linear(channels // 4, 1)\n",
    "        )\n",
    "\n",
    "        self.redraw_projection = RedrawProjection(\n",
    "            self.convs, \n",
    "            redraw_interval = 1000 if attn_type=='performer' else None\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x, pe, edge_index, edge_attr, batch): \n",
    "        # TODO: this should be rewritten in a more elegant, functional way\n",
    "        x_pe = self.pe_norm(pe)\n",
    "        x = torch.cat((self.node_emb(x.squeeze(-1)), self.pe_lin(x_pe)), 1)\n",
    "        edge_attr = self.edge_emb(edge_attr)\n",
    "\n",
    "        for conv in self.convs: \n",
    "            x = conv(x, edge_index, batch, edge_attr=edge_attr) \n",
    "        \n",
    "        x = global_add_pool(x, batch)\n",
    "        return self.mlp(x)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## training and testing and shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, \n",
    "          loader: DataLoader, \n",
    "          optimizer: torch.optim.Optimizer, \n",
    "          device: torch.device,): \n",
    "    model.train() # training mode\n",
    "\n",
    "    total_loss = 0.0 \n",
    "    for data in train_loader: \n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        model.redraw_projection.redraw_projections()  # redraw projections if needed \n",
    "        out = model(data.x, data.pe, data.edge_index, data.edge_attr, data.batch) \n",
    "\n",
    "        loss = (out.squeeze() - data.y).abs().mean() \n",
    "        loss.backward() \n",
    "        total_loss += loss.item() * data.num_graphs \n",
    "        optimizer.step()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model: torch.nn.Module,\n",
    "         loader: DataLoader, \n",
    "         device: torch.device): \n",
    "    model.eval()  # evaluation mode\n",
    "\n",
    "    total_error = 0.0 \n",
    "    for data in loader: \n",
    "        data = data.to(device) \n",
    "        out = model(data.x, data.pe, data.edge_index, data.edge_attr, data.batch) \n",
    "        total_error += (out.squeeze() - data.y).abs().sum().item() \n",
    "    return total_error /len(loader.dataset)\n",
    "\n",
    "\n",
    "def run_training(\n",
    "        model: torch.nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: ReduceLROnPlateau,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        test_loader: DataLoader,\n",
    "        device: torch.device,\n",
    "        epochs: int = 101,\n",
    "): \n",
    "    train_losses = []\n",
    "    val_maes = []\n",
    "    test_maes = []\n",
    "    for epoch in range(1, epochs):\n",
    "        loss = train(model, train_loader, optimizer, device)\n",
    "        val_mae = test(model, val_loader, device)\n",
    "        test_mae = test(model, test_loader, device)\n",
    "        scheduler.step(val_mae)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val MAE: {val_mae:.4f}, Test MAE: {test_mae:.4f}')\n",
    "        train_losses.append(loss)\n",
    "        val_maes.append(val_mae)\n",
    "        test_maes.append(test_mae)\n",
    "    return train_losses, val_maes, test_maes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Run training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda'if torch.cuda.is_available() else 'cpu')\n",
    "attention_type= 'multihead'  # 'performer', 'multihead'\n",
    "attn_kwargs = {'dropout': 0.5, }\n",
    "model =GPS(\n",
    "    channels = 64, \n",
    "    pe_dim=8, \n",
    "    num_layers=10, \n",
    "    attn_type = attention_type, \n",
    "    attn_kwargs=attn_kwargs, \n",
    ").to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, min_lr=1e-5)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) \n",
    "val_loader = DataLoader(val_dataset, batch_size=64) \n",
    "test_loader = DataLoader(test_dataset, batch_size=64) \n",
    "train_loss_64, val_mae_32, test_mae_32 = run_training(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    epochs=101,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda'if torch.cuda.is_available() else 'cpu')\n",
    "attention_type= 'multihead'  # 'performer', 'multihead'\n",
    "attn_kwargs = {'dropout': 0.5, }\n",
    "model =GPS(\n",
    "    channels = 64, \n",
    "    pe_dim=8, \n",
    "    num_layers=10, \n",
    "    attn_type = attention_type, \n",
    "    attn_kwargs=attn_kwargs, \n",
    ").to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, min_lr=1e-5)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True) \n",
    "val_loader = DataLoader(val_dataset, batch_size=256) \n",
    "test_loader = DataLoader(test_dataset, batch_size=256) \n",
    "\n",
    "train_loss_128, val_mae_128, test_mae_128 = run_training(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    epochs=101,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "This uses only one GPS layer and has limited heads and capacity. We don't expect it to be particularly good. Also, there is no early stopping and hardly any test loss seems unstable. hence the batches are perhaps too small? There also appears to be some overfitting and shit. however, the principle is sound, and the system works, but the code needs some cleanup and a better structure. Pytorch is not very strict. I would like to have something in JAX for this, but Jraph has been archived and I don't see how I could build it. Switching from batches of size 32 to 128 seems to solve the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
