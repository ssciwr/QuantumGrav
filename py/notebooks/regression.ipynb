{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch_geometric.loader import DataLoader \n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from functools import lru_cache\n",
    "from collections.abc import Callable\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: this function is too long, break it up into smaller ones\n",
    "\n",
    "\n",
    "def load_graph(\n",
    "    f: h5py.File,\n",
    "    idx: int,\n",
    "    float_dtype: torch.dtype,\n",
    "    int_dtype: torch.dtype,\n",
    "    validate: bool = False,\n",
    ") -> Data:\n",
    "    # Load adjacency matrix and convert to edge indices\n",
    "    adj_matrix = torch.tensor(f[\"adjacency_matrix\"][:, :, idx], dtype=float_dtype)\n",
    "    edge_index, edge_weight = dense_to_sparse(adj_matrix)\n",
    "    adj_matrix = adj_matrix.to_sparse()\n",
    "    # Load node features\n",
    "    node_features = []\n",
    "\n",
    "    # Sprinkling coordinates\n",
    "    # sprinkling = torch.tensor(f[\"sprinkling\"][:, :, idx], dtype=float_dtype)\n",
    "\n",
    "    # node_features.append(sprinkling)\n",
    "\n",
    "    # Degree information\n",
    "    in_degrees = torch.tensor(f[\"in_degrees\"][:, idx], dtype=float_dtype).unsqueeze(1)\n",
    "    out_degrees = torch.tensor(f[\"out_degrees\"][:, idx], dtype=float_dtype).unsqueeze(1)\n",
    "    node_features.extend([in_degrees, out_degrees])\n",
    "\n",
    "    # Path lengths\n",
    "    max_path_future = torch.tensor(\n",
    "        f[\"max_path_lengths_future\"][:, idx], dtype=float_dtype\n",
    "    ).unsqueeze(1)\n",
    "    max_path_past = torch.tensor(\n",
    "        f[\"max_path_lengths_past\"][:, idx], dtype=float_dtype\n",
    "    ).unsqueeze(1)\n",
    "    node_features.extend([max_path_future, max_path_past])\n",
    "\n",
    "    # I need more topological information here - angles, etc.\n",
    "    # Link-based path lengths\n",
    "    # max_path_future_links = torch.tensor(\n",
    "    #     f[\"max_path_lengths_future_links\"][:, idx], dtype=float_dtype\n",
    "    # ).unsqueeze(1)\n",
    "    # max_path_past_links = torch.tensor(\n",
    "    #     f[\"max_path_lengths_past_links\"][:, idx], dtype=float_dtype\n",
    "    # ).unsqueeze(1)\n",
    "    # node_features.extend([max_path_future_links, max_path_past_links])\n",
    "\n",
    "    # # Topological ordering --> TODO: check again what this does\n",
    "    # topo_future = torch.tensor(\n",
    "    #     f[\"topological_order_future\"][:, idx], dtype=float_dtype\n",
    "    # ).unsqueeze(1)\n",
    "    # topo_past = torch.tensor(\n",
    "    #     f[\"topological_order_past\"][:, idx], dtype=float_dtype\n",
    "    # ).unsqueeze(1)\n",
    "    # node_features.extend([topo_future, topo_past])\n",
    "\n",
    "    # Concatenate all node features\n",
    "    x = torch.cat(node_features, dim=1)\n",
    "\n",
    "    # Load graph-level features (targets for regression)\n",
    "    manifold_id = torch.tensor(f[\"manifold_ids\"][idx], dtype=int_dtype)\n",
    "    boundary_id = torch.tensor(f[\"boundary_ids\"][idx], dtype=int_dtype)\n",
    "    relation_dim = torch.tensor(f[\"relation_dim\"][idx], dtype=float_dtype)\n",
    "    atom_count = torch.tensor(f[\"atom_count\"][idx], dtype=int_dtype)\n",
    "    num_sources = torch.tensor(f[\"num_sources\"][idx], dtype=int_dtype)\n",
    "    num_sinks = torch.tensor(f[\"num_sinks\"][idx], dtype=int_dtype)\n",
    "    dimension = torch.tensor(f[\"dimension\"][idx], dtype=int_dtype)\n",
    "\n",
    "    # matrices\n",
    "    link_matrix = torch.tensor(\n",
    "        f[\"link_matrix\"][:, :, idx], dtype=float_dtype\n",
    "    ).to_sparse()\n",
    "    past_relations = torch.tensor(\n",
    "        f[\"past_relations\"][:, :, idx], dtype=float_dtype\n",
    "    ).to_sparse()\n",
    "    future_relations = torch.tensor(\n",
    "        f[\"future_relations\"][:, :, idx], dtype=float_dtype\n",
    "    ).to_sparse()\n",
    "\n",
    "    # Create Data object\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_weight.unsqueeze(1)\n",
    "        if edge_weight.numel() > 0\n",
    "        else None,  # Not sure if this is a good idea need to add edge attributes if possible\n",
    "        # node positions as positional attributes as well\n",
    "        # pos=sprinkling,\n",
    "        y=torch.tensor([manifold_id[0], boundary_id[0], dimension[0]]),\n",
    "        # Graph-level attributes\n",
    "        manifold_id=manifold_id,\n",
    "        boundary_id=boundary_id,\n",
    "        relation_dim=relation_dim,\n",
    "        dimension=dimension,\n",
    "        atom_count=atom_count,\n",
    "        num_sources=num_sources,\n",
    "        num_sinks=num_sinks,\n",
    "        # sprinkling=sprinkling, # don't use this for now.\n",
    "        # Additional matrices as graph attributes. make the shitty past and future relations and all that into node attributes!\n",
    "        adjacency_matrix=adj_matrix,\n",
    "        link_matrix=link_matrix,\n",
    "        past_relations=past_relations,\n",
    "        future_relations=future_relations,\n",
    "    )\n",
    "\n",
    "    if validate:\n",
    "        data.validate()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input: list[str],\n",
    "        output: str,  # = root directory for processed data\n",
    "        transform: Callable[[Data], Data] | None = None,\n",
    "        pre_transform: Callable[[Data], Data] | None = None,\n",
    "        pre_filter: Callable[[Data], Data] | None = None,\n",
    "        validate_data: bool = False,\n",
    "    ):\n",
    "        super().__init__(output, transform, pre_transform, pre_filter)\n",
    "        self.input = input\n",
    "        self._num_samples = None\n",
    "        self.validate_data = validate_data\n",
    "\n",
    "        if os.path.exists(output):\n",
    "            self.processed_data = self.processed_file_names\n",
    "            self.num_samples = len(self.processed_data)\n",
    "            with open(os.path.join(output, \"metadata.json\"), \"r\") as f:\n",
    "                metadata = json.load(f)\n",
    "                self.manifold_codes = metadata[\"manifold_codes\"]\n",
    "                self.manifold_names = metadata[\"manifold_names\"]\n",
    "                self.boundaries = metadata[\"boundaries\"]\n",
    "                self.boundary_codes = metadata[\"boundary_codes\"]\n",
    "        else:\n",
    "            with h5py.File(input, \"r\") as f:\n",
    "                self.manifold_codes = f[\"manifold_codes\"]\n",
    "                self.manifold_names = f[\"manifold_names\"]\n",
    "                self.boundaries = f[\"boundaries\"]\n",
    "                self.boundary_codes = f[\"boundary_codes\"]\n",
    "\n",
    "            self._num_samples = 0\n",
    "            for file in self.input:\n",
    "                if not os.path.exists(file):\n",
    "                    raise FileNotFoundError(f\"Input file {file} does not exist.\")\n",
    "                with h5py.File(file, \"r\") as f:\n",
    "                    self._num_samples += f[\"num_causal_sets\"]\n",
    "\n",
    "    @property\n",
    "    def raw_paths(self):\n",
    "        return self.input\n",
    "\n",
    "    @property\n",
    "    def output(self):\n",
    "        return self.root\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [os.path.basename(self.input)]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [\n",
    "            f\n",
    "            for f in os.listdir(self.root)\n",
    "            if f.startswith(\"data_\") and f.endswith(\".pt\")\n",
    "        ]\n",
    "\n",
    "    def process(self):\n",
    "        os.path.makedirs(self.root, exist_ok=True)\n",
    "\n",
    "        d = {\n",
    "            \"manifold_codes\": self.manifold_codes,\n",
    "            \"manifold_names\": self.manifold_names,\n",
    "            \"boundaries\": self.boundaries,\n",
    "            \"boundary_codes\": self.boundary_codes,\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(self.processed_dir, \"metadata.json\"), \"w\") as f:\n",
    "            json.dump(d, f)\n",
    "\n",
    "        for file in self.raw_paths:\n",
    "            if not os.path.exists(file):\n",
    "                raise FileNotFoundError(f\"Input file {file} does not exist.\")\n",
    "            with h5py.File(file, \"r\") as f:\n",
    "                # FIXME: this loop should be parallelized for large datasets\n",
    "                for idx in range(f[\"num_causal_sets\"]):\n",
    "                    data = load_graph(\n",
    "                        f,\n",
    "                        idx,\n",
    "                        float_dtype=torch.float32,\n",
    "                        int_dtype=torch.int64,\n",
    "                        validate=self.validate_data,\n",
    "                    )\n",
    "                    if self.pre_filter is not None:\n",
    "                        if not self.pre_filter(data):\n",
    "                            continue\n",
    "                    if self.pre_transform is not None:\n",
    "                        data = self.pre_transform(data)\n",
    "                    torch.save(data, os.path.join(self.processed_dir, f\"data_{idx}.pt\"))\n",
    "\n",
    "    def len(self):\n",
    "        return self._num_samples\n",
    "\n",
    "    @lru_cache(maxsize=100)  # cache some results for faster access\n",
    "    def get(self, idx):\n",
    "        data = torch.load(os.path.join(self.processed_dir, f\"data_{idx}.pt\"))\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "from torch.nn import Linear \n",
    "import torch.nn.functional as F \n",
    "from torch_geometric.nn.conv import GCNConv, GATConv, SAGEConv, GraphConv, GATv2Conv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool, SAGPooling, Set2Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBlock(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.5, gcn_type=GCNConv, batchnorm=torch.nn.Identity, activation = F.relu, gcn_kwargs=None):\n",
    "        super(GCNBlock, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.gcn_type = gcn_type\n",
    "        self.conv = gcn_type(input_dim, output_dim, **(gcn_kwargs if gcn_kwargs else {}))\n",
    "        self.activation = activation\n",
    "        self.batch_norm = batchnorm\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x_res = x \n",
    "        x = self.conv(x, edge_index, edge_weight=edge_weight)  # Apply the GCN layer\n",
    "        x = self.batch_norm(x, ) # this is a no-op if batch normalization is not used\n",
    "        x = self.activation(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training) # this is only applied during training\n",
    "        x = x + x_res # skip connection\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionBlock(torch.nn.Module): \n",
    "    def __init__(self, input_dim, output_dim, hidden_dims, activation = F.relu): \n",
    "        super(RegressionBlock, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        if len(hidden_dims) == 0: \n",
    "            self.layers = Linear(input_dim, output_dim)\n",
    "        else: \n",
    "            layers = []\n",
    "            in_dim = input_dim\n",
    "            for hidden_dim in hidden_dims: \n",
    "                layers.append(Linear(in_dim, hidden_dim)) # check again if we need the bias there, I don't think so actually... \n",
    "                layers.append(activation)\n",
    "                in_dim = hidden_dim\n",
    "            layers.append(Linear(in_dim, output_dim))\n",
    "            self.layers = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.layers(x) # <-- need softmax here somewhere? -> yes, for the categorical outputs, but not for the regression outputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphFeaturesBlock(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims, dropout=0.5, activation = F.relu): \n",
    "        super(GraphFeaturesBlock, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        if len(hidden_dims) == 0: \n",
    "            self.linear = Linear(input_dim, output_dim)\n",
    "        else: \n",
    "            layers = []\n",
    "            in_dim = input_dim\n",
    "            for hidden_dim in hidden_dims: \n",
    "                layers.append(Linear(in_dim, hidden_dim)) # check bias\n",
    "                layers.append(activation)\n",
    "                in_dim = hidden_dim\n",
    "            layers.append(Linear(in_dim, output_dim))\n",
    "            self.linear = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "basic model class to organize the other things. does the following: \n",
    "- passes input through gcn network. This is a succession of GCN blocks\n",
    "- applies graph feature network to graph level features and concatenates them with pooled node features **if** `use_graph_features = true`. \n",
    "\n",
    "- passes the result through the regression net to get out (dimension, boundary_id, manifold_id): \n",
    "```bash\n",
    "x -> gcn -> pool -> concat(_, g) -> regression -> output\n",
    "g ----------------> MLP_g(g) _|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModel(torch.nn.Module): \n",
    "\n",
    "    def __init__(self, gcn_net, regression_net, pooling_layer, use_graph_features=False, graph_features_net = torch.nn.Identity):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.gcn_net = gcn_net\n",
    "        self.regression_net = regression_net\n",
    "        self.graph_features_net = graph_features_net\n",
    "        self.use_graph_features = use_graph_features\n",
    "        self.pooling_layer = pooling_layer\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight = None, graph_features=None): \n",
    "        x = self.gcn_net(x, edge_index, edge_weight=edge_weight)\n",
    "        x = self.pooling_layer(x, batch)\n",
    "        if self.use_graph_features: \n",
    "            graph_features = self.graph_features_net(graph_features)\n",
    "            x = torch.cat((x, graph_features), dim=-1) # last dim\n",
    "        x = self.regression_net(x)\n",
    "        x = torch.softmax(x, dim=-1)  # Apply softmax for categorical outputs\n",
    "        return x\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "- [ ] how do GCNConv, GraphConv, SageConv work? \n",
    "- [ ] how does GlobalAttention, Set2Set work?\n",
    "- [ ] train the damn thing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Apply the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## make dataset from graphs and have them processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath  = os.path.join(os.path.home(), \"data\", \"causal_sets\")\n",
    "files = [\n",
    "    os.path.join(datapath, \"cset_data_min=300_max=600_N=10000_d=2.h5\"),\n",
    "    os.path.join(datapath, \"cset_data_min=300_max=600_N=10000_d=3.h5\"),\n",
    "    os.path.join(datapath, \"cset_data_min=300_max=600_N=10000_d=4.h5\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add one-hot encoding for manifold and boundary ids and for the dimension\n",
    "dset = CsDataset(\n",
    "    input=files,\n",
    "    output=os.path.join(datapath, \"processed\"),\n",
    "    transform=T.ToSparseTensor,  # Convert to sparse tensor format\n",
    "    pre_transform=None, # maybe add some augmentation stuff here later\n",
    "    pre_filter=None,  # Filter data before loading, e.g., based on manifold or boundary or something like that\n",
    "    validate_data=True,  # Validate data after loading\n",
    ").shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## make test, validation and train data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(math.ceil(0.8 * len(dset)))\n",
    "test_size = int(math.ceil(0.1 * len(dset)))\n",
    "val_size = len(dset) - train_size - test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dset[:test_size], batch_size=32, shuffle=True)\n",
    "train_loader = DataLoader(dset[test_size:test_size + train_size], batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dset[test_size + train_size:], batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## make model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "The model consists of 3 GCN blocks, a global mean pool (as a first step) and a final regression block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_node_features = dset[0].x.shape[1]  # Number of node features\n",
    "n_edge_features = dset[0].edge_attr.shape[1] if dset[0].edge_attr is not None else 0  # Number of edge features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "first build the graph convolutional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = torch.nn.BatchNorm1d\n",
    "conv_layer = GCNConv  # You can change this to GATConv, SAGEConv, etc. as needed\n",
    "\n",
    "conv1 = GCNBlock(\n",
    "    input_dim=n_node_features,\n",
    "    output_dim=128,\n",
    "    dropout=0.3,\n",
    "    batchnorm=normalizer(128),  # Use BatchNorm1d for batch normalization\n",
    "    gcn_type=conv_layer,  # You can change this to GATConv, SAGEConv, etc.\n",
    "    activation=F.relu,\n",
    "    gcn_kwargs={\"cached\": True}  # Example of passing additional arguments to the GCN layer\n",
    ")\n",
    "\n",
    "conv2 = GCNBlock(\n",
    "    input_dim=128,\n",
    "    output_dim=256,\n",
    "    dropout=0.3,\n",
    "    batchnorm=normalizer(256),  # Use BatchNorm1d for batch normalization\n",
    "    gcn_type=conv_layer,  # You can change this to GATConv, SAGEConv, etc.\n",
    "    activation=F.relu,\n",
    "    gcn_kwargs={\"cached\": True}  # Example of passing additional arguments to the GCN layer\n",
    ")\n",
    "\n",
    "conv3 = GCNBlock(\n",
    "    input_dim=256,\n",
    "    output_dim=128,\n",
    "    dropout=0.3,\n",
    "    batchnorm=normalizer(128),  # Use BatchNorm1d for batch normalization\n",
    "    gcn_type=conv_layer,  # You can change this to GATConv, SAGEConv, etc.\n",
    "    activation=F.relu,\n",
    "    gcn_kwargs={\"cached\": True}  # Example of passing additional arguments to the GCN layer\n",
    ")\n",
    "\n",
    "gcn_chain = torch.nn.Sequential(\n",
    "    conv1,\n",
    "    conv2,\n",
    "    conv3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "then build the regression layer, this is simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_net = RegressionBlock(\n",
    "    input_dim=128,  # Output dimension of the last GCN layer\n",
    "    output_dim=3,  # Assuming you want to predict manifold_id, boundary_id, and dimension\n",
    "    hidden_dims=[64, 32],  # Example hidden dimensions\n",
    "    activation=F.relu\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "finally put things together and add the global pooling layer. We don't use the graph features for the moment, so we don't define a graph features processing network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_layer = global_mean_pool  # You can change this to global_max_pool, global_add_pool, etc.\n",
    "\n",
    "model = GCNModel(\n",
    "    gcn_net=gcn_chain,\n",
    "    regression_net=regression_net,\n",
    "    pooling_layer=pooling_layer,\n",
    "    use_graph_features=False,  # Set to True if you want to use graph features\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "much of this will not work properly I'm sure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=5e-4)\n",
    "criterion = torch.nn.BinaryCrossEntropyLoss() # this is unlikely to work because we have mixed regression and classification outputs, so we need to handle this differently or do we? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Check that this is sensible first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, validation_loader, optimizer, criterion):\n",
    "    training_loss = np.zeros(len(train_loader), dtype=np.float32)\n",
    "    valid_loss = np.zeros(len(validation_loader), dtype=np.float32)\n",
    "    for epoch, data in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch, graph_features=data.y)\n",
    "        loss = criterion(out, data.y)  # Assuming data.y contains the target values\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss[epoch] = loss.item()\n",
    "\n",
    "        mean_validation_loss = 0.0\n",
    "        for j, data in enumerate(validation_loader):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model(data.x, data.edge_index, data.batch, graph_features=data.y)\n",
    "                loss = criterion(out, data.y)\n",
    "                mean_validation_loss += loss.item()\n",
    "        mean_validation_loss /= len(validation_loader)\n",
    "        print(f\"Epoch: {epoch}, Validation Loss: {mean_validation_loss:.4f}\")\n",
    "        valid_loss[epoch] = loss.item()\n",
    "\n",
    "    return training_loss\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            out = model(data.x, data.edge_index, data.batch, graph_features=data.y)\n",
    "            predicted = out[torch.argmax(out, dim=-1)] # this doesn't work, I'm sure. \n",
    "\n",
    "            total += data.y.size(0)\n",
    "            correct += (predicted == data.y).sum().item()\n",
    "\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, validation_loss = train(model, train_loader, val_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test(model, test_loader) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
